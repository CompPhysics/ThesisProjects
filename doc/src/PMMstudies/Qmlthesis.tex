\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{setspace}
\onehalfspacing

\usepackage{hyperref}
\usepackage{pgfgantt}

\title{Quantum Machine Learning for Finance\\[0.3em]
  \large Master of Science Thesis Project}
\author{%
  (Student: \textit{NN)\\[0.2em]
  Supervisors: \textit{Morten Hjorth-Jensen and other}\\[0.2em]
  Department of Physics, University of Oslo}
\date{Draft: \today}

\begin{document}

\maketitle

\section*{Background and Motivation}

Quantum Computing (QC) and Machine Learning (ML) are two of the most transformative paradigms in modern computational science. Quantum computers operate on qubits, which can exist in superpositions of classical states and can become entangled. These properties allow, in principle, new computational strategies that may outperform classical computers on selected tasks, such as simulation of quantum systems, high-dimensional optimization, and certain data-analysis problems.

Quantum Machine Learning (QML) aims to combine quantum computing with ML techniques to design algorithms that can exploit quantum resources for learning and inference. Typical examples include quantum-enhanced kernel methods (e.g., quantum support vector machines), variational quantum classifiers and regressors (QNNs based on parameterized quantum circuits), and quantum generalizations of generative models such as Boltzmann Machines.

The financial sector provides a rich domain of structured and high-dimensional data, such as time series of asset prices, interest rates, volatility indices, and risk factors. Understanding whether quantum-enhanced models can provide advantages---in accuracy, robustness, sample efficiency, or computational cost---is both scientifically interesting and of potential practical relevance.

In parallel, European quantum infrastructures are maturing. In particular, the EuroHPC Joint Undertaking supports quantum accelerators such as \emph{LUMI-Q} in Finland, which can be accessed through national and European allocations. This creates a realistic opportunity for academic projects to run hybrid quantum--classical workflows on actual quantum hardware, not only on simulators.

\section*{Project Scope and Objectives}

This thesis will investigate classical and quantum machine learning methods for supervised learning tasks relevant for finance, with a focus on time-series data (e.g., forecasting, classification, anomaly detection). The main methodological components are:

\begin{enumerate}
  \item \textbf{Support Vector Machines (SVM) and Quantum Support Vector Machines (QSVM):} \\
  Classical SVMs will be implemented as a baseline using standard kernels (linear, polynomial, RBF). Quantum kernels will be realized via quantum feature maps and overlap estimation on parameterized quantum circuits, leading to QSVM-type classifiers.

  \item \textbf{Neural Networks (NN) and Quantum Neural Networks (QNN):} \\
  Classical deep neural networks (fully connected, and optionally recurrent networks for time series) will be developed as benchmarks. QNNs based on Variational Quantum Circuits (VQCs) will be trained using hybrid quantum--classical optimization loops.

  \item \textbf{(Optional) Boltzmann Machines (BM) and Quantum Boltzmann Machines (QBM):} \\
  Time permitting, the thesis will explore classical Boltzmann Machines and their quantum extensions, where a parameterized Hamiltonian defines a quantum generalization of the Boltzmann distribution.
\end{enumerate}

The overarching objectives are:

\begin{itemize}
  \item To build a robust classical ML pipeline for selected financial datasets.
  \item To implement corresponding QML models (QSVM, QNN, optionally QBM).
  \item To execute selected QML algorithms on quantum simulators and on real quantum hardware, including LUMI-Q (Finland) and, where appropriate, IBM Quantum or other accessible devices.
  \item To systematically compare classical and quantum methods in terms of predictive performance, generalization, resource usage, and sensitivity to noise.
\end{itemize}

\section*{Methodology and Tools}

The thesis will rely on a combination of classical and quantum software frameworks:

\begin{itemize}
  \item \textbf{Classical ML:} Python, NumPy, SciPy, Scikit-Learn, and optionally PyTorch or TensorFlow for neural networks.
  \item \textbf{Quantum ML:} Libraries such as PennyLane, Qiskit, or similar frameworks providing:
  \begin{itemize}
    \item Variational quantum circuits and quantum kernels.
    \item Access to backends for both state-vector simulators and noisy hardware.
    \item Integration with hardware providers, including LUMI-Q via EuroHPC interfaces when available, and IBM Quantum through cloud APIs.
  \end{itemize}
  \item \textbf{Data:} Publicly available financial time-series data (e.g., equities, indices, interest rates), and/or data provided through NBIM or project partners, subject to access constraints.
\end{itemize}

Key methodological ingredients include:

\begin{itemize}
  \item Feature engineering and data preprocessing for financial time series.
  \item Design of classical baselines (SVM, NN, possibly RNN/LSTM).
  \item Construction of quantum feature maps and variational ans√§tze.
  \item Training and validation protocols (train/validation/test splits, cross-validation).
  \item Performance metrics (e.g., accuracy, F1-score, MSE, Sharpe-like metrics, calibration).
  \item Robustness checks under noise, limited data, and hardware imperfections.
\end{itemize}

\section*{Work Plan and Milestones (Spring 2026 -- June 2027)}

The thesis is planned to start in \textbf{spring 2026} and be submitted by \textbf{June 2027}. The work is divided into phases with associated milestones.

\subsection*{Phase 1: Foundations and Classical Baselines (Spring 2026)}

\begin{itemize}
  \item Conduct a detailed literature review on:
  \begin{itemize}
    \item Classical ML in finance (SVMs, NNs, time-series models).
    \item Quantum machine learning: QSVMs, QNNs, QBM, and related hybrid algorithms.
  \end{itemize}
  \item Choose and preprocess one or more financial datasets (classification and regression tasks).
  \item Implement classical baselines:
  \begin{itemize}
    \item SVMs (with different kernels).
    \item Neural networks (e.g., fully connected, optionally recurrent for time series).
  \end{itemize}
  \item Define baseline metrics and initial experimental protocols.
\end{itemize}

\textbf{Milestone M1 (end of Spring 2026):}\\
Classical pipeline implemented and validated on at least one financial dataset; preliminary results documented.

\subsection*{Phase 2: QML Prototyping on Simulators (Summer -- Early Autumn 2026)}

\begin{itemize}
  \item Implement quantum feature maps and QSVM models on simulators.
  \item Implement simple QNN/VQC architectures for classification and/or regression.
  \item Explore different data-encoding strategies (angle encoding, amplitude encoding, qubit-efficient encodings).
  \item Benchmark QML models against classical baselines on small problem instances.
\end{itemize}

\textbf{Milestone M2 (early Autumn 2026):}\\
Working QSVM and QNN prototypes on simulators, with initial performance comparison to classical models.

\subsection*{Phase 3: Hardware Experiments (Late 2026 -- Early 2027)}

\begin{itemize}
  \item Adapt QML circuits to hardware constraints (depth, connectivity, noise) of LUMI-Q and other available devices.
  \item Run selected QML models on:
  \begin{itemize}
    \item LUMI-Q (via EuroHPC or national allocation).
    \item IBM Quantum backends (or similar).
  \end{itemize}
  \item Collect and analyze results under realistic noise conditions.
  \item Time permitting, implement and test (Q)Boltzmann Machines on simulators or hardware.
\end{itemize}

\textbf{Milestone M3 (early Spring 2027):}\\
Demonstrated execution of at least one QML model on LUMI-Q and/or IBM Quantum, with comparative analysis versus simulator runs.

\subsection*{Phase 4: Analysis, Synthesis, and Writing (Spring 2027)}

\begin{itemize}
  \item Perform a systematic comparison between classical and quantum models:
  \begin{itemize}
    \item Predictive performance and generalization.
    \item Resource requirements (number of qubits, circuit depth, training time).
    \item Robustness to noise and limited data.
  \end{itemize}
  \item Discuss implications for the use of QML in finance in the near-term (NISQ) regime.
  \item Write the thesis, including introduction, methods, results, and discussion.
  \item Prepare final figures, tables, and appendices (e.g., code snippets).
\end{itemize}

\textbf{Milestone M4 (June 2027):}\\
Completed thesis submitted, with all analyses and conclusions finalized.

\section*{Expected Outcomes}

By the end of the project, the student is expected to have:

\begin{itemize}
  \item Developed a complete classical ML pipeline for selected financial datasets.
  \item Implemented and benchmarked QML algorithms (QSVM, QNN, and optionally QBM).
  \item Executed at least one QML model on real quantum hardware, including LUMI-Q.
  \item Performed a critical comparison between classical and quantum approaches in terms of performance, feasibility, and future prospects in finance.
\end{itemize}

\section*{Indicative Literature}

\begin{enumerate}
  \item M.~Schuld and F.~Petruccione, \emph{Supervised Learning with Quantum Computers}, Springer, 2018.
  \item C.~Conti, \emph{Quantum Machine Learning}, Springer (online).
  \item M.~Zhao et al., ``A Tutorial on Quantum Machine Learning and Quantum Neural Networks'', arXiv:2504.16131 (2025).
  \item M.~Amin et al., ``Quantum Boltzmann Machines'', \emph{Physical Review X} \textbf{8}, 021050 (2018).
  \item M.~Hjorth-Jensen, \emph{Quantum Computing and Quantum Machine Learning}, lecture notes and codes, \url{https://github.com/CompPhysics/QuantumComputingMachineLearning}.
\end{enumerate}

\end{document}


\section*{Gantt Chart: Timeline Overview}

Figure~\ref{fig:gantt} shows a Gantt chart summarizing the planned timeline from March 2026 to June 2027. Dates are indicative and can be adjusted based on actual progress and access to quantum hardware.

\begin{figure}[h!]
  \centering
  \begin{ganttchart}[
    time slot format=isodate-yearmonth,
    compress calendar,
    y unit chart=0.8cm,
    vgrid,
    hgrid,
    bar label font=\footnotesize,
    milestone label font=\footnotesize\bfseries
  ]{2026-03}{2027-06}
    \gantttitlecalendar{year, month=name} \\

    \ganttbar{Phase 1: Literature \& Classical Baselines}{2026-03}{2026-06} \\

    \ganttbar{Phase 2: QML Prototyping (Simulators)}{2026-07}{2026-10} \\

    \ganttbar{Phase 3: Hardware Experiments (LUMI-Q, IBM)}{2026-11}{2027-03} \\

    \ganttbar{Phase 4: Analysis \& Writing}{2027-02}{2027-06} \\

    \ganttmilestone{Thesis Submission}{2027-06}
  \end{ganttchart}
  \caption{Planned timeline for the thesis project, from Spring 2026 to June 2027.}
  \label{fig:gantt}
\end{figure}

