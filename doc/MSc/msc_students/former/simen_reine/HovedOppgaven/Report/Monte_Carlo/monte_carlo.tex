\chapter{Numerical Approaches to the Atomic Problem}
\label{NumericalApproaches}

The recent development in computer technology has made a revolution in
our ability to model the nature around us. The making of better
numerical procedures is currently a hot topic within the scientific
world, and will remain a great challenge also in the years to come.
\newline
%
\newline
In this chapter we will outline approaches for solving the atomic
problem introduced in the previous chapter. We want to find the
eigenstates and eigenenergies satisfying the Born-Oppenheimer
approximated Schr\"odinger equation 

\begin{equation}
  \left[-\sum_{i=1}^N \frac{1}{2} \nabla^2_i 
    - \sum_{i=1}^N \frac{Z}{r_i} + \sum_{i<j}^N \frac{1}{r_{ij}} 
    \right] \Psi(\mathbf{x}) = E \Psi(\mathbf{x})
  \label{SchrodingerBornOppenheimerAtomicUnits2}
\end{equation}

for $N\ge2$. With more than one electron present in
eq.~(\ref{SchrodingerBornOppenheimerAtomicUnits2}) we cannot find an
analytical solution and must resort to numerical efforts. In this
chapter we will examine the theory of several numerical methods,
commonly applied to the atomic problem.


%******* Hartree-Fock and Density Functional Theory *******
%*
%*
\section{Hartree-Fock and Density Functional Theory}

%******************* Hartree-Fock Theory ******************
\subsection{Hartree-Fock Theory}
\label{HartreeFockTheory}


\emph{Hartree-Fock} theory \cite{helgaker2002,atkins2003,bransden1983}
is one of the simplest approximate theories  
for solving the many-body Hamiltonian. It is based on a simple
approximation to the true many-body wave-function; that the
wave-function is given by a single Slater determinant of $N$ 
orthonormal orbitals.

\begin{equation}
  \Psi = \frac{1}{\sqrt{N!}}\left| 
  \begin{array}{cccc}
    \psi_1(\mathbf{x_1})&\psi_1(\mathbf{x_2})&\dots&\psi_1(\mathbf{x_N}) \\ [4pt]
    \psi_2(\mathbf{x_1})&\psi_2(\mathbf{x_2})&\dots&\psi_2(\mathbf{x_N}) \\[4pt] 
    \vdots              & \vdots            &\ddots&\vdots\\[4pt]
    \psi_N(\mathbf{x_1})&\psi_N(\mathbf{x_2})&\dots&\psi_N(\mathbf{x_N})
  \end{array}
  \right|.
\label{HartreeFockDet}
\end{equation}

Here the variables $\mathbf{x_i}$ include the coordinates of 
spin and space. The wave-function is antisymmetric with respect to an
interchange of any two electrons, as required by the Pauli principle

\begin{equation*}
  \Psi(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_i}, \dots,
  \mathbf{x_j}, \dots \mathbf{x_N}) = -
  \Psi(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_j}, \dots,
  \mathbf{x_i}, \dots \mathbf{x_N}).
\end{equation*}

We rewrite the Hamiltonian

\begin{equation*}
  \hat{H} = -\sum_{i=1}^N \frac{1}{2} \nabla^2_i 
  - \sum_{i=1}^N \frac{Z}{r_i} + \sum_{i<j}^N \frac{1}{r_{ij}}
\end{equation*}

as

\begin{equation}
    \hat{H} = \hat{H_1} + \hat{H_2} 
    = \sum_{i=1}^N\hat{h_i} + \sum_{i<j=1}^N\frac{1}{r_{ij}},
\label{H1H2}
\end{equation}

where

\begin{equation}
  \hat{h_i} = - \frac{1}{2} \nabla^2_i - \frac{Z}{r_i}.
\label{hi}
\end{equation}

The first term of eq.~(\ref{H1H2}), $H_1$, is the sum of the $N$
identical \emph{one-body} Hamiltonians $\hat{h_i}$. Each individual
Hamiltonian $\hat{h_i}$ contains the kinetic energy operator of an
electron and its potential energy due to the attraction of the
nucleus. The second term, $H_2$, is the sum of the $N(N-1)/2$
\emph{two-body} interactions between each pair of electrons.
Let us denote the ground state energy by $E_0$. According to the
variational principle we have

\begin{equation*}
  E_0 \le E[\Phi] = \int \Phi^*\hat{H}\Phi d\mathbf{\tau}
\end{equation*}

where $\Phi$ is a trial function which we assume to be normalized

\begin{equation*}
  \int \Phi^*\Phi d\mathbf{\tau} = 1.
\end{equation*}

In the Hartree-Fock method the trial function is the Slater
determinant of eq.~(\ref{HartreeFockDet}) which may be recast as

\begin{equation}
  \Psi = \frac{1}{\sqrt{N!}}\sum_{P} (-)^PP\psi_1(\mathbf{x_1})
    \psi_2(\mathbf{x_2})\dots\psi_N(\mathbf{x_N})=\sqrt{N!}{\cal A}\Phi_H,
\label{HartreeFockPermutation}
\end{equation}

with the \emph{anti-symmetry} operator given by a summation
of the different permutations

\begin{equation}
  {\cal A} = \frac{1}{N!}\sum_{P} (-)^PP,
\label{antiSymmetryOperator}
\end{equation}

and where the Hartree-function is given by

\begin{equation*}
  \Phi_H =
  \psi_1(\mathbf{x_1})\psi_2(\mathbf{x_2})\dots\psi_N(\mathbf{x_N}).
\end{equation*}

Both $\hat{H_1}$ and $\hat{H_2}$ are invariant under electron
permutations, and hence commute with ${\cal A}$

\begin{equation}
  [H_1,{\cal A}] = [H_2,{\cal A}] = 0.
  \label{cummutionAntiSym}
\end{equation}

Furthermore, ${\cal A}$ satisfies

\begin{equation}
  {\cal A}^2 = {\cal A},
  \label{AntiSymSquared}
\end{equation}

since every (either positive or negative) permutation of the Slater
determinant reproduces it. The expectation value of $\hat{H_1}$ 

\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_1}{\cal A}\Phi_H d\mathbf{\tau}
\end{equation*}

is readily reduced to

\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{H_1}{\cal A}\Phi_H d\mathbf{\tau},
\end{equation*}

where we have used eqs.~(\ref{cummutionAntiSym}) and
(\ref{AntiSymSquared}). The next step is to replace the anti-symmetry
operator by its definition eq.~(\ref{HartreeFockPermutation}) and to
replace $\hat{H_1}$ with the sum of one-body operators

\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \sum_{P} (-)^P\int 
  \Phi_H^*\hat{h_i}P\Phi_H d\mathbf{\tau}.
\end{equation*}

The integral vanishes if two or more electrons are permuted in only one
of the Hartree-functions $\Phi_H$ because the individual orbitals are
orthogonal. We obtain then

\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \int \Phi_H^*\hat{h_i}\Phi_H  d\mathbf{\tau}.
\end{equation*}

Orthogonality allows us to further simplify the integral, and we
arrive at the following expression for the expectation values of the
sum of one-body Hamiltonians 

\begin{equation}
  \int \Phi^*\hat{H_1}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{x_i})\hat{h_i}\psi_{\mu}
  d\mathbf{x_i}(\mathbf{x_i}).
  \label{H1Expectation}
\end{equation}

The expectation value of the two-body Hamiltonian may be obtained in a
similar manner. We have

\begin{equation*}
  \int \Phi^*\hat{H_2}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_2}{\cal A}\Phi_H d\mathbf{\tau},
\end{equation*}

which reduces to

\begin{equation*}
 \int \Phi^*\hat{H_2}\Phi d\mathbf{\tau} 
  = \sum_{i\le j=1}^N \sum_{P} (-)^P\int 
  \Phi_H^*\frac{1}{r_{ij}}P\Phi_H d\mathbf{\tau},
\end{equation*}

by following the same arguments as for the one-body
Hamiltonian. Because of the inter-electronic distances permutations of
two electrons no longer vanish, and we get

\begin{equation*}
  \int \Phi^*\hat{H_2}\Phi d\mathbf{\tau} 
  = \sum_{i < j=1}^N \int  
  \Phi_H^*\frac{1}{r_{ij}}(1-P_{ij})\Phi_H d\mathbf{\tau}.
\end{equation*}

where $P_{ij}$ is the permutation operator that interchanges
electrons $i$ and $j$. Again we use the assumption that the orbitals
are orthogonal, 
and obtain

\begin{equation}
\begin{split}
  \int \Phi^*\hat{H_2}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
    &\left[ \int \psi_{\mu}^*(\mathbf{x_i})\psi_{\nu}^*(\mathbf{x_j})\frac{1} 
    {r_{ij}}\psi_{\mu}(\mathbf{x_i})\psi_{\nu}(\mathbf{x_j})
    d\mathbf{x_i}\mathbf{x_j} \right.\\
  &\left.
  - \int \psi_{\mu}^*(\mathbf{x_i})\psi_{\nu}^*(\mathbf{x_j})
  \frac{1}{r_{ij}}\psi_{\nu}(\mathbf{x_i})\psi_{\mu}(\mathbf{x_j})
  d\mathbf{x_i}\mathbf{x_j}
  \right]. \label{H2Expectation}
\end{split}
\end{equation}

Here the fraction $1/2$ is introduced because we now run over
all pairs twice. Combining eq.~(\ref{H1Expectation}) and
(\ref{H2Expectation}) we have the functional

\begin{equation}
  E[\Phi] 
  = \sum_{\mu=1}^N \int \psi_{\mu}^*\hat{h_i}\psi_{\mu} d\mathbf{x_i} + 
  \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \left[ \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1} 
  {r_{ij}}\psi_{\mu}\psi_{\nu} d(\mathbf{x_ix_j})- \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1}{r_{ij}}\psi_{\nu}\psi_{\mu}
  d(\mathbf{x_ix_j}) \right]. 
\label{FunctionalEPhi}
\end{equation}

Having obtained the functional $E[\Phi]$, we now proceed to the second
step of the calculation. The functional is stationary with respect to
variations of the spin orbitals, subject to the $N^2$ conditions
imposed by the orthogonality requirements,

\begin{equation*}
  \int \psi_{\mu}(\mathbf{x})\psi_{\nu}(\mathbf{x}) d\mathbf{x} = 
  \delta_{\mu,\nu}
\end{equation*}

To satisfy these
conditions, we introduce $N^2$ Lagrange multipliers which we denote by 
$\epsilon_{\mu\nu}$. The variational equation then reads

\begin{equation}
  \delta E - \sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \epsilon_{\mu\nu} \delta
  \int \psi_{\mu}^* \psi_{\nu} = 0.
\label{variationalHFfull}
\end{equation}

For the orthogonal $\psi_{\mu}$ this reduces to

\begin{equation}
  \delta E - \sum_{{\mu}=1}^N \epsilon_{\mu} \delta
  \int \psi_{\mu}^* \psi_{\mu} = 0.
\label{variationalHF}
\end{equation}

Variation with respect to the spin-orbitals $\psi_{\mu}$ yields 

\begin{equation*}
\begin{split}
  \sum_{\mu=1}^N \int \delta\psi_{\mu}^*\hat{h_i}\psi_{\mu}
  d\mathbf{x_i}  
  + \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \left[ \int
  \delta\psi_{\mu}^*\psi_{\nu}^*\frac{1} 
  {r_{ij}}\psi_{\mu}\psi_{\nu} d(\mathbf{x_ix_j})- \int
  \delta\psi_{\mu}^*\psi_{\nu}^*\frac{1}{r_{ij}}\psi_{\nu}\psi_{\mu}
  d(\mathbf{x_ix_j}) \right] & \\
  + \sum_{\mu=1}^N \int \psi_{\mu}^*\hat{h_i}\delta\psi_{\mu}
  d\mathbf{x_i} 
  + \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \left[ \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1} 
  {r_{ij}}\delta\psi_{\mu}\psi_{\nu} d(\mathbf{x_ix_j})- \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1}{r_{ij}}\psi_{\nu}\delta\psi_{\mu}
  d(\mathbf{x_ix_j}) \right] & \\
  -  \sum_{{\mu}=1}^N E_{\mu} \int \delta\psi_{\mu}^*
  \psi_{\mu}d\mathbf{x_i} 
  -  \sum_{{\mu}=1}^N E_{\mu} \int \psi_{\mu}^*
  \delta\psi_{\mu}d\mathbf{x_i} & = 0.
\end{split}
\end{equation*}

Although the variations $\delta\psi$ and $\delta\psi^*$ are not
independent, they may in fact be treated as such, so that the 
terms dependent on either $\delta\psi$ and $\delta\psi^*$ individually 
may be set equal to zero. To see this, simply 
replace the arbitrary variation $\delta\psi$ by $i\delta\psi$, so that
$\delta\psi^*$ is replaced by $-i\delta\psi^*$, and combine the two
equations. We thus arrive at the Hartree-Fock equations

\begin{equation}
  \begin{split}
    \left[ -\frac{1}{2}\nabla_i^2-\frac{Z}{r_i} + \sum_{{\nu}=1}^N
      \int \psi_{\nu}^*(\mathbf{x_j})\frac{1}{r_{ij}}
      \psi_{\nu}(\mathbf{x_j})d\mathbf{x_j} \right]
    \psi_{\mu}(\mathbf{x_i})  & \\
    - \left[ \sum_{{\nu}=1}^N \int
      \psi_{\nu}^*(\mathbf{x_j}) 
      \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{x_j}) d\mathbf{x_j}
      \right] \psi_{\nu}(\mathbf{x_i})  & 
  = \epsilon_{\mu} \psi_{\mu}(\mathbf{x_i}).
  \end{split}
\label{HartreeFock}
\end{equation}

Notice that the integration $\int d\mathbf{x_j}$ implies an
integration over the spatial coordinates $\mathbf{r_j}$ and a summation
over the spin-coordinate of electron $j$.
\newline
%
\newline
The two first terms are the one-body kinetic energy and the
electron-nucleus potential. The third or
\emph{direct} term is the averaged electronic repulsion of the other
electrons. This term is identical to the Coulomb integral introduced in
the simple perturbative approach to the helium atom. As written, the
term includes the 'self-interaction' of 
electrons when $i=j$. The self-interaction is cancelled in the fourth
term, or the \emph{exchange} term. The exchange term results from our
inclusion of the Pauli principle and the assumed determinantal form of
the wave-function. The effect of exchange is for electrons of
like-spin to avoid each other.  A theoretically convenient form of the
Hartree-Fock equation is to regard the direct and exchange operator
defined through 

\begin{equation*}
  V_{\mu}^{d}(\mathbf{x_i}) = \int \psi_{\mu}^*(\mathbf{x_j}) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{x_j}) d\mathbf{x_j}
\end{equation*}

and

\begin{equation*}
  V_{\mu}^{ex}(\mathbf{x_i}) g(\mathbf{x_i}) 
  = \left(\int \psi_{\mu}^*(\mathbf{x_j}) 
  \frac{1}{r_{ij}}g(\mathbf{x_j}) d\mathbf{x_j}
  \right)\psi_{\mu}(\mathbf{x_i}),
\end{equation*}

respectively. The function $g(\mathbf{x_i})$ is an arbitrary function,
and by the substitution $g(\mathbf{x_i}) = \psi_{\nu}(\mathbf{x_i})$
we get

\begin{equation*}
  V_{\mu}^{ex}(\mathbf{x_i}) \psi_{\nu}(\mathbf{x_i}) 
  = \left(\int \psi_{\mu}^*(\mathbf{x_j}) 
  \frac{1}{r_{ij}}\psi_{\nu}(\mathbf{x_j})
  d\mathbf{x_j}\right)\psi_{\mu}(\mathbf{x_i}).
\label{modifiedHF}
\end{equation*}

We may then rewrite the Hartree-Fock equations as

\begin{equation}
  H_i^{HF} \psi_{\nu}(\mathbf{x_j}) = \epsilon_{\nu}\psi_{\nu}(\mathbf{x_i}),
\label{modifiedHF}
\end{equation}

with

\begin{equation}
  H_i^{HF}= h_i + \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{x_i}) -
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{x_i}),
\label{HFoperator}
\end{equation}

and where $h_i$ is defined by equation (\ref{hi}). 


%*************** Solving the Hartree-Fock Equations ***************
\subsection{Solving the Hartree-Fock Equations}
\label{SolvingHartree-Fock}

Both the direct and exchange terms of equation (\ref{HartreeFock})
depend on the orbitals we want to find. We need the solutions
to generate the two terms. This problem is usually solved by
iteration. Start by 
some initial guess for the orbitals.  Then the direct and exchange
terms are computed using these fixed guesses, and the orbitals are
solved for one at a
time. These solutions are used to recalculate the direct and the
exchange 
terms, and the HF equations are solved to find a new set of
spin-orbitals. This cycle is 
carried out until the solutions form a \emph{self-consistent field}
(SCF). Convergence problems are sometimes encountered but they are
usually not a problem in most calculations.
\newline
%
\newline
\emph{The Fock operator} of eq.~(\ref{HartreeFock}) depends on
the $N$ occupied spin-orbitals. Once these orbitals have been
determined, the Fock operator can be treated as a well-defined
hermitian operator. As for any hermitian operator there is an infinite
number of eigenfunctions of the Fock operator. In other words, there
is an infinite number of spin-orbitals $\psi_{\mu}$, each having an
eigenvalue $E_{\mu}$. In practice a finite number $k\ge N$ of spin
orbitals are used.
\newline
%
\newline
The $N$ lowest energy spin-orbitals are called the 
\emph{occupied orbitals}, and the remaining $k-N$ orbitals are called
the \emph{virtual orbitals}. The Slater determinant composed of the
occupied orbitals is the Hartree-Fock ground-state wave-function of the
system, and we shall denote it $\Phi_0$.
\newline
%
\newline
For closed shell atoms it is natural to consider the spin-orbitals as
paired. For example, two $1s$ orbitals with different spin have the same
spatial wave-function, but orthogonal spin functions. For open-shell
atoms two procedures are commonly used; the 
\emph{restricted Hartree-Fock} (RHF) and 
\emph{unrestricted Hartree-Fock} (UHF). 
In RHF all the electrons except those occupying open-shell orbitals
are forced to occupy doubly occupied spatial orbitals, while in UHF all
orbitals are treated independently. The UHF, of course, yields a lower
variational energy than the RHF formalism. One disadvantage of the
UHF over the RHF, is that whereas the RHF wave function is an
eigenfunction of $S^2$, the UHF function is not; that is, the
total spin angular momentum is not a well-defined quantity for a UHL
wave-function. Here we limit our attention to closed shell RHF's,
and show how the coupled HF equations may be turned into a matrix
problem by expressing the spin-orbitals using known sets of basis
functions.
\newline
%
\newline
We expand the orbitals $\psi_{\mu}$ by $M$ basis-functions
$\theta_{j}$,

\begin{equation}
  \psi_{\mu}(\mathbf{x}) = \sum_{j=1}^M
  c_{j,\mu}\theta_{j}(\mathbf{x}).
\label{basisFunctionExpansion}
\end{equation}

where $c_{j,\mu}$ are the unknown coefficients. From a set of $M$
basis function, we can obtain $M$ independent orbital wave-functions,
and the problem of calculating the wave-functions has now been
transformed to one of computing the coefficients $c_{j,\mu}$.
We insert the expanded wave-function of
eq.~(\ref{basisFunctionExpansion}) into the HF equation,
eq.~(\ref{modifiedHF}), and get

\begin{equation*}
  H_i^{HF} \sum_{j=1}^M c_{j,\mu}\theta_{j}(\mathbf{x}) 
  = \epsilon_{\mu}\sum_{j=1}^M c_{j,\mu}\theta_{j}(\mathbf{x}).
\end{equation*}

Multiplication of both sides with $\theta_{i}(\mathbf{x})$ and
integration over $d\mathbf{x}$ yields

\begin{equation*}
  \sum_{j=1}^M c_{j,\mu} \int \theta_{i}(\mathbf{x}) H_j^{HF}
  \theta_{j}(\mathbf{x}) d\mathbf{x} 
  = \epsilon_{\mu} \sum_{j=1}^M c_{j,\mu} 
  \int \theta_{i}(\mathbf{x}) \theta_{j} (\mathbf{x}) d\mathbf{x}. 
\end{equation*}

The set of Hartree-Fock equations have now been turned into a matrix
equation. This is seen by defining the \emph{overlap matrix},
$\mathbf{S}$, with elements

\begin{equation*}
  S_{ij} = \int \theta_{i}(\mathbf{x})
  \theta_{j}(\mathbf{x}) d\mathbf{x},
\end{equation*}

and the \emph{Fock matrix}, $\mathbf{F}$, with elements

\begin{equation*}
  F_{ij} = \int \theta_{i}(\mathbf{x}) H_j^{HF}
  \theta_{j}(\mathbf{x}) d\mathbf{x}.
\end{equation*}

Further, defining the matrix $\mathbf{c}$ corresponding to the $c_{i,\mu}$ and
the diagonal matrix $\mathbf{E}$ of the orbital energies
$\epsilon_{\mu}$ we get the matrix equation

\begin{equation*}
  \mathbf{F}\mathbf{c} = \mathbf{S}\mathbf{c}\mathbf{E},
\end{equation*}

know as the \emph{Roothaan equations} (ref. \cite{atkins2003}). The
elements of the Fock matrix 
depend on the orbital wave-functions, and as before we must
apply a self-consistent field approach.
\newline
%
\newline
In principle, a complete set of basis functions must be used to
represent spin-orbitals exactly, but this is not computationally
feasible. A given finite set of basis functions is, due to the
incompleteness of the basis set, associated with 
a \emph{basis-set truncation error}. The limiting HF energy, with
truncation error equal to zero, will be referred to as the
\emph{Hartree-Fock limit}.
\newline
%
\newline
The computational time depends on the number of
basis-functions and of the difficulty in computing the integrals of
both the Fock matrix and the overlap matrix. Therefore we wish to keep
the number of basis functions as low as possible and choose the
basis-functions cleverly. By cleverly we mean that the
truncation error should be kept as low as possible, and that the
computation of the matrix elements of both the overlap and the Fock
matrices should not be too time consuming.
\newline
%
\newline
One choice of basis functions are \emph{Slater type orbitals} (STO)
(ref. \cite{atkins2003}),

\begin{equation}
  \Psi_{nlm_l}(r,\theta,\phi) = {\cal N}r^{n_{_{eff}}-1}
  e^{\frac{Z_{_{eff}}\rho}{n_{_{eff}}}}Y_{lm_l}(\theta,\phi).
\label{STO}
\end{equation}

Here ${\cal N}$ is a normalization constant that for the purpose of
basis set expansion may be put into the unknown $c_{i\mu}$'s,
$Y_{lm_l}$ is a spherical harmonic (see table \ref{sphericalHaromical})
and $\rho=r/a_0$. \footnote{In atomic units $a_0=1$ so that $\rho=r$.}

The normalization constant of the spherical harmonics may of course
also be put into the expansion coefficients $c_{i\mu}$. The effective
principal quantum number $n_{_{eff}}$ is related to the true principal
quantum number $N$ by the following mapping (ref. \cite{atkins2003})

\begin{equation*}
  n\to n_{_{eff}}:1\to1\phantom{aa} 2\to2\phantom{aa} 3\to3\phantom{aa}
  4\to3.7\phantom{aa} 5\to4.0\phantom{aa} 6\to4.2.
\end{equation*}

The effective atomic number $Z_{_{eff}}$ for the ground state orbitals
of some neutral ground-state atoms are listed in table \ref{Zeff}. The
values in table \ref{Zeff} have been constructed by fitting STOs to
numerically computed wave-functions \cite{clementi1963}.
\newline

\begin{table}[hbtp]
\begin{center} {\large \bf Effective Atomic Number} \\ 
$\phantom{a}$ \\
\begin{tabular}{lllllllll}
\hline\\
  & H     &       &       &       &       &       &       & He    \\
1s& 1     &       &       &       &       &       &       & 1.6875\\
  & Li    & Be    & B     & C     & N     & O     & F     & Ne    \\
1s& 2.6906& 3.6848& 4.6795& 5.6727& 6.6651& 7.6579& 8.6501& 9.6421\\
2s& 1.2792& 1.9120& 2.5762& 3.2166& 3.8474& 4.4916& 5.1276& 5.7584\\
2p&       &       & 2.4214& 3.1358& 3.8340& 4.4532& 5.1000& 5.7584\\
  & Na    & Mg    & Al    & Si    & P     & S     & Cl    & Ar    \\
1s&10.6259&11.6089&12.5910&13.5754&14.5578&15.5409&16.5239&17.5075\\
2s& 6.5714& 7.3920& 8.2136& 9.0200& 9.8250&10.6288&11.4304&12.2304\\
2p& 6.8018& 7.8258& 8.9634& 9.9450&10.9612&11.9770&12.9932&14.0082\\
3s& 2.5074& 3.3075& 4.1172& 4.9032& 5.6418& 6.3669& 7.0683& 7.7568\\
3p&       &       & 4.0656& 4.2852& 4.8864& 5.4819& 6.1161& 6.7641\\ [10pt]
\hline
\end{tabular} 
\end{center}
\caption{Values of $Z_{_{eff}}$ for neutral ground-state atoms
  \cite{clementi1963}.}
\label{Zeff}
\end{table}

We will in this thesis limit our attention to STO basis functions, as
these basis-functions are well suited for atoms.  

It should be mentioned however, that depending on the system studied
improvements can be made with regard to the basis-set truncation error
by choosing different sets of basis-functions; for example 
\emph{Gaussian-type orbitals} (GTO), \emph{double-zeta basis set}
(DZ), \emph{triple-zeta basis set} (TZ) or several other basis sets.


%**************** Density Functional Theory ***************
\subsection{Density Functional Theory}

In the different HF methods one works with large basis sets. This
poses a problem for large systems. An alternative to the HF methods is
\emph{density functional theory} (DFT) \cite{atkins2003}. DFT takes into 
account electron correlations but is less demanding computationally
than for example CI and MP2.
\newline
%
\newline
The electronic energy $E$ is said to be a \emph{functional} of the
electronic density, $E[\rho]$, in the sense that for a given function
$\rho(r)$, there is a single corresponding energy. The  
\emph{Hohenberg-Kohn theorem} \cite{hohenberg1964} confirms that such
a functional exists, but does not tell us the form of the
functional. As shown by Kohn and Sham, the exact ground-state energy
$E$ of an $N$-electron system can be written as

\begin{equation*}
  E[\rho] = -\frac{1}{2} \sum_{i=1}^N\int
  \Psi_i^*(\mathbf{r_1})\nabla_1^2 \Psi_i(\mathbf{r_1}) d\mathbf{r_1}
  - \int \frac{Z}{r_1} \rho(\mathbf{r_1}) d\mathbf{r_1} +
  \frac{1}{2} \int\frac{\rho(\mathbf{r_1})\rho(\mathbf{r_2})}{r_{12}}
  d\mathbf{r_1}d\mathbf{r_2} + E_{XC}[\rho]
\end{equation*}

with $\Psi_i$ the \emph{Kohn-Sham} (KS) \emph{orbitals}. The
ground-state charge density is given by

\begin{equation*}
  \rho(\mathbf{r}) = \sum_{i=1}^N|\Psi_i(\mathbf{r})|^2, 
  %\label{}
\end{equation*}

where the sum is over the occupied Kohn-Sham orbitals. The last term,
$E_{EX}[\rho]$, is the \emph{exchange-correlation energy} which in
theory takes into account all non-classical electron-electron
interaction. However, we do not know how to obtain this term exactly,
and are forced to approximate it. The KS orbitals are found by solving
the \emph{Kohn-Sham equations}, which can be found by applying a
variational principle to the electronic energy $E[\rho]$. This approach
is similar to the one used for obtaining the HF equation in section
\ref{HartreeFockTheory}. The KS equations reads

\begin{equation*}
  \left\{ -\frac{1}{2}\nabla_1^2 - \frac{Z}{r_1} + \int 
  \frac{\rho(\mathbf{r_2})}{r_{12}} d\mathbf{r_2} +
  V_{XC}(\mathbf{r_1}) \right\} \Psi_i(\mathbf{r_1}) =
  \epsilon_i \Psi_i(\mathbf{r_1})
  %\label{}
\end{equation*}

where $\epsilon_i$ are the KS orbital energies, and where the 
\emph{exchange-correlation potential} is given by

\begin{equation*}
  V_{XC}[\rho] = \frac{\delta E_{XC}[\rho]}{\delta \rho}.
  %\label{}
\end{equation*}

The KS equations are solved in a self-consistent fashion. A variety of
basis set functions  can be used, and the experience gained in HF
calculations are often useful. The computational time needed for a DFT
calculation formally scales as the third power of the number of basis
functions. 
\newline
%
\newline
The main source of error in DFT usually arises from the approximate
nature of $E_{XC}$. In the \emph{local density approximation} (LDA) it
is approximated as

\begin{equation*}
  E_{XC} = \int \rho(\mathbf{r})\epsilon_{XC}[\rho(\mathbf{r})]
  d\mathbf{r},
  %\label{}
\end{equation*}

where $\epsilon_{XC}[\rho(\mathbf{r})]$ is the exchange-correlation
energy per electron in a homogeneous electron gas of constant density.
The LDA approach is clearly an approximation as the charge is not
continuously distributed. To account for the inhomogeneity of the
electron density, a nonlocal correction involving the gradient of
$\rho$ is often added to the exchange-correlation energy.



%******************** Many-Body Methods *******************
%*
%*
\section{Many-Body Methods}

Hartree-Fock theory, by assuming a single-determinant form of the
wave-function, neglects correlation between electrons. The electrons
are subject to an \emph{average} non-local potential arising from the
other electrons. This can lead to a poor description of the
electronic structure; it does not take into account the
\emph{instantaneous} electrostatic interaction between the
electrons. A great deal of work in the field of electronic
structure calculation aims at including such correlation.
\newline
%
\newline
The HF method yields a finite set of spin-orbitals when a finite basis
set expansion is used. In general, a set of $M$ basis functions
results in $2M$ different spin-orbitals. The $N$ occupied orbitals are
used to form the HF ground state, while there remain $2M-N$ virtual
orbitals. By using the single determinantal wave-function $\Phi_0$ as
a reference, it is possible to classify all other determinants
according to how many electrons have been promoted from occupied
orbitals to virtual orbitals. A \emph{singly excited determinant}
corresponds to one for which a single electron is excited, a
\emph{doubly excited determinant} corresponds to one for which two
electrons are excited, etc. Each of the determinants, or a linear
combination of a small number of them constructed so as to have the
correct symmetry, is called a \emph{configuration state function}
(CSF). These excited CSFs can be taken to approximate excited-state
wave-functions, or they can be used in a linear combination with
$\Phi_0$ to improve the ground (or any excited) state. 


%***************** Configuration Interaction ****************
\subsection{Configuration Interaction}

In \emph{configuration interaction} (CI) \cite{atkins2003} calculations,
the ground- or excited-state wave-function is represented as a linear
combination of Slater determinants. The method is flexible and give
highly accurate wave-functions for small systems, and can be used to
describe complex electronic-structure problems. The principal
shortcoming of the CI method is that it does not provide a compact
description of electron correlation and has a large growth in the
number of configurations needed for large systems
(ref. \cite{helgaker2002}). From a complete set of spin-orbitals we
can write the exact electronic wave-function $\Psi$ for any state of
the system in the form 

\begin{equation}
  \Psi = C_0\Phi_0 + \sum_{a,i} C_a^i\Phi_a^i + \sum_{ab,ij}
  C_{ab}^{ij}\Phi_{ab}^{ij} + \sum_{abc,ijk}
  C_{abc}^{ij}k\Phi_{abc}^{ijk} + \dots 
\label{fullCI}
\end{equation}

where the C's are expansion coefficients, and the $\Phi_a^i$'s are the
singly excited determinants, the $\Phi_{ab}^{ij}$'s are the doubly
excited determinants and so on; the electrons have been promoted from
$\phi_a$ to $\phi_i$, from $\phi_{a}$ and $\phi_b$ to $\phi_i$ and
$\phi_j$, and so on. A calculation is classified as a
\emph{full CI} if all CFS's of the appropriate symmetry are used for a
given basis set. As the number of Slater determinants to be determined
in a full CI is

\begin{equation*}
  \left( \begin{array}{c} 2M\\N \end{array} \right)
\end{equation*}

equation (\ref{fullCI}) must for most practical purposes be truncated.
This approach is referred to as the \emph{limited CI}; the state is
given as the linear combination

\begin{equation}
  \Psi_s = \sum_{I=1}^L C_{I,s}\Phi_I.
\label{truncatedCI}
\end{equation}
 
where the sum is over a finite number ($L$) of CSF's. 
The expansion coefficients are determined variationally by minimizing
the energy expectation value, or the so-called \emph{Rayleigh ratio},
ref. \cite{atkins2003},

\begin{equation*}
  \langle E \rangle = \frac{\int \Psi^* \hat{H} \Psi} {\int \Psi^*
  \Psi}. 
\end{equation*}

As in the HF approach we arrive at a matrix equation

\begin{equation*}
  \mathbf{H}\mathbf{C} = \mathbf{E}\mathbf{S}\mathbf{C}
\end{equation*}

with

\begin{equation*}
  H_{IJ} = \int \Psi_I^* \hat{H} \Psi_J
\end{equation*}

and the overlap matrix defined through

\begin{equation*}
  S_{IJ} = \int \Psi_I^* \Psi_J.
\end{equation*}

In CI a wave-function is a linear combination of
Slater determinants. The orbitals in each determinant are again 
a linear combination of basis functions. This makes the evaluation of
the matrix-elements of $\mathbf{H}$ and $\mathbf{S}$ very
important in CI calculations.
\newline
%
\newline 
One deficiency that plagues limited CI calculations is the lack of 
\emph{size-consistency}. A method is size-consistent if the energy of
a many-electron system is proportional to the number of electrons $N$
in the limit $N \to \infty$. In particular, the energy of a system
$AB$ computed when the systems are far apart should equal the sum of
energies of the two systems $A$ and $B$ when computed separately.
\newline
%
\newline
In \emph{multireference configuration iteration} (MRCI), a set of
reference configurations is created, from which excited determinants are
formed for use in a CI calculation. For example, a MCSCF (section
\ref{MulticonfigurationMethods}) calculation
could be performed and a set of reference configurations composed of
those determinants having a coefficient greater than a given
threshold value. For each of the reference determinants, electrons are
moved from occupied to unoccupied orbitals to create more orbitals for
inclusion in the CI expansion of eq.~(\ref{truncatedCI}).




%********************** Coupled-Cluster *******************
\subsection{Coupled-Cluster}


The coupled-cluster (CC) method \cite{fulde1995} is one of the the
most important practical advances over the CI method. Although
non-variational, it resolves the problem of size extensivity, and is
very accurate. Of the different
many-body methods, Coupled-Cluster (CC) represents the most effective
method for atomic systems. In practice, the CC method is restricted to
systems that are dominated by a single electronic configuration, and
the CC wave-function is best regarded as providing an accurate
correction to the HF description, see ref. \cite{helgaker2002}.
\newline
%
\newline
The CC method assumes an exponential ansatz for the
wave-function

\begin{equation*}
  \Psi_{CC} = exp(\hat{T}\Psi_{HF})
\end{equation*}

where the coupled-cluster wave-function is given by an excitation
operator acting on a reference wave-function, usually the Hartree-Fock
determinant $\Psi_{HF} = D_0$. The operator $\hat{T}$ generates
k-fold excitations from a reference state

\begin{equation*}
  \hat{T} = \sum_k \hat{T}_k
\end{equation*}

so that, for example,

\begin{equation*}
  \hat{T}_2 \Psi_{HF} = \sum_{ij,ab} t_{ij}^{ab}D_{ij}^{ab}
\end{equation*}

with $D_{ij}^{ab}$ the excitation of the
occupied states $ij$ to the virtual states $ab$. The expansion
coefficients $t_{ij}^{ab}$ are determined self-consistently. A
'coupled-cluster doubles' wave-function is written as

\begin{equation*}
\begin{split}
  \Psi_{CCD} &= exp(\hat{T}_2)\Psi_{HF} = (1 + \hat{T}_2 +
  \frac{1}{2!}\hat{T}_2^2 + \dots)\Psi_{HF}\\
  &= D_0 + \sum_{ij,ab} t_{ij}^{ab}D_{ij}^{ab} 
  + \frac{1}{2} \sum_{ij,ab}\sum_{kl,cd} t_{ij}^{ab}t_{kl}^{cd}
  D_{ij}^{ab} D_{kl}^{cd} + \dots .
\end{split}
\end{equation*}

The CC expansion is usually terminated after all double excitations or
all quadruple excitations have been included. It can be shown
\cite{fulde1995} that this expansion is size consistent. By including
many terms in the expansion, CC methods are computationally very
expensive relative to HF calculations. Formally, CC singles and doubles
scale as the sixth power of the number of basis states included in
the expansion, and calculations including up to quadruple excitations
scale as the tenth power of the number of states. The key limitation
of the CC methods is the rapid increase in computational cost with
system size; scalings are to the sixth power of the number of
particles (or higher).



%****** Møller-Plesset Many-Body Perturbation Theory  ******
\subsection{M\o ller-Plesset Many-Body Perturbation Theory}

Perturbation theory (PT) provides an alternative approach to finding
the correlation energy. Perturbative methods are size-consistent, but
the energies are not in general upper bounds to the exact energy.
\newline
%
\newline
Perturbation theory applied to a system of many interacting particles
is generally called \emph{many-body perturbation theory}
(MBPT). In the method called 
\emph{M\o ller-Plesset Many-Body Perturbation Theory} (MPPT) the
zero-order Hamiltonian $H^{(0)}$ is taken as the sum of one-electron Fock
operators defined through equation (\ref{HFoperator}). 
\newline
%
\newline
The perturbation
$H^{(1)}$ is given by

\begin{equation*}
  H^{(1)} = \hat{H} - H^{(0)} = \hat{H} - \sum_{i=1}^N H_i^{HF}.
\end{equation*}

The HF energy $E_{HF}$ associated with the (normalized) ground-state
HF wave-function $\Phi_0$ is the expectation value

\begin{equation*}
  \langle\Phi_0|\hat{H}|\Phi_0\rangle 
  = \langle\Phi_0|H^{(0)} + H^{(1)}|\Phi_0\rangle.
\end{equation*}

If we define the zero-order energy

\begin{equation*}
  E^{(0)} = \langle\Phi_0|H^{(0)}|\Phi_0\rangle,
\end{equation*}

and the first-order correction

\begin{equation*}
  E^{(0)} = \langle\Phi_0|H^{(1)}|\Phi_0\rangle,
\end{equation*}

The HF energy becomes the sum of these

\begin{equation*}
  E^{HF} = E^{(0)} + E^{(1)}.
\end{equation*}

Therefore the first correction to the HF ground state energy is given
by the second order perturbation 

\begin{equation}
  E^{(2)} = \sum_{J\ne0} 
  \frac{\langle\Phi_0|H^{(1)}|\Phi_J\rangle
    \langle\Phi_J|H^{(1)}|\Phi_0\rangle}{E^{(0)}-E_J}.
\label{secondOrderPertubation}
\end{equation}

This equation is readily shown by considering the following set of
expansions:

\begin{equation*}
  \hat{H} = H^{(0)} + \lambda H^{(1)} + \lambda^2 H^{(2)} + \dots,
\end{equation*}

for the Hamiltonian, 

\begin{equation*}
  \Psi = \Psi_0^{(0)} + \lambda \Psi_0^{(1)} + \lambda^2 \Psi_0^{(2)} + \dots,
\end{equation*}

for the state, and finally

\begin{equation*}
  E = E_0^{(0)} + \lambda E_0^{(1)} + \lambda^2 E_0^{(2)} + \dots,
\end{equation*}

for the energy. Here $\lambda$ is introduced to keep track of
the order of the perturbation, and set equal to unity after the order
of the terms is worked out. We start by inserting the above
expansions into Schr\"odinger's equation

\begin{equation*}
  \hat{H}\Psi=E\Psi,
\end{equation*}

and rearrange the terms by the order of $\lambda$. The expressions of
the different orders of perturbation is then obtained by setting 
the coefficients of each power of $\lambda$ equal to zero, and by
integrating from the left with $\int \Phi_J$. This can be
done because, as of yet, $\lambda$ is arbitrary. In our particular
case $\hat{H} = H^{(0)} + \lambda H^{(1)}$ so the only remaining term
in the second order perturbation is the one given by equation
(\ref{secondOrderPertubation}).
\newline
%
\newline
Note that the following matrix elements are all zero

\begin{equation*}
  \langle\Phi_J|H_{HF}|\Phi_0\rangle = \int
  \Phi_J H_{HF} \Phi_0 d\mathbf{\tau} = 0.
\end{equation*}

This is because the $\Phi_J$'s are eigenfunctions of $H_{HF}$ and
hence orthogonal. Therefore, the following must be true 

\begin{equation*}
  \langle\Phi_J|H|\Phi_0\rangle = \langle\Phi_J|H^{(1)}|\Phi_0\rangle.
\end{equation*}

By \emph{Brillouin's theorem} \cite{atkins2003}, Hamiltonian matrix
elements between 
$\Psi_0$ and all singly excited states vanish, so as a first
approximation we keep the doubly excited states. An analysis of these
matrix elements yields the following expression

\begin{equation*}
  E^{(2)} = \frac{1}{4}\sum_{ab}^{occ}\sum_{pq}^{vir} 
  \frac{\langle ab||pq\rangle
    \langle pq||ab\rangle}
  {\epsilon_a+\epsilon_b-\epsilon_p-\epsilon_q},
  %\label{}
\end{equation*}

where

\begin{equation*}
  \langle ab||pq\rangle = 
  \int \phi_a^*(1)\phi_b^*(2)H^{(1)}\phi_p(1)\phi_a(2)
  d\mathbf{x_1} d\mathbf{x_2}
  - \int \phi_a^*(1)\phi_b^*(2)H^{(1)}\phi_p(2)\phi_a(1)
  d\mathbf{x_1} d\mathbf{x_2}
  %\label{}
\end{equation*}

with $\phi_a$, $\phi_b$ occupied spin-orbitals and $\phi_p$,
$\phi_q$ virtual spin-orbitals. The inclusion of the second-order
energy correlation is labeled MP2. Third and fourth order
corrections are referred to as MP3 and MP4. As one moves to higher
orders of perturbation, the algebra involved becomes more and more
complicated and it is common to use diagrammatic representation. These
diagrams can be used to prove that MPPT is
size-consistent in all orders.




%***** Multiconfiguration and Multireference Methods  *****
\subsection{Multiconfiguration Methods}
\label{MulticonfigurationMethods}

Electronic wave-functions are often dominated by more than one
electronic configuration, and the presence of several important
configurations poses a challenge for electronic structure theory. For
such systems CC and MP are not so well suited \cite{helgaker2002}; the
orbitals generated 
self-consistently in the field of a single electronic configuration may
have little relevance to a multiconfigurational system. 
In the \emph{multiconfiguration self-consistent field method} (MCSCF)
both the coefficients $C_{I,s}$ of equation (\ref{truncatedCI}) and the
coefficients $c_{j,\mu}$ of equation (\ref{basisFunctionExpansion})
are optimized. This simultaneous optimization of both the
orbitals and the CSF's expansion coefficients makes MCSCF
computationally demanding. However, accurate results can be obtained
with the inclusion of even a relatively small number of CSF's. The
development of effective MCSCF methods is still actively being pursued
and is particularly important for excited states.
\newline
%
\newline
One such scheme is the 
\emph{complete active-space self-consistent field method} (CASSCF). In
this approach the spin-orbitals are divided into three classes;
\emph{inactive}, \emph{virtual} and \emph{active} orbitals. The
inactive orbitals are the doubly occupied orbitals, the virtual
orbitals are the sets of very high energy spin-orbitals which are
unoccupied, and the active orbitals are the energetically intermediate
orbitals. The CSF's included in the CASSCF calculation are
configurations (of the appropriate symmetry and spin) that arise from
all possible ways of distributing the active electrons over the active
orbitals.





%********************* Monte Carlo Theory *******************
%
%
\section{Monte Carlo Theory}

In this thesis the Variational Monte Carlo (VMC) method is studied for
atomic systems.
A \emph{Monte Carlo} (MC) method is a method based on random
numbers. It is therefore stochastic and has associated statistical
properties. Monte Carlo methods are widely used, and are particularly
interesting for solving high-dimensional integrals, which is the case
for VMC. In this section we will outline the basics of MC methods, and
develop the theoretical description of the VMC routine. Also, we will
look at the basic principles of the Diffusion Monte Carlo (DMC) method.
Other methods like Path Integral Monte Carlo, Auxiliary Field
Monte Carlo and Reptation Monte Carlo have become popular over the last
decade, but are omitted in this presentation.
\newline
%
\newline
The VMC method is more effective when combined with DMC. This
combination has proved accurate for several quantum 
mechanical systems. The ultimate accuracy of the DMC is only limited
by the trial function. A common procedure is to use the VMC
algorithm to find an optimized trial wave-function, and use this
optimized form as  the input function for the DMC calculation.
\newline
%
\newline
Even though DMC calculations are not performed in this thesis, one
should keep in mind that VMC is used primarily as a starting point
for DMC calculations. Trial wave-functions are optimized through VMC
and are used as the input to a DMC calculation. The DMC method in
itself gives little insight in terms of understanding the physics
involved in the system, whereas our physical understanding is
incorporated into the variational trial wave-function. Therefore,
understanding how the VMC algorithm works and how to construct
trial-wave-functions is important before implementing the DMC
method. Furthermore, when developing the VMC code most of the key
building blocks of the DMC method are also created.
The purpose of this thesis is to develop the VMC algorithm. Our focus
is mainly to make the code fast and flexible. In addition, some of the
basic insights regarding the making of trial wave-functions will be
established. 


%******************** Monte Carlo Integration **********************
\subsection{Monte Carlo Integration}

To motivate for Monte Carlo integration we first take a look at
the conventional methods. Conventional integration methods involve
choosing some evaluation points and combine the value of the integrand
with weights for each and every point,

\begin{equation}
  \int_{\Omega} f(\mathbf{x}) d\Omega 
  = \sum_{i=1}^m \omega_i f(\mathbf{x}_i).
\label{traditionalIntegration}
\end{equation}

The value of the weights $\omega_i$ are associated with how the
evaluation points have been chosen. In one dimension the simplest form
of eq.~(\ref{traditionalIntegration}) is to choose the evaluation
points to be equally spaced. The weights then become the length of
the interval divided by the number of integration points, $m$. For
simplicity we assume that the integration interval has been
transformed to the unit length. This gives

\begin{equation*}
  \int_0^1 f(x) dx 
  = \frac{1}{m} \left(\sum_{i=1}^m f(x_i) 
  + {\cal O} (\frac{1}{m}) \right).
\end{equation*}

Similarly, a two dimensional integration on the unit square may be
approximated as

\begin{equation*}
  \int_0^1 \int_0^1 f(x,y) dx dy 
  = \frac{1}{m^2} \left( \sum_{i=1}^m \sum_{j=1}^m f(x_i, y_j) 
  + {\cal O} (\frac{1}{m}) \right),
\end{equation*}

where we have taken the number of integration points to be equal in
each of the two dimensions. For two dimensions we therefore need $N^2$
integration points instead of the $m$ points we needed in one
dimension to obtain the same order of accuracy. By the same argument,
integration over a $d$ dimensional unit cube needs $m^d$ integration
points to obtain an accuracy of order $1/m$. For a $d$ dimensional
quantum $N$-body systems we 
have $dN$ spatial degrees of freedom. The integral becomes an integral
over $dN$ dimensions, and we need $m^{dN}$ points to acquire an
accuracy of order $1/m$. Sophistication of the traditional methods is
obtained by choosing the points cleverly. By choosing more points
where the function varies more than where the function is smooth, the
number of integration points may be reduced while at the same time
preserving the accuracy. However, the number of calculations needed is
still of the order of $dN$. 
\newline
%
\newline
As can be seen from this argument the conventional integration
procedure is practically impossible to carry out as $N$ grows large.
This problem is avoided in the HF approach by solving the one-particle
mean-field HF equations. The problem is here reduced to $N$
$2d$-dimensional ones, where the factor two is due to the direct and
the exchange terms of the HF approximation.
Also, by choosing the basis functions cleverly the
evaluation of these integrals can be made an easy
task. Integral-evaluations in most other numerical many-body methods 
are similarly reduced to integrals over $d$ or $2d$ dimensions.
%, where $a$ is a small integer; usually $1$ or $2$.
\newline
%
\newline
The introduction of Monte Carlo integration makes high-dimensional
integration possible. Here the integrand is evaluated at random
points $\mathbf{x}_i$ taken from an arbitrary probability distribution 
$\rho(\mathbf{x})$ (ref. \cite{kent1999}),

\begin{equation}
  \int_{\Omega} f(\mathbf{x}) d\Omega 
  = \int_{\Omega} \frac{f(\mathbf{x})}{\rho(\mathbf{x})}
  \rho(\mathbf{x})d\Omega 
  \equiv \int_{\Omega} g(\mathbf{x}) \rho(\mathbf{x})d\Omega.
  = \sum_{i=1}^m g(\mathbf{x}_i) + {\cal O}(\frac{1}{\sqrt{m}}).
\label{MonteCarloIntegration}
\end{equation}

In the limit of $m\to \infty$ this approximation is exact, but in a
numerical approach we are forced to truncate the summation at some
finite value $m$.
\newline
%
\newline
By choosing $\rho(\mathbf{x})=1$ we simply sample the integrand
uniformly at freely chosen random points. If the function varies
considerably over the 
integration domain, the variation of the individual samples will be
considerable. The trick is therefore to choose the function
$\rho(\mathbf{x})$ to duplicate the behavior of the function
$f(\mathbf{x})$ we want to integrate. If we choose
$\rho(\mathbf{x})={\cal N}f(\mathbf{x})$ the fraction $g(\mathbf{x}) = 
f(\mathbf{x})/\rho(\mathbf{x})={\cal N}$ is simply a constant. This
means that we obtain the true value of the integral (here the
normalization constant ${\cal N}$) by taking only one sample of the
function $g(\mathbf{x})$. Of course this mean that we must know the
value ${\cal N}$ of the integral in advance, and this makes little
sense as this is the value we want to find. From a theoretical point
of view, however, this example illustrates how we can optimize the
Monte Carlo integration scheme by choosing the shape of the probability
distribution to be as close to the integrand $f(\mathbf{x})$ as
possible. This optimization routine is referred to as \emph{importance
  sampling}.
\newline
%
\newline
The true beauty of the Monte Carlo integration scheme is that it is
inherently independent of the number of dimensions. The 
evaluation time of the integrand depends solely on its functional
form. Also, the 
variance of the integral estimate depends solely on how much the
integrand varies. For a practical application to the quantum
mechanical $N$-body fermionic system, the evaluation of the
Slater determinant in the integrand yields approximately $N^3$
calculations. Furthermore, an additional factor $N^s$ is included
because, with increasing $N$, the trial wave-function (section
\ref{TheTrialWaveFunction}) becomes less
accurate and the auto-correlation effects (section
\ref{StatisticalAnalysis}) become increasingly significant. For
atomic systems the VMC routine is thus of the order $N^{3+s}$. We will
provide an example of this in section \ref{AtomicResults}.
\newline
%
\newline
In the case of atoms, the wave-function varies greatly
near the nucleus and is smoother further away. For eigenstates the
energy does not vary at all, since

\begin{equation*}
  \hat{H} \Psi_k = E_k \Psi_k.
\end{equation*}

Therefore, for the eigenstate the energy expectation value

\begin{equation*}
  \langle E \rangle  
  = \frac{\int \Psi^*(\mathbf{X}) \hat{H} \Psi(\mathbf{X}) d\mathbf{\tau}}
  {\int \Psi^*(\mathbf{X}) \Psi(\mathbf{X}) d\mathbf{\tau}},
\end{equation*}

depends only on $|\Psi(\mathbf{X})|^2$. This implies that
sampling random points from the $|\Psi(\mathbf{X})|^2$ 
distribution, is equivalent to sampling from the integrand
$\Psi^*(\mathbf{X}) \hat{H} \Psi(\mathbf{X})$. In Variational Monte
Carlo we sample from the square of a trial wave-function
$\Psi_{trial}(\mathbf{X})$, not the eigenstate. The
relation $\hat{H} \Psi = E \Psi$ no longer holds, but as the
trial wave-function limits the true eigenstate the variation of the
integrand vanishes.  The number of samples needed to obtain a given
accuracy depends therefore directly on the quality of the trial 
wave-function. 
\newline
%
\newline
Before we are ready to formulate the VMC routine we
need a routine to sample probability-distributions. 



%***************** The Metroplolis Algorithm ****************
\subsection{The Metropolis Algorithm}

The \emph{Metropolis algorithm} \cite{metropolis1953} generates a
stochastic or random sequence of phase space points that 
sample a given probability distribution. In Quantum Monte Carlo (QMC)
methods, each point in phase space is a vector $\mathbf{X} = \left\{
\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_N} \right\}$ in the
Hilbert space. Here $\mathbf{x_i}$ represents the spatial and the spin
coordinates of electron i. Each point in the phase space, coupled with
a quantum mechanical operator, can be associated with physical
quantities, such as the kinetic and potential energy. The 
sequence of individual \emph{samples} of these quantities can be
combined to arrive at average values which describe the quantum
mechanical state of the system. This is the fundamental idea behind
the Metropolis algorithm, and the algorithm is used to generate the
sample points. We will refer to the randomized walk through the phase
space as a \emph{random walk}. From an initial position in phase or
configuration space, a \emph{proposed move} is generated and the move
either \emph{accepted} or \emph{rejected} according to the Metropolis
algorithm. A random walk thus generates a sequence

\begin{equation*}
  \left\{ \mathbf{X}_0, \mathbf{X}_1, \dots, \mathbf{X}_i, \dots, \right\}
\end{equation*}

of points in the phase space. An important requirement is for the
random walk to be \emph{ergodic}, which means that all points in the
phase or configuration space can be reached from any initial point. By
taking a sufficient number of trial steps all of phase space is then
explored and the Metropolis algorithm ensures that the points are
distributed according to the required probability distribution.
\newline
%
\newline
Let us for the time being suppose that we know the probability
distribution $\rho(\mathbf{X})$ we are to draw the points
from. Metropolis \emph{et al} \cite{metropolis1953} showed that the
sampling is most easily accomplished if the points  $\mathbf{X}$ form
a \emph{Markov chain}. A random walk is Markovian if each point in the
chain depends only on the position of the preceding point. A Markov
process may be completely specified by choosing values 
of the transition probabilities $P(\mathbf{X}_n, \mathbf{X}_m)$ of
moving from $\mathbf{X}_n$ to $\mathbf{X}_m$. The Metropolis
algorithm works by choosing the transition probabilities in such a way
that the sequence of points generated by the random walk sample the
required probability distribution. 
To understand the Metropolis algorithm it is  necessary to work out
the statistical properties of the points on the Markov chain. This may
be done by considering a large ensemble of random walkers all evolving
simultaneously. The walkers all move step by step in accordance with
the transition probabilities. At a given time the number of
walkers at a point $\mathbf{X}_n$ is $N(\mathbf{X}_n, t)$. As
the Markov chains evolve in time the number of walkers develops
according to the Master equation, 

\begin{equation*}
  \frac{d}{dt}N(\mathbf{X}_n, t) = 
  -\sum_{\mathbf{X}_m} P(\mathbf{X}_n,\mathbf{X}_m)N(\mathbf{X}_n,t)
  +\sum_{\mathbf{X}_m} P(\mathbf{X}_m,\mathbf{X}_n)N(\mathbf{X}_m,t).
\end{equation*}

As $t\to\infty$ the derivative $dN(\mathbf{X}_n, t)/dt \to
0$ so that

\begin{equation*}
  \sum_{\mathbf{X}_m} P(\mathbf{X}_n,\mathbf{X}_m)N(\mathbf{X}_n)
  =\sum_{\mathbf{X}_m} P(\mathbf{X}_m,\mathbf{X}_n)N(\mathbf{X}_m),
\end{equation*}

where $N(\mathbf{X}_n, t) \to N(\mathbf{X}_n)$.
Metropolis realized that the distribution of walkers will settle down
to the required distribution $\rho(\mathbf{X})$ as long as the
transition probabilities obey the equation of \emph{detailed balance}

\begin{equation*}
  P(\mathbf{X}_n,\mathbf{X}_m)\rho(\mathbf{X}_n)
  = P(\mathbf{X}_m,\mathbf{X}_n)\rho(\mathbf{X}_m).
\end{equation*}

Imposing the condition of detailed balance is a sufficient but not a
necessary requirement for a random process to sample the phase space
with the probability density $\rho(\mathbf{X})$. 
Other transition probabilities can be created, and are particularly
interesting for reducing auto-correlation effects (see section
\ref{StatisticalAnalysis}). Such approaches have unfortunately not
been studied in this thesis. Imposing the condition of detailed
balance on the transition probabilities gives  

\begin{equation*}
  \sum_{\mathbf{X}_m} P(\mathbf{X}_n,\mathbf{X}_m)\left(
  N(\mathbf{X}_n) - \frac{\rho(\mathbf{X}_n)}{\rho(\mathbf{X}_m)}
  N(\mathbf{X}_m) \right)=0,
\end{equation*}

so that

\begin{equation*}
  \frac{\rho(\mathbf{X}_n)}{\rho(\mathbf{X}_m)} = 
  \frac{ N(\mathbf{X}_n)}{ N(\mathbf{X}_m)}.
\end{equation*}

This shows us that the number of walkers in the state $\mathbf{X}_n$
becomes proportional to the steady state distribution
$\rho(\mathbf{X}_n)$, which we wish to sample.
\newline
%
\newline
We still have some freedom in choosing the transition probabilities,
which are not uniquely defined by the detailed balance condition. In
the Metropolis approach, the walk is generated by starting from a
point $\mathbf{X}_n$ and making a \emph{trial move} to a new point
$\mathbf{X}_m$ somewhere nearby in phase space. The way we choose our
trial move is not crucial, except that it is important to satisfy the 
detailed balance condition. One such approach is to choose

\begin{equation*}
  P_{trial}(\mathbf{X}_n,\mathbf{X}_m) =
  P_{trial}(\mathbf{X}_m,\mathbf{X}_n),
\end{equation*}

and then  \emph{accepted} or \emph{rejected} according to the rule

\begin{equation*}
  P_{accept}(\mathbf{X}_n,\mathbf{X}_m) =
  min\left( 1, \frac{\rho(\mathbf{X}_m)}{\rho(\mathbf{X}_n)} \right).
\end{equation*}

Note that since this method involves the ratios of
probabilities there is no need to worry about normalization of the
distribution $\rho(\mathbf{X}_n)$. Combining the trial and acceptance
probabilities we get

\begin{equation*}
 \frac{P(\mathbf{X}_n,\mathbf{X}_m)}{P(\mathbf{X}_m,\mathbf{X}_n)} = 
 \frac
 {P_{trial}(\mathbf{X}_n,\mathbf{X}_m)P_{accept}(\mathbf{X}_n,\mathbf{X}_m)}
 {P_{trial}(\mathbf{X}_m,\mathbf{X}_n)P_{accept}(\mathbf{X}_m,\mathbf{X}_n)} 
 = \frac{\rho(\mathbf{X}_m)}{\rho(\mathbf{X}_n)}
\end{equation*}

and hence the detailed balance condition is satisfied.

%******************** Variational Monte Carlo **********************
\subsection{Variational Monte Carlo}

The Variational Monte Carlo (VMC) routine uses the Metropolis
algorithm combined with Monte Carlo integration and the variational
principle in finding the eigenstates of the system. Finding the
ground state energy of a many-body system of $N$ particles
in $d$ dimensions requires the minimization of the $dN$ dimensional
integral 

\begin{equation}
  \langle E \rangle = E[\Psi] 
  = \frac{\int \Psi^*(\mathbf{X}) \hat{H} \Psi(\mathbf{X}) d\mathbf{\tau}}
  {\int \Psi^*(\mathbf{X}) \Psi(\mathbf{X}) d\mathbf{\tau}}.
\label{EnergyExpectation}
\end{equation}

In general the energy will, according to the variational principle,
be a minimum for the exact ground state wave-function $\Psi_0$. The
functional $E[\Psi]$ thus provides an upper bound to the ground state
energy.
\newline
%
\newline
We rewrite eq.~(\ref{EnergyExpectation}) as

\begin{equation}
  \langle E \rangle 
  = \int E_L(\mathbf{X})
  \frac{|\Psi(\mathbf{X})|^2} {\int |\Psi(\mathbf{X})|^2 d\mathbf{\tau}} 
  d\mathbf{\tau},
\label{LocalEnergyExpectation}
\end{equation}

where the \emph{local energy} has been defined as

\begin{equation}
  E_L = \frac{1}{\Psi(\mathbf{X})} \hat{H} \Psi(\mathbf{X}).
\label{LocalEnergy}
\end{equation}

The square of the wave-function divided by its norm may be interpreted
as the probability distribution of the system of particles. So we
arrive at

\begin{equation}
  \langle E \rangle 
  = \int E_L(\mathbf{X}) \rho(\mathbf{X}) d\mathbf{\tau},
\label{LocalEnergyExpectationProbability}
\end{equation}

with

\begin{equation*}
  \rho(\mathbf{X}) = \frac{ |\Psi(\mathbf{X})|^2 }
      {\int |\Psi(\mathbf{X})|^2 d\mathbf{\tau}}.
\end{equation*}

This integral is carried out by Monte Carlo integration.
We move a walker randomly through the phase space according
to the Metropolis algorithm, and sample the local energy with each
move. This gives us a statistical evaluation of the integral.
\newline
%
\newline
The VMC algorithm consists of two distinct phases. In the first
\emph{thermalization} phase the walker is propagated by the
Metropolis algorithm, in order to equilibrate it according to the
probability distribution $\rho(\mathbf{X})$.\footnote{Remember that
  the Metropolis algorithm reproduces the probability distribution
  in the limit $t\to\infty$.} In the second phase, the walker
continues to be moved, but energies and other observables are also
\emph{sampled} for computation of averages and other statistical
quantities.
\newline

\begin{table}[hbtp]
\begin{center}\large{\bf{VMC algorithm}}\end{center}
\begin{tabular}{l}
\hline\\ 
\bf{\emph{Scheme for the VMC algorithm}}\\
\emph{Generate initial randomized electron configuration.} \\
loop (\emph{thermalization steps}) \\ \phantom{A}
 loop (\emph{all electrons}) \\ \phantom{AA}
  \emph{Propose move from $\mathbf{X}$ to $\mathbf{X'}$.} \\ \phantom{AA}
  \emph{Compute ratio $R = \vert \Psi(\mathbf{X'})/\Psi(\mathbf{X})
    \vert^2$.} \\ \phantom{AA}
  \emph{Accept or reject move according to the Metropolis probability
    min(1,R).} \\
\emph{Initialize samplers.} \\
loop (\emph{VMC steps}) \\ \phantom{A}
 loop (\emph{all electrons}) \\ \phantom{AA}
  \emph{Propose move from $\mathbf{X}$ to $\mathbf{X'}$.} \\ \phantom{AA}
  \emph{Compute ratio $R = \vert \Psi(\mathbf{X'})/\Psi(\mathbf{X})
    \vert^2$.} \\ \phantom{AA}
  \emph{Accept or reject move according to the Metropolis probability
    min(1,R).} \\ \phantom{AA}
  \emph{Sample the contributions to the local energy and other
    observables.} \\
\emph{Calculate and return statistical properties.} \\ [10pt]
\hline
\end{tabular}
\label{VMCalgorithm}
\end{table}

In this algorithm, the electrons are moved individually and not as a
whole configuration. This improves the efficiency of the algorithm for
larger systems, as full configuration-moves require increasingly small
steps to maintain the acceptance ratio.


%******************** Statistical Analysis **********************
\subsection{Statistical Analysis}
\label{StatisticalAnalysis}

The statistical analysis of the results produced through the VMC
routine needs special attention. First of all, a statistically
produced average or \emph{mean} has little or no value by itself, and
should always be given with its associated variance or 
\emph{standard deviation}. Second, in the practical application of the
VMC routine the individual samples are correlated\footnote{Do not
  confuse the statistical correlation with the correlation effects we
  optimize through the introduction of a Jastrow factor.}.
\newline
%
\newline
Given a sequence of normally distributed data points 
$\left\{ A_1, A_2, \dots, A_M \right\}$ the mean is given by

\begin{equation*}
  \bar{A} = \frac{1}{M} \sum_{i=1}^{M} A_i,
\end{equation*}

and the \emph{standard deviation} is given by

\begin{equation*}
  \sigma^2 = \frac{1}{M-1} \sum_{i=1}^{M} \sum_{j=1}^{M} 
    \left(A_i-\bar{A}\right) \left(A_j-\bar{A}\right).
\end{equation*}

The standard deviation can be split into two terms

\begin{equation}
  \sigma^2 = \frac{1}{M-1} \sum_{i=1}^{M}
    \left(A_i-\bar{A}\right)^2
  + \frac{ 2 }{M-1} \sum_{i=1}^{M-1}\sum_{j>i}^{M}
    \left(A_i-\bar{A}\right)\left(A_j-\bar{A}\right). 
\label{covariantStandardDeviation}
\end{equation}

The first term of eq.~(\ref{covariantStandardDeviation}) is the
un-correlated estimate of the variance, the second term is the
covariance. For un-correlated data this last term is zero, but for the
Markov chains produced by the Metropolis algorithm this is
not the case. Therefore correlation effects must be
included in our statistical analysis of the sample points. The
straightforward approach of using 
eq.~(\ref{covariantStandardDeviation}) directly is extremely
time-consuming when a large number of samples are included. The double
sum makes such calculations virtually impossible for practical
applications. One way to approach this problem is to split the double
sum of the covariance into $M-1$ single sums,

\begin{equation*}
  \begin{split}
  \sum_{i=1}^{M-1}\sum_{j>i}^{M}
  \left(A_i-\bar{A}\right)\left(A_j-\bar{A}\right) = 
  & \sum_{i=1}^{M-1}
  \left(A_i-\bar{A}\right)\left(A_{i+1}-\bar{A}\right) \\
  & + \sum_{i=1}^{M-2}
  \left(A_i-\bar{A}\right)\left(A_{i+2}-\bar{A}\right) \\
  & \dots \\
  & + \sum_{i=1}^{1}
  \left(A_i-\bar{A}\right)\left(A_{i+M-1}-\bar{A}\right).
  \end{split}
\end{equation*}

If we define

\begin{equation*}
  \tau_m \equiv \sum_{i=1}^{M-m}
  \left(A_i-\bar{A}\right)\left(A_{i+m}-\bar{A}\right)
\end{equation*}

we simply get

\begin{equation}
  \sum_{i=1}^{M-1}\sum_{j>i}^{M}
  \left(A_i-\bar{A}\right)\left(A_j-\bar{A}\right) =
  \sum_{m=1}^{M-1} \tau_m
\label{sumCorrelationLenghts}
\end{equation}

which is computed by considering samples that are
separated (in the chain of samples) by a distance $m$.
\newline
%
\newline
The value of $\tau_m$ reflects how much the samples separated by a
distance $m$ (in the chain of samples) are correlated. Given large
enough distances the correlation effects should eventually die out,
and $\tau_m$ should therefore be equal to zero for all $m \ge k$.
The value $k$ is called the \emph{correlation length}. We should by
this argument be able to truncate the sum in
eq.~(\ref{sumCorrelationLenghts}) at the value $k$. In theory this
seems promising, but in practice this approach is not well
suited. Because of the  statistical nature of the sample points, the
value of $\tau_m$ will not die out completely, but fluctuate around
zero. Truncation at different values of $m$ therefore yields different
estimates of the variance, and these estimates do not
relax to any given value even as $m >> k$. The fluctuations in
$\tau_m$ are too moderate for automated procedures to give reliable
estimates, and the estimation of $\tau_m$ therefore needs special care
for each individual data sets.
\newline
%
\newline
A simple procedure commonly used in the literature is the procedure known
as \emph{reblocking} or \emph{block analysis} (see for example
ref. \cite{kent1999}). This procedure is based on a reblocking of the
data into a series of blocks of varying sizes, and the standard
deviation can be obtained by computing the variance of the block
averages. For each block an estimate of the (un-correlated) standard
deviation of the mean is given by

\begin{equation*}
  \sigma_b^2 = \frac{1}{M_b-1} \sum_{j=1}^{M_b} (A_{b,j} -
  \bar{A})^2,
\end{equation*}

where $M_b$ is the number of blocks of size $b$, and $A_{b,j}$ is the
average of the samples in block $j$ (Note that the average remains
unchanged $\bar{A}_b = \bar{A}$). The idea behind the reblocking
scheme is that as the block size increases, the estimate of the standard
deviation will eventually relax to the true correlated standard
deviation. This can be seen by some simple manipulation of
eq.~(\ref{covariantStandardDeviation}). Start by defining the
un-correlated part as

\begin{equation*}
  \sigma_u^2 \equiv \frac{1}{M-1} \sum_{i=1}^{M}
    \left(A_i-\bar{A}\right)^2,
\end{equation*}

and define for short the covariance as

\begin{equation*}
  cov \equiv \frac{ 1 }{M-1} \sum_{i=1}^{M-1}\sum_{j>i}^{M}
    \left(A_i-\bar{A}\right)\left(A_j-\bar{A}\right).
\end{equation*}

Rewriting of eq.~(\ref{covariantStandardDeviation}) by the above
equations yield

\begin{equation}
  \sigma^2 = \sigma_u^2 \left( 1 + \frac{2}{\sigma_u^2}
  cov \right) \equiv \kappa \sigma_u^2,
\label{correlatedUnCorrelatedVariance}
\end{equation}

where $\kappa$ is the \emph{auto-correlation time}.
When the size of the blocks are longer than the correlation length
($b>>\tau_k$) the individual block samples $A_{b,j}$ become
un-correlated. When the blocks are un-correlated the auto-correlation
time is equal to unity, $\kappa = 1$. This procedure works much better
than the first procedure, as the fluctuations in $\tau_m$ are 'smeared
out' due to the block averages. However, fluctuations are experienced
in this approach also.
\newline
\begin{figure}[hbtp]
  \input{Monte_Carlo/autoCorrelationTimeScaleIllustration}
  \caption{Fictive one-dimensional wave-function included for
  illustrative purposes.
  }
  \label{autoCorrelationTimeScaleIllustration2}
\end{figure}

Another problem that may occur is auto-correlation
effects on different time scales. Consider the one-dimensional
trial wave-function depicted in figure 
\ref{autoCorrelationTimeScaleIllustration2}. 
In this example the instantaneous evaluation of different physical
quantities may vary in the two regions $A$ and
$B$. Random movement between the two regions is unlikely by making
only short steps in the Metropolis algorithm, but it will
eventually happen. This results in an auto-correlation that becomes
visible only on large time scales.



%********************** Kato Cusp Conditions ***********************
\subsection{Kato Cusp Conditions}

An important physical feature in quantum mechanics are the so-called
\emph{cusp conditions}, see ref. \cite{kato1957}. When
two particles are close in proximity the 
Coulomb forces between these two particles become dominant, and
the influence of all other particles become unimportant. This fact
should be incorporated into the wave-function, so that the diverging
Coulomb potential is cancelled by a corresponding divergence in the
kinetic energy.  This constraint on the wave-function is
\emph{local} and is a constraint on the derivatives of the
wave-function.
\newline
%
\newline
To examine the cusp condition we return our attention to the
radial part of the hydrogenic wave-function (given by
eq.~\ref{hydrogenRadialEquation} with $V(r) = -Z/r$),

\begin{equation}
  \left[ -\frac{1}{2} \frac{1}{r^2}\frac{\partial}{\partial r} 
    \left( r^2 \frac{\partial}{\partial r}\right)  
    - \frac{Z}{r} + \frac{l(l+1)}{r^2} \right]
  R(r) = E R(r).
\label{hydrogenLikeRadialEquation}
\end{equation}

For $l=0$ the two $1/r$ terms must cancel, which leads to the cusp
condition

\begin{equation*}
  \left.\frac{dR(r)}{dr}\right\vert_{r=0} = -Z R(r)\vert _{r=0}.
\end{equation*}

For $l\ne 0$ the angular momentum term must be canceled by the kinetic
energy. By factoring out the leading $r$ dependency, as we did when
introducing the solid harmonics in section \ref{TheHydrogenAtom},
namely 

\begin{equation*}
  R_{nl}(r) = r^l {\cal R}_{nl}(r),
\end{equation*}

the kinetic energy term becomes

\begin{equation}
  r^l {\cal R}''(r) + \frac{2(l+1)}{r} r^l{\cal R}'(r) +
  \frac{l(l+1)}{r^2}r^l{\cal R}(r).
\label{AngularElectronNuclearCusp}
\end{equation}

We see that the $1/r^2$ term in eq.~\ref{hydrogenLikeRadialEquation}
is cancelled by the last term of eq.~\ref{AngularElectronNuclearCusp},
yielding the following equation for ${\cal R}(r)$

\begin{equation*}
  \left( \frac{2(l+1)}{r}\frac{{\cal R}'(r)}{{\cal R}(r)} +
  \frac{2Z}{r}+\frac{{\cal R}''(r)}{{\cal R}(r)} + 2E \right) r^l{\cal R}(r) = 0 
\end{equation*}

By equating the $1/r$ terms we arrive at the general electron-nucleus
cusp condition

\begin{equation} 
  \left.\frac{d{\cal R}(r)}{dr}\right\vert_{r=0} = -\frac{Z}{l+1}
  {\cal R}(r)\vert _{r=0}.
\label{katoCuspNucleus}
\end{equation}

For hydrogenic systems the Kato cusp condition uniquely determines the
overall exponential behavior of the wave-function for each value of $l$,
$R(l=0) \propto e^{-Zr}$, $R(l=1) \propto e^{-Zr/2}$, $R(l=2) \propto
e^{-Zr/3}$, etc., which is in accordance with table
\ref{hydrogenRadialFunctions} of section \ref{TheHydrogenAtom}.
\newline
%
\newline
The electron-electron cusp can be derived by a similar argument. In
this case, both electrons contribute to the kinetic energy. Expanding
the wave-functions in spherical coordinates centered on electron $i$,
leads to (ref. \cite{hammond1994})

\begin{equation*}
  \left(
  2\frac{d^2}{dr_{ij}^2} + \frac{4}{r_{ij}}\frac{d}{dr_{ij}}
  +\frac{2}{r_{ij}} - \frac{l(l+1)}{r_{ij}^2} + 2E
  \right) R_{ij}(r_{ij}) = 0,
\end{equation*}

which gives the electron-electron cusp condition

\begin{equation*} 
  \left.\frac{d{\cal R}(r_{ij})}{dr_{ij}}\right\vert_{r_{ij}=0} = \frac{1}{2(l+1)}
  {\cal R}(r_{ij})\vert _{r_{ij}=0}.
\end{equation*}

For electrons with anti-parallel spin the most likely configuration is
the energetically lowest one, in which $l=0$. This implies a
cusp condition equal to $1/2$. Two electrons cannot both occupy the
same state by the Pauli principle. For two electrons with parallel spin 
the energetically lowest configuration is therefore for $l=1$. We
arrive at the general electron-electron cusp condition

\begin{equation*} 
  \left.\frac{d{\cal R}(r_{ij})}{dr_{ij}}\right\vert_{r_{ij}=0} = \left\{ 
  \begin{split} 1/2 &\text{ for } spin(i) \ne spin(j) \\ 
                1/4 &\text{ for } spin(i) = spin(j) \end{split} \right..
\label{katoCuspElectron}
\end{equation*}

The hydrogen-like cusp conditions apply to the many-electron
system. As one single electron approaches the nucleus or another
electron, the exact wave-function behaves asymptotically to the
hydrogenic wave-function. Higher order cusp conditions are of course
also possible. As two electrons are close to the nucleus
simultaneously, an extension to the above picture 
applies. The theoretical derivation of such higher-order terms
is omitted here.



%********************* The Trial Wave-Function *********************
\subsection{The Trial Wave-Function}
\label{TheTrialWaveFunction}

The development of Slater determinants is outlined in the beginning of
this chapter. Even though both HF and DFT are able to generate
good single-electron orbitals, the approximations introduced by both of
these methods result in a poor description of the instantaneous
structure of the system. In the many-body methods these approximations
are accounted for by generating linear combinations of several Slater
determinants. In the limit of including an infinite number of Slater
determinants these methods are exact. These methods are limited by
basis set truncation errors and slow convergences.
\newline
%
\newline
Variational approaches are limited by how well the variational or
\emph{trial wave-function} approximates the behavior of the true
eigenstate, and by the method used to optimize the 
parameters of the trial function. Most variational methods rely on a
double basis set expansion in one-electron orbitals, combined into one
or more $N$-electron Slater determinants.  A unique characteristics of
Monte Carlo methods is their ability to use arbitrary wave-function
forms, thereby enabling treatment beyond the Slater determinants
constructed solely with one-electron functions.
\newline
%
\newline
In the literature, see for example refs. \cite{kent1999,hammond1994},
the common approach is to begin with a Slater  
determinant. The Slater determinant is then multiplied with a
variational \emph{Jastrow-factor}, $G_{\beta}$. This Jastrow factor is
constructed to include two-body and higher order correlation
effects. To obtain even better results, the Slater 
determinant is substituted with a linear-combination of several Slater
determinants. This latter approach is not implemented in the code
developed in this thesis, but is a natural extension of the existing
code. For now we limit our attention to single determinants,
$D_{\alpha}$. This results in a trial wave-function of the form

\begin{equation*} 
  \Psi_{\alpha,\beta}(\mathbf{X}) = D_{\alpha}(\mathbf{X})
  G_{\beta}(\mathbf{X}),
\end{equation*}

where $\alpha = \left\{ \alpha_i \right\}$ and $\beta = \left\{
\beta_i \right\}$ are the variational parameters.
\newline
%
\newline
In this thesis we have used two forms of the Slater determinant. The
first form which consisted of variational hydrogen orbitals was easy
to implement and therefore provided a practical means for testing the
code as it developed. The radial functions listed in table
\ref{hydrogenRadialFunctions} were used, with the charge $Z$
exchanged with a variational parameter $\alpha$, and combined with the
solid harmonics of table \ref{solidHarmonics}; for example
$\phi_{2p_{m=-1}}(\mathbf{r}_i) = y_i e^{-\alpha r_i}$. The
determinant used for producing results, however, was composed of
Slater Type Orbitals (STO) optimized through restricted Hartree-Fock
(RHF), see ref. \cite{clementi1974}, also combined with the solid
harmonics. The HF energies were accurately reproduced by the VMC code
of this thesis, providing an excellent test of the Slater-dependent
part of our code.
\newline
%
\newline
Several forms of the Jastrow-factor exist in the literature, and we
will mention only a selected few to give the general idea of how they
are constructed. A form given by Hylleraas (taken from
ref. \cite{hammond1994}) for the helium atom is

\begin{equation*}
  G_{\beta} = e^{-\epsilon s}
  \sum_{\mu} c_{\mu} r^l_{\mu} s^{m_{\mu}} t^{n_{\mu}},
\end{equation*}

where $r$ the inter-electronic distance, $s = r_1 + r_2$ and $t =
r_1 -r_2$ . The convergence of the Hylleraas function is quite slow,
but for small systems, like that of helium above, excellent results can
be obtained. The Pad\'{e}-Jastrow factor, however, is more suited for
larger systems. It is constructed to include both electron-electron
and electron-nucleus correlations and takes an exponential ansatz,
namely $e^{U}$, with, see ref. \cite{hammond1994},

\begin{equation*}
  U(r_i, r_{ij}) = \sum_{i=1}^N \left(
  \frac{\sum\limits_{k=1}^{m_{\alpha}} \alpha_k r_i^k} 
  {1+\sum\limits_{k=1}^{m_{\alpha}} \alpha_k' r_i^k} 
  \right)+
  \sum_{j>i=1}^N \left( 
  \frac{\sum\limits_{k=1}^{m_{\beta}}
  \Delta_{ij}\beta_k r_{ij}^k}
  {1+\sum\limits_{k=1}^{m_{\beta}} \beta_k' r_{ij}^k} 
  \right).
\end{equation*}

Here the parameter $\alpha_1$ describes the behavior as $r_i$ limits
zero, the different $\alpha_k$ and $\alpha_k'$ describe the
behavior in the intermediate distances, and the ratio
$\alpha_{m_{\alpha}}/\alpha_{m_{\alpha}}'$ the long distance
limit. The different $\beta_k$ and $\beta_k'$ similarly describe the 
electron-electron correlations. 
\newline
%
\newline
The Pad\'{e}-Jastrow factor must be constructed to include the
cusp-conditions described in the previous section. Failing to
incorporate these conditions would lead to divergences in the short
distance limits, and optimizing the parameters will therefore give a
poor result. The $\Delta_{ij}$ term is included to satisfy the
different cusp conditions for parallel or anti-parallel spin electrons,

\begin{equation*}
  \Delta_{ij} = \left\{ \begin{split} 
    1/2 &\text{ for } spin(i) \ne spin(j) \\
    1/4 &\text{ for } spin(i) = spin(j)
  \end{split} \right..
\end{equation*}

Another Jastrow-factor frequently encountered in the literature is
the Schmidt-Moskowitz function $e^{U}$, with $U$ given by
(ref. \cite{schmidt1990})

\begin{equation*}
  U_{ij} = \sum_{k} \Delta(m_k,n_k)c_k(\bar{r}_i^{m_k}\bar{r}_j^{n_k}+
  \bar{r}_j^{m_k}\bar{r}_i^{n_k})\bar{r}_{ij}^{o_k}.
\end{equation*}

Here $\bar{r} = r/(1 + \alpha r)$, $c_k$ are the expansion coefficients,
$m_k$, $n_k$ and $o_k$ are integers that may be set to
include electron-electron ($m_k=n_k=0$, $o_k \ge 1$),
electron-nucleus (either $m_k\ne 0$ or $n_k\ne 0$, or both $m_k,n_k\ne
0$ , and $o_k = 0$) and finally electron-electron-nucleus correlations
(either $m_k\ne 0$ or $n_k\ne 0$, or both $m_k,n_k\ne 0$ and $o_k \ne
0$). To maintain consistency with Boys and Handy,
ref. \cite{hammond1994}, the term, ref. \cite{schmidt1990},

\begin{equation*}
  \Delta(m_k,n_k) = \left\{ 
  \begin{split} 1 & \text{, for } m \ne n \\ 
    1/2 & \text{, for } m = n \end{split}  
  \right.,
\end{equation*} 

is included. Furthermore, to satisfy the 
electron-electron cusp condition for unlike spins, the only term with
$o = 1$ is with both $m=n=0$. Similarly, to satisfy the nuclear
cusp-condition, additional terms with $n=1$ or $m=1$ are not included.


%********************* Efficiency *******************
%
%
\subsection{Efficiency}

The efficiency, $\eta$, of a MC integration process is inversely
proportional to the time, $\tau_{s.e.}$, it takes to obtain a result
with a given standard error, $s.e.$,

\begin{equation*}
  \eta \propto \frac{1}{\tau_{s.e.}}.
\end{equation*}

The amount of time spent in obtaining a given accuracy is the
product of the time it takes to perform one MC cycle, $\tau_{MC}$,
with the number of MC steps needed, $N_{s.e.}$

\begin{equation*}
  \tau_{s.e.} = N_{s.e.} \tau_{MC}.
\end{equation*}

The standard error of an MC estimate is given by

\begin{equation*}
  s.e. = \frac{\sigma}{\sqrt{M}},
\end{equation*}

where $M$ is the number of samples. Therefore, the number of steps
$N_{s.e.}$ needed is again proportional to the square of the variance
of the integrand, namely 

\begin{equation*}
  N_{s.e.} \propto \sigma^2.
\end{equation*}

Finally, the variance is related to the un-correlated variance,
$\sigma_u^2$, obtained through the Metropolis algorithm, and the
auto-correlation time $\kappa$, through
eq.~(\ref{correlatedUnCorrelatedVariance}). We arrive at an efficiency
proportional to

\begin{equation}
  \eta \propto \frac{1}{\tau_{MC}  \kappa \sigma_u^2}
\end{equation}

Therefore, there are three ways to improve the efficiency of the VMC 
integration: (i) reduce the time it takes to perform one MC
cycle, (ii) reduce auto-correlation effects and (iii) reducing the
variance. 
\newline
%
\newline
The first way to improve the efficiency (i) was discussed in sections
\ref{OptimizingTheSlaterDeterminant} and
\ref{OptimizingTheCorrelation}. The bottle-neck of each MC cycle 
is the evaluation of the wave-function and both its gradient and its
Laplacian. The main concern here was the Slater determinant, but as the 
energy and variance optimization schemes were introduced, efficient
evaluation of the Jastrow-factor became increasingly important as
well. For larger systems like for example molecules, some of the
one-particle orbitals may have negligible overlap. Therefore given an 
instantaneous electronic configuration, the negligible parts of the
Slater determinant may be identified and removed from the calculation
of the wave-function and its derivatives. Also, if the Jastrow-factor
is given only as function of the distances between the different
particles, a requirement is that with increasing distance the
corresponding terms in the Jastrow factor should approach a
constant. These terms will not contribute to the local energy because
both the first and second derivatives of these terms are almost
zero. Such an approach could therefore, for large molecules and
similar systems, result in a method  that is linear with respect to
the evaluation of the integrand, without violating the Pauli
principle.
\newline
%
\newline
The second way to improve the efficiency (ii) is to reduce the
auto-correlation effects. If the individual samples are correlated, it
implies that overhead calculations are performed without providing
additional information about the fluctuations of the
integrand. Reducing the auto-correlations is therefore essential in
the further development of the program code.
\newline
%
\newline
The third way to improve the efficiency (iii) regards choosing good
trial-wave function. The better the wave-function represents the true
eigenstate, the lower the variance. The trial wave-function must both
include the local cusp conditions of two-body or higher interaction as
well as the global trends of the eigenfunction.
\newline
%
\newline
In addition, trial wave-function optimization is essential
when regarding the overall efficiency of the VMC method, not just the
efficiency of the integration process. In the next two sections,
approaches for optimizing trial wave-functions are studied.



%***************** Energy and Variance Optimization ****************
\subsection{Energy and Variance Optimization}
\label{EnergyAndVarianceOptimization}

We will here discuss two approaches for optimizing the trial
wave-function. When optimizing we seek the parameter configuration
of the trial wave-function $\Psi_T$ that best approximates the behavior
of the true eigenfunction $\Psi_n$. For the true ground state $\Psi_0$
a natural choice is to optimize with respect to energy
minimization. This scheme, which is in accordance with the variational
principle, is known as \emph{energy optimization}. Optimization with
respect to variance is another natural choice in that for every
eigenfunction, not just the ground state, the variance of the local
energy vanishes. This means that a \emph{variance optimization} scheme
could be applied in finding any eigenstate, because we know
in advance that the value of the variance should be zero. 
\newline
%
\newline
The energy optimization scheme is based on the variational principle
(introduced in section \ref{TheHeliumAtom}),

\begin{equation*} 
  \langle E_L \rangle = \frac{\int \Psi_{\mathbf{\alpha}}^* \hat{H}
  \Psi_{\mathbf{\alpha}} d\tau}{\int \vert
  \Psi_{\mathbf{\alpha}}\vert^2 d\tau} \ge E_0.
\end{equation*}

Here the fact that the variational energy provides an upper
bound to the ground state energy is exploited. The parameters
$\alpha = \{ \alpha_1, \alpha_2, \dots \}$, which correspond to a
global energy minimum, are chosen as 
the best fit to the true ground state wave-function. A drawback in
this seemingly brilliant scheme, is instability problems. In
section \ref{TestingTheCode} an example where the energy optimization
fails is given. 
\newline
%
\newline
In the literature there is a consensus that the variance
optimization scheme provides the most stable approach, and a
thorough investigation is given by Kent, ref. \cite{kent1999}.
%The true value of the un-correlated variance is given by 
%
%\begin{equation*}
%  \sigma_t^2 = \int \left(E_{L} - E_t \right)^2 d\tau,
%\end{equation*}
%
%where $E_t$ is the true mean of the local energy. 
The straightforward estimate of the un-correlated
  variance\footnote{The auto-correlation time $\kappa$ is a
  constant, so optimization with respect to either the variance
  $\sigma^2 = \kappa \sigma_u^2$ or the un-correlated variance
  $\sigma_u^2$ are equivalent.} is 

\begin{equation*}
  \sigma_u^2 = \frac{1}{M-1} \sum_{i=1}^{M}
    \left(E_{L,i} - \langle E_L \rangle \right)^2,
%\label{uncorrelatedVariance}
\end{equation*}

where $\langle E_L \rangle$ is the average of the individual samples
$E_{L,i}$. Many have chosen instead to optimize,

\begin{equation}
  \sigma_d^2 = \frac{1}{M-1} \sum_{i=1}^{M}
    \left(E_{L,i} - E_{ref} \right)^2,
\label{varianceEstimateReferenceEnergy}
\end{equation}

where $E_{ref}$ is taken to be as close to the expected average of the
local energy as possible.


%The above estimate, eq.~(\ref{uncorrelatedVariance}),
%depend on the value of the mean $\langle E_L \rangle$. To provide a
%good estimate of the variance the estimate of the mean must also be
%good. To avoid this problem, many workes have chosen instead to
%optimize,
%
%\begin{equation*}
%  \sigma_d^2 = \frac{1}{M-1} \sum_{i=1}^{M}
%    \left(E_{L,i} - E_{ref} \right)^2.
%\end{equation*}
%
%By taking the reference energy $E_{ref}$ to be as close to the true
%mean of the energy 
%By taking the reference energy to be $E_{ref} = E_t + \delta E$
%Substituting
%the estimate of the mean $\langle E_L \rangle$ with a reference energy
%$E_{ref}$, allows accumulation of induvidual samples of the variance
%during the VMC algorithm is performed. If $E_{ref}$ is taken to be a
%little below the expected estimate of the energy $\langle E_L \rangle$, the
%variance estimate actually optimize with respect to both the energy
%and the variance simultaneously. We have
%\begin{equation*}
%  \sigma_{ref}^2 = \frac{1}{M-1} \sum_{i=1}^{M}
%    \left(E_{L,i} - \langle E_L \rangle + \delta E \right)^2 ,
%\label{}
%\end{equation*}
%
%which equals
%
%\begin{equation*}
%  \sigma_{ref}^2 = \frac{1}{M-1} \sum_{i=1}^{M} (E_{L,i} -
%  \langle E_L \rangle)^2 - 2  \delta E (E_{L,i} - \langle E_L
%  \rangle) + \delta E^2
%\end{equation*}
%
%\begin{equation*}
%  \sigma_{ref}^2= \sigma_u^2 - 2 \delta E \langle E_L \rangle +
%    \frac{M}{M-1}\delta E^2
%\end{equation*}




%******************* Correlated Sampling ********************
\subsection{Correlated Sampling}
\label{CorrelatedSampling}

Wave-function optimization is one of the most critical, time consuming
and important stages of a VMC calculation. In VMC calculations, the
accuracy of the trial wave-function limits the statistical efficiency
of the calculation and the final accuracy of the result
obtained. Therefore, several variational parameters are put into the
trial wave-function. As more and more parameters are put into the
wave-function the accuracy needed to obtain statistically significant
improvements becomes more demanding and time-consuming. We 
wish of course to limit the number of parameters by choosing the trial 
functions as wisely as possible, but as the systems grow larger the
number of parameters needed is increasing.
\newline
%
\newline
The straightforward approach to optimize the parameters numerically,
is to use well established statistical tools to fit a surface to
a set of data-points chosen by the user. The minimum of the surface 
can then be obtained. This procedure, however, is not very
efficient. First, the data points are statistical and we therefore
need several (or a few very accurate) data points to be able to
significantly pinpoint a parameter minimum. Further, we must choose
the shape of the surface. Close to the minimum, a parabolic surface
would be a good approximation, but as we do not know where the minimum
is we must use intuition and insight to choose the shape of the
surface. We want a procedure that is fast and able to localize the
minimum without much effort. Therefore, we have incorporated an
optimizing procedure commonly used in the literature known as
\emph{correlated sampling}. Introduction of \emph{guiding functions},
$\Psi_{\mathbf{\alpha'}}$, allows the same random walk to produce several
\emph{local} estimates of the integral,


\begin{equation*}
  \langle E \rangle_{\mathbf{\alpha'}} 
  = \frac  {\int|\Psi_{\mathbf{\alpha'}}(\mathbf{X})|^2
  E_L^{\mathbf{\alpha'}} (\mathbf{X})d\mathbf{\tau}} 
  {\int|\Psi_{\mathbf{\alpha'}} (\mathbf{X})|^2d\mathbf{\tau} }.
\end{equation*}

Each of these local estimates of the energy $\langle E
\rangle_{\mathbf{\alpha'}}$ must be in the neighborhood of the
\emph{central} parameter set $\mathbf{\alpha}$ in parameter space. 
By the central parameter set we mean the set that produces the
random walk by means of the Metropolis algorithm. Multiplication of

\begin{equation*}
  1 = \frac{|\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2}
  {|\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2}
\end{equation*}

inside the integrals of both the numerator and the determinator yields

\begin{equation*}
  \langle E \rangle_{\mathbf{\alpha'}} 
  = \frac  {\int\omega_{\alpha,\alpha'}(\mathbf{X})
    E_L^{\mathbf{\alpha'}} (\mathbf{X})
    |\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2 
    d\mathbf{\tau}
  } 
  {
    \int
    \omega_{\alpha,\alpha'}(\mathbf{X})
    |\Psi_{\mathbf{\alpha} }(\mathbf{X})|^2
    d\mathbf{\tau} }, 
\end{equation*}

with

\begin{equation*}
  \omega_{\alpha,\alpha'}(\mathbf{X}) =
  \frac{|\Psi_{\mathbf{\alpha'}}(\mathbf{X})|^2}
       {|\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2}.
\end{equation*}

By dividing with the norm,

\begin{equation*}
  {\cal N}_{\alpha} 
  = \int|\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2 d\mathbf{\tau},
\end{equation*}

in both the numerator and the determinator we have

\begin{equation*}
  \langle E \rangle_{\mathbf{\alpha'}} 
  = \frac  {\int\omega_{\alpha,\alpha'}(\mathbf{X})
    E_L^{\mathbf{\alpha'}} (\mathbf{X})
    \rho_{\mathbf{\alpha}}(\mathbf{X})
    d\mathbf{\tau}
  } 
  {
    \int
    \omega_{\alpha,\alpha'}(\mathbf{X})
    \rho_{\mathbf{\alpha}}(\mathbf{X})
    d\mathbf{\tau} },
\end{equation*}

with

\begin{equation}
  \rho_{\mathbf{\alpha}}(\mathbf{X}) = 
  \frac{|\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2}
       {\int |\Psi_{\mathbf{\alpha}}(\mathbf{X})|^2 d\mathbf{\tau}}.
\label{probabilityDistribution}
\end{equation}

Here $\rho_{\mathbf{\alpha}}(\mathbf{X})$ is the probability
distribution of the central parameter set. The random walk of the
central parameter set may therefore be used to generate estimates of
several local variations in parameter space. We arrive at

\begin{equation*}
  \langle E \rangle_{\mathbf{\alpha'}} 
  \approx \frac  {
    \sum\limits_{i=1}^{M} \omega_{\alpha,\alpha'}(\mathbf{X}_i)
    E_L^{\mathbf{\alpha'}} (\mathbf{X}_i)
  } 
  {
    \sum\limits_{i=1}^{M} \omega_{\alpha,\alpha'}(\mathbf{X}_i)
  },
\end{equation*}


where the sample points are taken from the distribution
$\rho_{\mathbf{\alpha}}(\mathbf{X})$ given by
eq.~(\ref{probabilityDistribution}). 
\newline
%
\newline
This approach, in theory, looks very promising, but in fact it poses a
few problems. The weights $\omega_{\alpha,\alpha'}$ may vary by several
orders of magnitude, especially close to the nodes. The sample points
generated by the Metropolis algorithm depends only on the central
wave-function. If the value of the central wave-function is small
compared to the local wave-function, it implies that the value of the
weight becomes large. This manifests itself near the nodes due to
lack of more complicated many-body correlations. This could lead to a
few sample points dominating the estimate of the 
integral. These few dominant points may give really poor estimates of
for example the energy, as the trial wave-functions fail to
cancel divergent terms. Also, if the nodes of the local variation do
not coincide with the nodes of the central wave-function
we may actually allow sampling at the nodes!
\newline
%
\newline
Nevertheless, the introduction of guiding functions allows a fast
and effective routine for optimizing the wave-function. 
A thorough investigation of the numerical instabilities induced by the
introduction of guiding functions is given by Kent in
ref. \cite{kent1999}. In this thesis Kent argues the use of
variance optimization with the weights set to unity. This reduces the
instability due to the few dominating weights, without changing the
optimized parameters considerably. An example is given in section
\ref{TestingTheCode}.



%******************* Diffusion Monte Carlo ******************
\subsection{Diffusion Monte Carlo}


Diffusion Monte Carlo (DMC) is a method of solving for the ground
state of the many-body Schr\"odinger equation. In principle DMC could
solve this equation exactly, but in practice it must be approximated.  
The DMC method is based on rewriting the Schr\"odinger equation in
imaginary time by defining $\tau=it$ (ref.~\cite{kent1999}),

\begin{equation}
   \frac{\partial \Psi}{\partial \tau}= -\hat{H}\Psi.
\label{imaginaryTimeSchrodingerEquation}
\end{equation}

As there is no time-dependency\footnote{We still seek the stationary
  solution, so the imaginary equations above has nothing to do with
  the 'real' time propagation of the wave-function.} in the
Hamiltonian, a formal solution to the imaginary time Schr\"odinger
equation is

\begin{equation*} 
   \Psi(\tau+\delta\tau)=e^{-\hat{H}\delta\tau}\Psi(\tau),
\end{equation*}

where the state  $\Psi$ evolves from an imaginary time
$\tau$ to a later time $\tau+\delta\tau$. By expanding the initial
state $\Psi(\tau)$ in the true eigenstates $\Phi_i$ the final
state becomes

\begin{equation*}
  \Psi(\tau + \delta\tau) =
  \sum_i^{\infty} c_i e^{-\epsilon_i\delta\tau}\Phi_i,
\end{equation*}

with $\epsilon_i$ the eigenenergies and $c_i$ the expansion
coefficients. Over time the term with the lowest
eigenvalue will dominate. Hence, any initial state $\Psi$ that is not
orthogonal to the ground state $\Phi_0$, will evolve to the ground
state, that is 

\begin{equation}
  \lim_{\delta\tau\rightarrow\infty}\Psi(\tau + \delta\tau) =
  c_0 e^{-\epsilon_0\tau}\Phi_0. 
\label{longTimeLimit}
\end{equation}

In the DMC method the imaginary time evolution results in excited
states decaying exponentially fast, whereas in the VMC method they
remain. The ground state of eq.~(\ref{longTimeLimit}) also
decays exponentially fast, and this problem is solved by introducing a
constant offset to the energy $E_T \approx \epsilon_0$ in the
Hamiltonian.
If the Hamiltonian is separated into the kinetic energy and potential
energy terms, the imaginary time Schr\"odinger equation takes a form
similar to a diffusion equation, 

\begin{equation}
   \frac{\partial \Psi(\mathbf{X}, \tau)}{\partial \tau}=
    \left[\sum_i^{N}\frac{1}{2}\nabla^2_i\Psi(\mathbf{X}, \tau)\right]
    +(E_T - V(\mathbf{X}))\Psi(\mathbf{X}, \tau).
    \label{diffusionBrancingEquation}
\end{equation}

By considering a population of walkers $\mathbf{X}_i$ at a given time
$\tau$, where $\Psi(\mathbf{X}_i, \tau)$ is the density of walkers at
$\mathbf{X}_i$, the interpretation of
eq.~(\ref{diffusionBrancingEquation}) becomes clear. The first term on
the right hand side is simply a diffusion term that tends to drive the
walkers from dense areas to less populated areas. The second term
describes a \emph{branching} process. A branching process induces a
growth of walkers when positive and a decay of walkers when negative.
The method given by eq.~(\ref{diffusionBrancingEquation}) is in
theory exact, but it leads to a very inefficient 
algorithm. The Coulomb potential $V(\mathbf{X})$ is unbounded  
and therefore the rate term $E_T - V(\mathbf{X})$ can
diverge. This results in a very large fluctuation of walkers, and gives
large statistical errors.
\newline
%
\newline
Importance sampling is a way of efficiently reducing these large
fluctuations, and is essential for a DMC simulation to be efficient. 
A trial or guiding wave function $\Psi_T(\mathbf{X})$, which closely
approximates the ground state wave function, is introduced.
The optimized trial wave-function of VMC calculations are typically
used in this respect. A new distribution $f(\mathbf{X}, \tau)$ is
defined as 

\begin{equation}
  f(\mathbf{X}, \tau) \equiv \Psi_T(\mathbf{X})\Psi(\mathbf{X},
  \tau). 
  \label{DMCdistribution}
\end{equation}

This new distribution is a solution of the modified Schr\"odinger
equation

\begin{equation}
  \frac{\partial f(\mathbf{X}, \tau)}{\partial \tau}=
  \frac{1}{2}\nabla\left[\nabla -F(\mathbf{X})\right]f(\mathbf{X}, \tau) 
  +(E_T - E_L(\mathbf{X}))f(\mathbf{X}, \tau),
  \label{diffusionBrancingDriftEquation}
\end{equation}

where the \emph{force-term} $F$ is given by

\begin{equation*}
   F(\mathbf{X})=\frac{2\nabla \Psi_T(\mathbf{X})}{ \Psi_T(\mathbf{X})},
\end{equation*}

and where the local energy $E_L$ is defined as before

\begin{equation}
  E_L(\mathbf{X})=-\frac{1}{\Psi_T(\mathbf{X})}
  \frac{\nabla^2 \Psi_T(\mathbf{X})}{2}+V({\bf
    R}).%\Psi_T(\mathbf{X}).
\end{equation}

The first term of eq.~(\ref{diffusionBrancingDriftEquation}) is still
a diffusion term. The second term caused by the force $F$ is a
\emph{drift} term; a drift in a diffusional process drives the
population according to the drift-force. The branching term 
is now in a form suited for Monte Carlo. The branching effect 
is greatly reduced compared to the branching of
eq.~(\ref{diffusionBrancingEquation}). In the limit $\Psi_T = \Psi_0$
and $E_T=\epsilon_0$ there is 
no branching at all. The branching is simply a result of the
trial wave-function failing to duplicate the ground state, and
because of the inaccuracy of the trial energy. This last effect can be
accounted for in the DMC method, as we will come back to at the end of
this section.
\newline
%
\newline
Insertion of eq.~(\ref{DMCdistribution}) into
eq.~(\ref{diffusionBrancingDriftEquation}) reproduces
eq.~(\ref{diffusionBrancingEquation}), and therefore,
eq.~(\ref{diffusionBrancingDriftEquation}) is apparently
exact. However, there are problems associated with the introduction of
the distribution $f$. At the nodes of the trial wave-function this term
vanishes. This implies that the nodes are fixed by the form of the trial
wave-function, and will limit the accuracy of the DMC algorithm.
Furthermore, both the force of the drift term and the local energy
in the branching term are extremely sensitive to the trial wave-function
in the close proximity to these nodes. Failing to incorporate physical
effects into $\Psi_T$ thus leads to inaccuracies in the
solutions. Still, DMC provides a very good method for solving the
many-body system, and excellent results can be obtained even by using
quite simple guiding-functions, at least for small systems. The
ultimate accuracy depends solely on how well the trial 
wave-function duplicates the behavior of $\Phi_0$. 
\newline
%
\newline
The above interpretation of eq.~(\ref{diffusionBrancingDriftEquation})
poses a problem. The diffusion and drift
term move the population of walkers around, whereas the branching
either produces or removes walkers. These two effects cannot both be
performed simultaneously, and in practice we need to go to the limit
of short time and perform successive diffusion-drift and branching
processes. This approximation is established through the use of
Green's functions.
The details are left to the reader, see for example
ref. \cite{hammond1994}, but we will summarize some of the basic
implications. The diffusion and drift terms of
eq.~(\ref{diffusionBrancingDriftEquation}) reduce to the Green's
function 

\begin{equation*}
  \tilde{G}_{diff}(\mathbf{Y}, \mathbf{X}, d\tau) =
  (2\pi d\tau)^{-3n/2}e^{-(\mathbf{X}-\mathbf{Y}-d\tau
  F(\mathbf{Y})/2)^2/2d\tau},
\end{equation*}

where we move from $\mathbf{X}$ to $\mathbf{Y}$. Detailed balance must
be imposed, because

\begin{equation*}
 \tilde{G}_{diff}(\mathbf{Y}, \mathbf{X}, d\tau) \ne
 \tilde{G}_{diff}(\mathbf{X}, \mathbf{Y}, d\tau),
\end{equation*}

which is achieved by the following Metropolis acceptance

\begin{equation*}
  A(\mathbf{Y}, \mathbf{X}, d\tau) \equiv 
  min\left(1,\phantom{a}\frac{|\Psi(\mathbf{Y})|^2}{|\Psi(\mathbf{X})|^2}
  \frac{\tilde{G}_{diff}(\mathbf{X}, \mathbf{Y}, d\tau)}
       {\tilde{G}_{diff}(\mathbf{Y}, \mathbf{X}, d\tau)}\right).
\end{equation*}

The branching is realized by destruction and generation of
walkers. The branching \emph{rate} is given by
(ref. \cite{hammond1994})

\begin{equation*}
  \tilde{G}_{B}(\mathbf{Y}, \mathbf{X}, d\tau) =
  e^{-([E_L(\mathbf{X}) + E_L(\mathbf{Y})]/2 - E_T) \tau},
\end{equation*}

which ignores the path of the random walk. Nevertheless, as $\tau \to
0$ $\tilde{G}_{diff}\tilde{G}_{B}$ converges to the exact Green's
function. To get a form suited for Monte Carlo, the rate is written as
a sum of an integer $r$ and fraction $0 \le \delta r <1$, namely

\begin{equation*}
  \tilde{G}_{B}(\mathbf{Y}, \mathbf{X}, d\tau) = r + \delta r.
\end{equation*}

The population of walkers are then updated by generation of a random
number ${\cal X}$, between zero and one, for each walker. If ${\cal
  X} \le \delta r$ the current walker is copied to $r+1$ walkers, and
if ${\cal X} > \delta r$ the current walker becomes $r$ walkers.
For example if the rate of a walker is $0.36$ and ${\cal X} = 0.70$ the
walker is removed, and if the rate is $2.12$ and ${\cal X} = 0.10$ two
extra duplicates of the walker are created. 
\newline
%
\newline
The diffusion-drift and branching process is only valid in the limit
$d\tau \to 0$, whereas the stationary solution to the imaginary time
Schr\"odinger equation is obtained only in the limit $d\tau \to
\infty$. This is realized by first starting with a population of
walkers that represents the true ground state in the best possible
way, for example selected equilibrated walkers from a VMC run. Then
the imaginary time evolution $d\tau$ is taken to be a small number,
for example $d\tau = 1/1000$, and the population is evolved till it
equilibrates.
\newline
%
\newline
Finally, the trial energy $E_T$ is updated in accordance with the
population changes. If the total population is growing, reduce the
energy, and vice versa. As the DMC walkers equilibrate, the average of
this fluctuating energy provides an excellent estimate to the ground
state energy.
