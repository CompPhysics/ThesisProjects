\appendix

\chapter{Quasi-Newton method}

This optimization method use gradient information to build up corvature information at each iteration to formulate a quadratic model problem of the form

$$
\min_{x}\frac{1}{2}\bfv{x}^{T} + c^{T} \bfv{x} + \bfv{b},
$$

where $c$, $b$ are constants and the Hessian matrix, $H$, is positive definite symmetric. The optimal solution $x = x^*$ happens when the partial derivative of $x$ goes to zero

$$
\bfv{\nabla} f(\bfv{x^*}) = H \bfv{x}^* + \bfv{c} = 0
$$

with the optimal solution point given by

$$
x^* = -H^{-1} \bfv{c}
$$



% % % \input appendix/greensFunction
\input appendix/acceptUpdat.tex



