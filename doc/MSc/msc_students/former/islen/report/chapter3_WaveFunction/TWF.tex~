\chapter{Computational and parametric optimization of the trial wave function}\label{TWF}
The trial wave function plays a central role in quantum variational Monte Carlo simulations. Its importance lies in the fact that all the observables are computed with respect to the probability distribution function defined from the trial wave function. Moreover, it is needed in the Metropolis algorithm and in the evaluation of the quantum force term when importance sampling is applied. Computing a determinant of an $N \times N$ matrix by standard Gaussian elimination is of the order of ${\cal O}(N^3)$ calculations. As there are $N\cdot d$ independent coordinates we need to evaluate $Nd$ Slater determinants for the gradient (quantum force) and $N\cdot d$ for the Laplacian (kinetic energy). Therefore, it is diserable to find alternative ways of computating quantities related to the trial wave function such that the computational perfomance becomes improved.

\section{Splitting the Slater determinant}
Following Ref.\cite{Foulkes}, assume that we wish to compute the expectation value of a spin-independent quantum mechanical operator $\Op{O}(\bfv{r})$ using the spin-dependent state $\Psi(\bfv{x})$, where $\bfv{x} = (\bfv{r}, \bfv{\sigma})$ represents the space-spin coordinate par. Then,
$$
\Obs{O} = \frac{\langle\Psi(\bfv{x})|\Op{O}(\bfv{r})|\Psi(\bfv{x})\rangle}{\langle\Psi(\bfv{x})|\Psi(\bfv{x})\rangle}.
$$
If for each spin configuration $\bfv{\sigma} = (\bfv{\sigma_1}, \ldots, \bfv{\sigma_N})$ we replace the total antisymmetric wave function by a version with permuted arguments arranged such that the first $N_{\uparrow}$ arguments are spin up and the rest $N_{\downarrow} = N - N_{\uparrow}$ are spin down we get
\begin{eqnarray*}
 \Psi(\bfv{x_1},\ldots,\bfv{x_N}) & \rightarrow & \Psi(\bfv{x_{i1}},\ldots,\bfv{x_{iN}})\\
				  &    =        & \Psi(\{\bfv{r_{i1}},\uparrow\},\ldots,\{\bfv{r_{iN_{\uparrow}}}, \uparrow\},              		\{\bfv{r_{iN_{\uparrow\!+1}}},\downarrow\},\ldots,\{\bfv{r_{iN}}, \downarrow\})\\
				  &    =        & \Psi(\{\bfv{r_{1}},\uparrow\},\ldots,\{\bfv{r_{N_{\uparrow}}}, \uparrow\},              		\{\bfv{r_{1N_{\uparrow\!+ 1}}},\downarrow\},\ldots,\{\bfv{r_{N}}, \downarrow\})
\end{eqnarray*}
Because the operator $\Op{O}$ is symmetric with respect to the exchange of labels in a pair of particles, each spin configuration gives an identical contribution to the expectation value. Hence, 
$$
\Obs{O} = \frac{\langle\Psi(\bfv{r})|\Op{O}(\bfv{r})|\Psi(\bfv{r})\rangle}{\langle\Psi(\bfv{r})|\Psi(\bfv{r})\rangle}
$$
The new state is antisymmetric with respect to exchange of spatial coordinates of pairs of spin-up or spin-down electrons. Therefore, for spin-independent Hamiltonians, the Slater determinant can be splittet as a product of Slater determinants obtained from single particle orbitals with different spins. For electronic systems then yields that
$$
\Psi_D = D_\uparrow D_\downarrow,
$$
where 
\begin{equation}
D_\uparrow = |\bfv{D}(\bfv{r_1}, \bfv{r_2},\ldots,\bfv{r_{N/2}})|_{\uparrow} = 
 \begin{vmatrix}
 \phi_1(\bfv{r_1}) & \phi_2(\bfv{r_1}) & \cdots & \phi_{N/2}(\bfv{r_1})\\
\phi_1(\bfv{r_2}) & \phi_2(\bfv{r_2}) & \cdots & \phi_{N/2}(\bfv{r_2})\\
\vdots  & \vdots & \ddots & \vdots  \\
\phi_1(\bfv{r_{N/2}}) & \phi_2(\bfv{r_{N/2}}) & \cdots & \phi_{N/2}(\bfv{r_{N/2}})  
 \end{vmatrix}_{\uparrow}.
\end{equation}
In a similar way, $D_\downarrow = |\bfv{D}(\bfv{r_{N/2+1}}, \bfv{r_{N/2+2}},\ldots,\bfv{r_{N}})|_{\downarrow}$. The normalization factor has been removed, since it cancels in the ratios needed by the variational Monte Carlo algorithm, as shown later. The new state $\Psi_D(\bfv{r})$ gives in this case the same expectation value as $\Psi(\bfv{x})$, but it results to be more convenient in terms of computational cost. Observe that the Slater determinant in Eq.~(\ref{slaterJastrowTWF}) can now be factorized yielding

\begin{equation}\label{detDetJas}
 \boxed{\Psi_T(\bfv{x})  = D_\uparrow D_\downarrow \Psi_C.}
\end{equation}


\section{Computational optimization of the Metropolis/hasting ratio}\label{psi_psi_ratio}

In the Metropolis/hasting algorithm, the \emph{acceptance ratio} determines the probability for a particle to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by

\begin{equation}\label{acceptanceRatio}
\boxed{R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{cur}} = \underbrace{\frac{\Det{D}_{\uparrow}^{new}}{\Det{D}_{\uparrow}^{cur}} \frac{\Det{D}_{\downarrow}^{new}}{\Det{D}_{\downarrow}^{cur}}}_{R_{SD}}\, \underbrace{\frac{\Psi_{C}^{new}}{\Psi_{C}^{cur}}}_{R_{C}}.}
\end{equation}


\subsection{Evaluating the determinant-determinant ratio}

Evaluating the determinant of a $N \times N$ matrix by gaussian elimination takes of order of $\mathcal{O}(N^3)$ operations, expensive for a many-particle quantum system. An alternative algorithm no requiring the separated evaluation of the determinants will be derived in the following. We start defining a Slater matrix $\bfv{D} $ with its corresponding $(i,j)-$entries given by 
\begin{equation}\label{indexform_SM}
 D_{ij} \equiv \phi_j(\bfv{r_i}),
\end{equation}
where $\phi_j(\bfv{r_i})$ is the $j^{th}$ single particle wave function evaluated for the particle at position $\bfv{r_i}$.\\
\\
\noindent
The inverse of a (Slater) matrix is related to its adjoint (transpose matrix of cofactors) and its determinant by
\begin{equation}\label{inverseDef}
\bfv{D^{-1}} = \frac{adj{\bfv{D}}}{\Det{D}} \Rightarrow \Det{D} = \frac{adj{\bfv{D}}}{\bfv{D^{-1}}}, 
\end{equation}
or
\begin{equation}\label{det_def}
 \Det{D} = \sum_{j=1}^{N}\frac{C_{ji}}{D^{-1}_{ij}} = \sum_{j=1}^{N}D_{ij} C_{ji},
\end{equation}
i.e., the determinant of a matrix equals the scalar product of any column(row) of the matrix with the same column(row) of the matrix of cofactors.\\
\\
\noindent
In the particular case when only one particle is moved at the time (say particle at position $\bfv{r_i}$), this changes only one row (or column)\footnote{Some authors prefer to express the Slater matrix by placing the orbitals in a row wise order and the position of the particles in a column wise one.} of the Slater matrix. An efficient way of evaluating that ratio is as follows \cite{Ceperley_Chester,Hammond}.\\
\\
\noindent
We define the ratio of the new to the old determinants in terms of Eq.~Eq.~(\ref{det_def}) such that
$$R_{SD} \equiv \frac{|\bfv{D}(\bfv{x^{new}})|}{|\bfv{D}(\bfv{x^{cur}})|} = \frac{\sum_{j=1}^{N}D_{ij}(\bfv{x^{new}}) C_{ji}(\bfv{x^{new}})}{\sum_{j=1}^{N}D_{ij}(\bfv{x^{cur}}) C_{ji}(\bfv{x^{cur}})}.
$$
When the particle at position $\bfv{r_i}$ is moved, the $i^{th}-$row of the matrix of cofactors remains unchanged, i.e.,the row number $i$ of the cofactor matrix are independent of the entries in the rows of its corresponding matrix $\bfv{D}$. Therefore, 
$$C_{ij}(\bfv{x^{new}}) = C_{ij}(\bfv{x^{cur}}),$$
and 
\begin{equation}\label{Rratio}
R_{SD} = \frac{\sum_{j=1}^{N}D_{ij}(\bfv{x^{new}}) C_{ji}(\bfv{x^{cur}})}{\sum_{j=1}^{N}D_{ij}(\bfv{x^{cur}}) C_{ji}(\bfv{x^{cur}})} =  \frac{\sum_{j=1}^{N}D_{ij}(\bfv{x^{new}}) D_{ji}^{-1}(\bfv{x^{cur}}) \Det{D}(\bfv{x^{cur}})}{\sum_{j=1}^{N}D_{ij}(\bfv{x^{cur}}) D_{ji}^{-1}(\bfv{x^{cur}}) \Det{D}(\bfv{x^{cur}})}.\end{equation}
The invertibility of $\bfv D$ implies that 
\begin{equation}\label{inverseSlaterMatrix}
 \sum_{k}^{N} D_{ik} D^{-1}_{kj} = \delta_{ij}.
\end{equation}
Hence, the denominator in Eq.~(\ref{Rratio}) equals the unity. Then, $$
R_{SD} = \sum_{j=1}^{N}D_{ij}(\bfv{x^{new}}) D_{ji}^{-1}(\bfv{x^{cur}}).
$$
Substituting Eq.~(\ref{indexform_SM}) we arrive to
\begin{equation}\label{RSD}
 \boxed{R_{SD} = \sum_{j=1}^{N} \phi_j(\bfv{x^{new}_i}) D_{ji}^{-1}(\bfv{x^{cur}})}
\end{equation}
which means that determining $R_{SD}$ when only the particle $i$ has been moved, requires only the evaluation of the dot product between a vector containing orbitals (evaluated at the new position) and all the entries in the $i^{th}$ column of the inverse Slater matrix (evaluated at the current position). This requires of order of $\mathcal{O}(N)$ operations.\\
\\
\noindent
Further optimizations can be done by noting that when only one particle is moved at the time, one of the two determinants in the numerator and denominator of Eq.~Eq.~(\ref{acceptanceRatio}) is unaffected, cancelling each other. This let us to carry out calculations with only half of the total number of particles every time a move occurs, requiring only $(N/2)^d$ o\-pe\-ra\-tions, where $d$ is the number of spatial components of the problem, in systems with equal number of electrons with spin up and down. The total number of operations for a problem in three dimensions becomes $(N/2)^3 = N^3/8$, i.e., the total calculations are reduced up to by a factor of eight.


\section{Optimizing the $\nabla \Psi_T / \Psi_T$ ratio}\label{gradToDetRatio}
Equation (\ref{quantumForceEQ}) defines the quantum force required by the Metropolis algorithm with importance sampling. It implies computing the ratio of the gradient of the trial wave function to the wave function self. Setting $\Psi_D = \Det{D}_{\uparrow} \Det{D}_{\downarrow}$ in Eq.~(\ref{detDetJas}) we get,
\begin{eqnarray}
\frac{\Grad \Psi}{\Psi} & = &\frac{\Grad (\Psi_{D} \, \Psi_{C})}{\Psi_{D} \, \Psi_{C}}  =  \frac{ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad \Psi_{C}}{\Psi_{D} \Psi_{C}} = \frac{\Grad \Psi_{D}}{\Psi_{D}} + \frac{\Grad  \Psi_C}{ \Psi_C}\nonumber\\ & = & \frac{\Grad (\Det{D}_{\uparrow} \Det{D}_{\downarrow})}{\Det{D}_{\uparrow} \Det{D}_{\downarrow}} + \frac{\Grad  \Psi_C}{ \Psi_C}\nonumber,
\end{eqnarray}
or 
\begin{equation}\label{grad_det_ratio_gen}
\boxed{\frac{\Grad \Psi}{\Psi} =  \frac{\Grad (\Det{D}_{\uparrow}) }{\Det{D}_{\uparrow}} + \frac{\Grad (\Det{D}_{\downarrow})}{\Det{D}_{\downarrow}} + \frac{\Grad  \Psi_C}{ \Psi_C}.}
\end{equation}


\subsection{Evaluating the gradient-determinant-to-determinant ratio}
As stated, the evaluation of Eq.~(\ref{grad_det_ratio_gen}) requires differentiating the $N$ entries of the Slater matrix with respect to all the $d$ spatial components. Since the evaluation of the Slater determinant scales as $\mathcal{O}(N^3)$ this would involve of order of $N \cdot d \cdot \mathcal{O}(N^3) \approx \mathcal{O}(N^4)$ floating point operations. A cheaper algorithm can be derived by noting that when only one particle is moved at once, only one row in the Slater matrix need to be reevaluated. Thus, only the derivatives of that row with respect to the coordinates of the particle moved need to be updated. Obtaining the gradient-determinant ratio required in Eq.~(\ref{grad_det_ratio_gen}) becomes straigforward. It is analog to the procedure used in deriving Eq.~(\ref{acceptanceRatio}). From Eq.~(\ref{RSD}) and Eq.~(\ref{acceptanceRatio}) we see that
\begin{equation}\label{gradDetRatioO}
\boxed{\frac{\bfv{\nabla_i}|\bfv{D}(\bfv{x})|}{|\bfv{D}(\bfv{x})|} = \sum_{j=1}^{N} \bfv{\nabla_i} D_{ij}(\bfv{x}) D_{ji}^{-1}(\bfv{x}) = \sum_{j=1}^{N} \bfv{\nabla_i}\phi_j(\bfv{x_i}) D_{ji}^{-1}(\bfv{x}),}
\end{equation}
which means that when one particle is moved at the time, the gradient-determinant ratio is given by the dot product between the gradient of the single wave functions evaluated for the particle at position $\bfv{r_i}$ and the inverse Slater matrix.
A small modification has to be done when computing the gradient to determinant ratio after a move has been accepted. Denoting by $\bfv{y}$ the vector containing the new spatial coordinates, by definition we get,
$$
\frac{\bfv{\nabla_i}|\bfv{D}(\bfv{y})|}{|\bfv{D}(\bfv{y})|} = \sum_{j=1}^{N} \bfv{\nabla_i} D_{ij}(\bfv{y}) D_{ji}^{-1}(\bfv{y}) = \sum_{j=1}^{N} \bfv{\nabla_i}\phi_j(\bfv{y_i}) D_{ji}^{-1}(\bfv{y}),
$$
which can be expressed in terms of the transpose inverse of the Slater matrix evaluated at the old positions\cite{Hammond} to get
\begin{equation}\label{gradDetRatioN}
\boxed{\frac{\bfv{\nabla_i}|\bfv{D}(\bfv{y})|}{|\bfv{D}(\bfv{y})|} = \frac{1}{R} \sum_{j=1}^{N} \bfv{\nabla_i}\phi_j(\bfv{y_i}) D_{ji}^{-1}(\bfv{x}).}
\end{equation}
Computing a single derivative is an $\mathcal{O}(N)$ operation. Since there are $d . N$ derivatives, the total time scaling becomes $\mathcal{O}(d . N^2)$.


\section{Optimizing the $\nabla^2 \Psi_T/\Psi_T$ ratio}\label{kineticEnergyTerm}
From the single-particle kinetic energy operator Eq.~(\ref{kineticEnergy}), the expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\begin{equation}
 \langle \Op{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
\end{equation}
which is obtained by using Monte Carlo integration. The energy of each space con\-fi\-gu\-ra\-tion is cummulated after each Monte Carlo cycle. For each electron we evaluate
\begin{equation}\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
Following a procedure similar to that of section \ref{gradToDetRatio}, the term for the kinetic energy is obtained by
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi} & = & \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [\Grad {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad  \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  = & \frac{\Grad  \Psi_C \cdot \Grad \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \Grad \Psi_{D} \cdot \Grad  \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\end{eqnarray}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_C}{ \Psi_C}\nonumber\\
& = & \frac{\nabla^2 (\Det{D}_{\uparrow} \Det{D}_{\downarrow})}{(\Det{D}_{\uparrow} \Det{D}_{\downarrow})} + \frac{\nabla^2 \Psi_C}{\Psi_C} + 2 \frac{\Grad (\Det{D}_{\uparrow} \Det{D}_{\downarrow})}{(\Det{D}_{\uparrow} \Det{D}_{\downarrow})}\cdot\frac{\Grad \Psi_C}{\Psi_C},\nonumber
\end{eqnarray}
or 
\begin{equation}\label{laplacian_psi_psi_ratio}
\boxed{\frac{\nabla^2 \Psi}{\Psi} = \frac{\nabla^2 \Det{D}_{\uparrow}}{\Det{D}_{\uparrow}} + \frac{\nabla^2 \Det{D}_{\downarrow}}{\Det{D}_{\downarrow}} + \frac{\nabla^2 \Psi_C}{\Psi_C} + 2 \left[\frac{\Grad{\Det{D}_{\uparrow}}}{\Det{D}_{\uparrow}} +  \frac{\Grad{\Det{D}_{\downarrow}}}{\Det{D}_{\downarrow}}\right]\cdot \frac{\Grad{\Psi_C}}{\Psi_C},}
\end{equation}
where the \emph{laplace-determinant-to-determinant ratio} is given by 
\begin{equation}\label{lapDetRatio}
 \boxed{\frac{\nabla_{i}^{2}|\bfv{D}(\bfv{x})|}{|\bfv{D}(\bfv{x})|} = \sum_{j=1}^{N} \nabla_{i}^{2} D_{ij}(\bfv{x}) D_{ji}^{-1}(\bfv{x}) = \sum_{j=1}^{N} \nabla_{i}^{2}\phi_j(\bfv{x_i}) D_{ji}^{-1}(\bfv{x})}
\end{equation}
for particle at $\bfv{x_i}$ as deduced from Eq.~(\ref{RSD}) and Eq.~(\ref{acceptanceRatio}). The comments given in section \ref{gradToDetRatio} on performance yields also for this case. Moreover, Eq.~(\ref{lapDetRatio}) is computed with the trial move only if it is accepted.


\section{Updating the inverse of the Slater matrix}
Computing the ratios in Eqs.~(\ref{RSD}), (\ref{gradDetRatioO}), (\ref{gradDetRatioN}) and (\ref{lapDetRatio}) requires maintain the inverse of the Slater matrix evaluated at the current position. Each time a trial position is accepted, the row number $i$ of the Slater matrix changes and updating of its inverse has to be carried out. Getting the inverse of a $N \times N$ matrix by gaussian elimination has a complexity of order of $\mathcal{O}(N^3)$ operations, a luxury that we can not afford for each time a particle move is accepted. An alernative way of updating the inverse of a matrix when only a row/column is changed was suggested by Sherman and Morris\footnote{A derivation can be found in appendix \ref{updatingAlgorithmDer}} (1951)\cite{Sherman1950}. It has a time scaling of order of $\mathcal{O}(N^2)$ \cite{Hammond, PaulKent, Ceperley_Chester, Williansom} and is given by
\begin{eqnarray}\label{updatingInverse}
\boxed{D^{-1}_{kj}(\bfv{x^{new}})  = \left\{ 
\begin{array}{l l}
  D^{-1}_{kj}(\bfv{x^{cur}}) - \frac{D^{-1}_{ki}(\bfv{x^{cur}})}{R} \sum_{l=1}^{N} D_{il}(\bfv{x^{new}})  D^{-1}_{lj}(\bfv{x^{cur}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{D^{-1}_{ki}(\bfv{x^{cur}})}{R} \sum_{l=1}^{N} D_{il}(\bfv{x^{cur}}) D^{-1}_{lj}(\bfv{x^{cur}}) & \mbox{if $j=i$}
\end{array} \right.}\\
\end{eqnarray}

% % % % % % % % % % % % % % % % \begin{program}
% % % % % % % % % % % % % % % % \caption{. \emph{Updating the inverse of the Slater matrix.}}
% % % % % % % % % % % % % % % % \begin{algorithmic}%[1]
% % % % % % % % % % % % % % % % \medskip
% % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % \REQUIRE $R_{SD}$, $\bfv{D^{-1}}(\bfv{x^{cur}})$, $\bfv{D}(\bfv{x^{new}})$
% % % % % % % % % % % % % % % % \ENSURE $\bfv{D^{-1}}(\bfv{x^{new}})$
% % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % \medskip
% % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % %  \FOR{$j = 0$ to $N-1$}
% % % % % % % % % % % % % % % %    \STATE{\COMMENT{\emph{Update all but the $i^{th}-$column of $\bfv{D}^{-1}$.}}}
% % % % % % % % % % % % % % % %    \IF{$j \neq i$}
% % % % % % % % % % % % % % % %      \FOR{$l=0$ to $N-1$} 
% % % % % % % % % % % % % % % %       \STATE{\COMMENT{For each column $j \neq i$ calculate the intermediate quantity $S_j$}}
% % % % % % % % % % % % % % % %       \STATE{$S_j  \,+=\,  D_{il}(\bfv{x^{new}}) D_{lj}^{-1}(\bfv{x^{cur}})$}
% % % % % % % % % % % % % % % %      \ENDFOR
% % % % % % % % % % % % % % % %      
% % % % % % % % % % % % % % % %      \FOR{$k=0$ to $N-1$}
% % % % % % % % % % % % % % % %          \STATE{$D_{kj}^{-1}(\bfv{x^{new}}) = D_{kj}^{-1}(\bfv{x^{cur}}) - \frac{S_j}{R_{SD}} D_{ki}^{-1}(\bfv{x^{cur}})$}
% % % % % % % % % % % % % % % %      \ENDFOR
% % % % % % % % % % % % % % % %    \ENDIF
% % % % % % % % % % % % % % % %    \IF{$j == i$}
% % % % % % % % % % % % % % % %      \FOR{$k=0$ to $N-1$}
% % % % % % % % % % % % % % % %        \STATE{$D_{ki}^{-1}(\bfv{x^{new}}) = \frac{1}{R_{SD}} D_{ki}^{-1}(\bfv{x^{cur}})$}
% % % % % % % % % % % % % % % %      \ENDFOR
% % % % % % % % % % % % % % % %    \ENDIF
% % % % % % % % % % % % % % % % \ENDFOR
% % % % % % % % % % % % % % % % \\
% % % % % % % % % % % % % % % % \medskip
% % % % % % % % % % % % % % % % $\bfv{D^{-1}}\bfv{x^{cur}} = \bfv{D}(\bfv{x^{new}})$
% % % % % % % % % % % % % % % % \end{algorithmic}\label{updatingMatrixAlgo}
% % % % % % % % % % % % % % % % \end{program}
The evaluation of the determinant of an $N \times N$ matrix by standard Gaussian elimination is of the order of ${\cal O}(N^3)$
calculations. As there are $N . d$ independent coordinates we need to evaluate $N . d$ Slater determinants for the gradient (quantum force) and $N . d$ for the Laplacian (kinetic energy). With the updating algorithm we need only to invert the Slater determinant matrix once. This can by calling standard LU decomposition methods.\\
\\
\noindent
Table \ref{performance} summarizes the computational cost associated with the Slater determinant part of the trial wave function.

\begin{table}
\centering
\begin{tabular}{l*{6}{l}l}
\hline
Operation       & No optimization & With optimization\\
\hline
Evaluation of $R$    & $\mathcal{O}(N^2)$ & $\mathcal{O}\left(\frac{N^2}{2}\right)$\\
Updating inverse& $\mathcal{O}(N^3)$ & $\mathcal{O}\left(\frac{N^3}{4}\right)$  \\
Transition of one particle & $\mathcal{O}(N^2) +  \mathcal{O}(N^3)$ & $\mathcal{O}\left(\frac{N^2}{2}\right) + \mathcal{O}\left(\frac{N^3}{4}\right)$\\
\hline
\end{tabular}
\caption{Comparison of the computational cost invoved in computing the Slater determinant with and without optimization.}
\label{performance}
\end{table}


\section{Reducing the computational cost fo the correlation form}\label{optimizingCorrelation}
From Eq.(\ref{psiC}), the total number of different relative distances $r_{ij}$ is $N(N-1)/2$. In a matricial storage format, the set conforms a strictly upper triangular matrix\footnote{In the implementation, however, we do not store the entries lying on diagonal.}
\begin{equation}\label{utrij}
 \bfv{r} \equiv \begin{pmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{pmatrix}.
\end{equation}
The same yields for $\bfv{g} = \bfv{g}(r_{ij})$. 
% % % % % % Hence, 
% % % % % % \begin{equation}
% % % % % %  g_{ij} \equiv g(r_{i,j}) = 
% % % % % %  \begin{pmatrix}
% % % % % %   0 & g(r_{1,2}) & g(r_{1,3}) & \cdots & g(r_{1,N}) \\
% % % % % %   \vdots & 0       & g(r_{2,3}) & \cdots & g(r_{2,N}) \\
% % % % % %   \vdots & \vdots  & 0  & \ddots & \vdots  \\
% % % % % %   \vdots & \vdots  & \vdots  & \ddots  & g(r_{N-1,N}) \\
% % % % % %   0 & 0  & 0  & \cdots  & 0 
% % % % % %  \end{pmatrix}
% % % % % % \end{equation}
\section{Computing the correlation-to-correlation ratio}\label{jastrowJastrowDer}
For the case when all particles are moved at once, all the $g_{ij}$ have to be reevaluated. The number of operations for getting $R_{C}$ scales as $\mathcal{O}(N^2)$\cite{Roestad}. When moving only one particle at a time, say the $k$th, only $N-1$ of the distances $r_{ij}$ having $k$ as one of their indices are changed. It means that the rest of factors in the numerator of the Jastrow-Jastro ratio has a similar contrapart in the denominator and cancel each other. Therefore, only $N-1$ factors of $\Psi_{C}^\mathrm{new}$ and $\Psi_{C}^\mathrm{cur}$ avoid cancellation and 
\begin{equation}\label{RjfRatio}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}\;
\prod_{i=k+1}^{N}\frac{g_{ki}^\mathrm{new}}{g_{ki}^\mathrm{cur}}}.
\end{equation}\label{padepadeRatio}
For the Pad\'e-Jastrow form
\begin{equation}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = \frac{e^{U_{new}}}{e^{U_{cur}}} = e^{\Delta U},}
\end{equation}
where
\begin{equation}
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
\end{equation}

One needs to develop a special algorithm 
that iterates only through the elements of the upper triangular
matrix $\bfv{g}$ that have $k$ as an index. 

\section{Evaluating the $\bfv{\nabla} \Psi_C/\Psi_C$ ratio}\label{gradJastrow}
The expression to be derived in the following is of interest when computing quantum force and the kinetic energy. It has the form
$$
\frac{\bfv{{\nabla_i}}\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$
for all dimensions and with $i$ running over all particles.
From the discussion in section \ref{jastrowJastrowDer}, for the first derivative only $N-1$ terms survive the ratio because the $g$-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,
\begin{equation}\label{1jgradG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
\end{equation}
An equivalent equation is obtained for the exponential form after replacing $g_{ij}$ by $\exp(g_{ij})$, yielding:
\begin{equation}\label{1jgradEG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
\end{equation}
with both expressions scaling as $\mathcal{O}(N)$.\\
\\
\noindent
Later, using the identity 
\begin{equation}\label{firstDerIdentity}
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij} 
\end{equation}
on the most right hand side terms of Eq.~(\ref{1jgradG}) and Eq.~(\ref{1jgradEG}), we get expressions where all the derivatives act on the particle represented by the
\emph{second} index of $g$:
\begin{equation}\label{gradJasGen}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
}
\end{equation}
and for the exponential case:
\begin{equation}\label{gradJasGenExp}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
}
\end{equation}


\subsection{Special case: correlation functions depending on the scalar relative distances}
For correlation forms depending only on the scalar distances $r_{ij}$ defined by Eq.~(\ref{scalarDistance}), a trick introduced in \cite{Albrigtsen} consists in using the chain rule. Noting that 
\begin{equation}\label{chainRule}
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
\end{equation}
after substitution in Eq.~(\ref{gradJasGen}) and Eq.~(\ref{gradJasGenExp}) we arrive to
\begin{equation}\label{generalCorrelation}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\bfv{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
}
\end{equation}
Note that for the Pad\'e-Jastrow form we can set $g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}}$ and 
\begin{equation}
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
\end{equation}
Therefore, 
\begin{equation}\label{padeJastrowGradJasRatio}
\boxed{
\frac{1}{\Psi_{PJ}}\frac{\partial \Psi_{PJ}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\bfv{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
}
\end{equation}
where 
\begin{equation}\label{distanceVector}
 \bfv{r}_{ij} = |\bfv{r}_j - \bfv{r}_i| = (x_j - x_i)\uvec{e}_1 + (y_j - y_i)\uvec{e}_2 + (z_j - z_i)\uvec{e}_3
\end{equation}
is the vectorial distance. When the correlation function is the \emph{linear Pad\'e-Jastrow} given by Eq.~(\ref{linealPJ}), we set \begin{equation}
f_{ij} = \frac{a_{ij} r_{ij}}{(1 + \beta_{ij} r_{ij})},
\end{equation}
for which yields the analytical expression
\begin{equation}\label{analyticalPJGrad}
 \boxed{\frac{\partial f_{ij}}{\partial r_{ij}} = \frac{a_{ij}}{(1 + \beta_{ij} r_{ij})^2}}.
\end{equation}



\section{Computing the $\nabla^2 \Psi_C/\Psi_C$ ratio}\label{lapCorOpt}

For deriving this expression we note first that Eq.~(\ref{generalCorrelation}) can be written as 

$$\bfv{\nabla}_k \Psi_C = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}.$$
After multiplying by $\Psi_C$ and taking the gradient on both sides we get,

\begin{align}\label{gradLap}
\nabla_{k}^2 \Psi_C & = \bfv{\nabla}_k \Psi_C \cdot 
\left(\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
&+
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
& = \Psi_C \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 +
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right).
\end{align}
Now,
\begin{align}
 \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= \bfv{\nabla}_k \left(\frac{1}{g_{ik}}\right)\cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}}\bfv{\nabla}_k \cdot \bfv{\nabla}_k g_{ik}\nonumber\\
 & = -\frac{1}{g_{ik}^2} \bfv{\nabla}_k g_{ik} \cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}} \bfv{\nabla}_k \cdot \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
 & = -\frac{1}{g_{ik}^2} (\bfv{\nabla}_k g_{ik})^2 \nonumber\\&+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) \bfv{\nabla}_k \cdot \bfv{r}_{ik}  \right] \nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right]\nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right], \label{subs0}
 \end{align}
with $d$ being the number of spatial dimensions.

Moreover, 
\begin{align*}
\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) &= \frac{\bfv{r}_{ik}}{r_{ik}} \frac{\partial }{\partial r_{ik}} \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
&=\frac{\bfv{r}_{ik}}{r_{ik}}\left(-\frac{1}{r_{ik}^2}\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{1}{r_{ik}}\frac{\partial^2 g_{ik}}{\partial r_{ik}^2}\right).\label{subs1}
\end{align*}

The substitution of the last result in Eq.~(\ref{subs0}) gives

\begin{align*}
  \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right].
\end{align*}

Inserting the last expression in Eq.~(\ref{gradLap}) and after division by $\Psi_C$ we get,

\begin{align}
 \frac{\nabla_{k}^2 \Psi_C}{\Psi_C} & =  \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(\frac{\partial g_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial g_{ki}}{\partial r_{ki}} + \frac{\partial^2 g_{ki}}{\partial r_{ki}^2} \right].
\end{align}
For the exponential case we have
\begin{align*}
 \frac{\nabla_{k}^2 \Psi_{PJ}}{\Psi_{PJ}} & =  \left(\frac{\bfv{\nabla}_k \Psi_{PJ}}{\Psi_{PJ}}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(g_{ik}\frac{\partial f_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial }{\partial r_{ki}}\left(g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}}\right) \right].
 \end{align*}
Using
\begin{align*}
 \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) & = \frac{\partial g_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\left(\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}
\end{align*}
and substituting this result into the equation above gives rise to the final expression,
\begin{align}\label{lapJasRatio}
\frac{\nabla_{k}^2 \Psi_{PJ}}{\Psi_{PJ}}  &=  \left(\frac{\bfv{\nabla}_k \Psi_{PJ}}{\Psi_{PJ}}\right)^2\nonumber\\
  &+ \sum_{i=1}^{k-1} \left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial^2  f_{ik}}{\partial r_{ik}^2} \right]
  + \sum_{i=k+1}^{N} \left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial^2 f_{ki}}{\partial r_{ki}^2} \right].
 \end{align}

Again, for the \emph{linear Pad\'e-Jastrow} of Eq.~(\ref{linealPJ}), we get in this case the analytical result
\begin{equation}\label{analyticalLinearPJLap}
\boxed{\frac{\partial^2 f_{ij}}{\partial r_{ij}^2} = - \frac{2 a_{ij} \beta_{ij} }{(1 + \beta_{ij} r_{ij})^3}}.
\end{equation}


\section{Efficient parametric optimization of the trial wave function}\label{effParamDer}

Energy minimization as expressed by Eq.~(\ref{definition}) requires the evaluation of the derivative of the trial wave function with respect to its the variational parameters. The computational cost of this operation depends, of course, on the algorithm selected. In practice, evaluating the derivatives of the trial wave function with respect to the variational parameters analitically is possible only for small systems (two to four electrons). On the other hand, the numerical solution needs the repetead evaluation of the trial wave function (the product of a Slater determinant by a Jastrow function) with respect to each variational parameter. As an example, consider using a central difference scheme to evaluate the derivative of the Slater determinant part with respect to a parameter $\alpha$, 
$$\frac{d \Psi_{SD}}{d \alpha} = \frac{\Psi_{SD}(\alpha + \Delta \alpha) - \Psi_{SD}(\alpha - \Delta \alpha)}{2\Delta \alpha} + \mathcal{O}(\Delta \alpha^2).$$
The reader should note that for the Slater determinant part we need to compute the expression above two times per Monte Carlo cycle per variational parameter. As mentioned, computing a determinant is a highly costly operation. Moreover, the numerical accuracy in the solution will depend on the choice of the step size $\Delta \alpha$.\\
\\
\noindent
In the following we suggest a method to efficiently compute the derivative of the energy with respect to the variational parameters. It derives from the fact that Eq.~(\ref{definition}) is equivalent to
$$
 \frac{\partial E}{\partial c_m} = 2\left[\left\langle E_L \frac{\partial \ln \Psi_{T_{c_m}}}{\partial c_m}\right\rangle - E \left\langle \frac{\partial \ln \Psi_{T_{c_m}}}{\partial c_m}\right\rangle \right],
$$
or more precisicely,
\begin{equation}
\boxed{\frac{\partial E}{\partial c_m}\! =\! 2\left\{\!\frac{1}{N} \sum_{i=1}^{N} \left[(E_L[c_m])_i \left(\frac{\partial \ln \Psi_{T_{c}}}{\partial c_m}\right)_i\right]\! -\! \frac{1}{N^2} \sum_{i=1}^{N} (E_L[c_m])_i \sum_{j=1}^{N} \left(\frac{\partial \ln \Psi_{T_{c}}}{\partial c_m}\right)_j\right\}\!},
\end{equation}
and because $\Psi_{T_{c_m}} = \Psi_{{SD}_{c_m}} \Psi_{{J}_{c_m}}$, we get that 
\begin{align*}
 \ln \Psi_{T_{c_m}} & = \ln(\Psi_{{SD}_{c_m}} \Psi_{J_{c_m}}) = \ln(\Psi_{{SD}_{c_m}}) + \ln(\Psi_{J_{c_m}}) \\
                          & = \ln(\Psi_{{{SD}_{c_m}\uparrow}} \Psi_{{{SD}_{c_m}\downarrow}}) + \ln(\Psi_{J_{c_m}}) \\ 
                          & = \ln(\Psi_{{{SD}_{c_m}}\uparrow}) + \ln({\Psi_{{SD}_{c_m}\downarrow}}) + \ln(\Psi_{J_{c_m}}).
\end{align*}
Then,
\begin{equation}\label{derLnPsi}
 \boxed{
 \frac{\partial \ln \Psi_{T_{c_m}}}{\partial c_m} = \frac{\partial \ln(\Psi_{{SD_{c_m}}\uparrow})}{\partial c_m} + \frac{\partial \ln(\Psi_{{SD}_{c_m}\downarrow})}{\partial c_m}  + \frac{\partial \ln(\Psi_{J_{c_m}})}{\partial c_m}
 },
\end{equation}
which is a convenient expression in terms of implementation in an object oriented fa\-shion because we can compute the contribution to the expression above in two separated classes independently, namely the the Slater determinant and Jastrow  classes.\\
\\
\noindent
For further computation remark for each of the derivatives of concerning the determinants above we have, in general, that
$$\frac{\partial \ln(\Psi_{{SD_{c_m}}})}{\partial c_m} = \frac{\frac{\partial \Psi_{{SD_{c_m}}}}{\partial c_m}}{\Psi_{{SD_{c_m}}\uparrow}}$$

% % % % % % % % % % % % % % % % % The problem we face now is how to compute the derivative of a determinant, in general. This issue has been treated in \cite{Hanche-Olsen1997} and it will be addressed in the following in relation with each of the Slater determinants appearing in the trial wave function. 
% % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % Let $\Phi(t) \in \mathbb{R}^{N\times N}$ be a matrix depending on the parameter $t$. If $\Phi(t)$ is differenciable with respect to $t$ so is its determinant $\det \Phi(t) = \d(\varphi_1, \varphi_2,\ldots,\varphi_N)$, because it is a polynomial in the components of $\Phi(t)$. In other words, the $d$ function is linear in each of its arguments (the rows of $\Phi(t)$) as long as we keep each of the remaining rows constant. Then,
% % % % % % % % % % % % % % % % % \begin{equation}\label{derivativeNDeterminants}
% % % % % % % % % % % % % % % % % \frac{d}{d t} \det \Phi(t)= d(\dot{\varphi}_1, \varphi_2,\ldots,\varphi_N) + d(\varphi_1, \dot{\varphi}_2,\ldots,\varphi_N) + \ldots + d(\varphi_1, \varphi_2,\ldots,\dot{\varphi}_N) 
% % % % % % % % % % % % % % % % % \end{equation}
% % % % % % % % % % % % % % % % % Doing this, however, could not be convenient in terms of computational cost and execution time because of the need of computing $N$ determinants for getting a single derivative. There is, however, a way of do it better.
% % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % Letting $\Phi(t)$ be the identity matrix, we note that the right hand side of Eq.~(\ref{derivativeNDeterminants}) is the trace of $\dot{\Phi}(t)$, where the first terms is given by  
% % % % % % % % % % % % % % % % % $$
% % % % % % % % % % % % % % % % % \begin{bmatrix}
% % % % % % % % % % % % % % % % % \dot{\varphi}_{11} & \dot{\varphi}_{12} & \cdots & \dot{\phi}_{1N}\\
% % % % % % % % % % % % % % % % % 0               &         1       & \cdots &          0    \\
% % % % % % % % % % % % % % % % % \vdots          &         \ddots       & \ddots &        \vdots \\
% % % % % % % % % % % % % % % % % 0               &         0  & \cdots &           1   
% % % % % % % % % % % % % % % % % \end{bmatrix} = \dot{\varphi}_{11},
% % % % % % % % % % % % % % % % % $$
% % % % % % % % % % % % % % % % % and so on. Then, what we have on the right hand side of Eq.~(\ref{derivativeNDeterminants}) is that 
% % % % % % % % % % % % % % % % % $$
% % % % % % % % % % % % % % % % % \dot{\varphi}_{11} + \dot{\varphi}_{22} +  \cdots + \dot{\varphi}_{NN} = \sum_{i=1}^{N} \dot{\varphi}_{ii} = \mathrm{tr} \dot{\Phi}(t),
% % % % % % % % % % % % % % % % % $$ 
% % % % % % % % % % % % % % % % % or 
For the derivative of the Slater determinant yields that if $\mathbf A$ is an invertible matrix which depends on a real parameter $t$, and if $\frac{\mathrm d\mathbf A}{\mathrm dt}$ exists, then\cite{Slattery,matrix2008} 
$$
\frac{\mathrm d}{\mathrm dt}(\det\mathbf A)=(\det\mathbf A)\mathop{\textrm{tr}}\biggl(\mathbf A^{-1}\frac{\mathrm d\mathbf A}{\mathrm dt}\biggr).
$$



% % % % % % % 
% % % % % % % $$
% % % % % % % \frac{d}{d t} \det \Phi(t) = \sum_{i=1}^{N} \dot{\varphi}_{ii} = \mathrm{tr} \dot{\Phi}(t) \quad \text{when} \quad \dot{\Phi}(t) = I.
% % % % % % % $$
% % % % % % % Now, letting $A \in \mathbb{R}^{N\times N}$ be an invertible matrix we get that when $(A \dot{\Phi}(t)) = I$,
% % % % % % % $$\det (A \Phi(t)) = \det A \det \Phi(t) = \det A \frac{d}{d t} \det \Phi(t) = \mathrm{tr}(A \dot{\Phi}(t)), 
% % % % % % % $$
% % % % % % % which is valid whenever $\Phi(t)$ is invertible. Setting $A = \Phi(t)^{-1}$ and rearranging we get
% % % % % % % $$
% % % % % % % \frac{\frac{d}{d t}\det \Phi(t)}{\det \Phi(t)} =  \mathrm{tr} [\Phi(t)^{-1} \dot{\Phi}(t)],
% % % % % % % $$
% % % % % % % or even better
\begin{equation}\label{derivativeSD}
 \boxed{\frac{d}{dt}\ln\det \mathbf A(t) = \mathrm{tr} \left(\mathbf A^{-1} \frac{d \mathbf{A}}{dt}\right) = \sum_{i=1}^{N} \sum_{j=1}^{N} A^{-1}_{ij} \dot{A}_{ji}},
\end{equation}
where $N$ is the number of entries in a row. What we now have gotten is the expression for computing the derivative of each of the determinants appearing in Eq.~(\ref{derLnPsi}).  Furthemore, note that the specialization of this expression to the current problem implies that the term $\mathbf{A}^{-1}$ appearing on the right hand side is the inverse of the Slater matrix, already available after finishing each Monte Carlo cycle as deduced from the algorithms discussed in the previous sections. It means that the only we have to do is to take the derivative of each single wave function in the Slater matrix with respect to its variational parameter and taking the trace of $\Psi_{SD}(\alpha)^{-1} \dot{\Psi_{SD}}(\alpha)$. The implementation of this expression and its computation using analytical derivatives for the single state wave functions is straighforward. The flow chart for the Quantum Variational Monte Carlo method with optimization of the trial wave function is shown in figure \ref{chartFlowOptim}.

\begin{figure}
\begin{tikzpicture}[scale=1., node distance = 2.2cm, auto]
%%%%%\usebodyfont       [sansserif, 10pt]

  \footnotesize
    % Place nodes
    \node [block] (init) {Initialize $\bfv{R}$,\\
    set $\alpha$ and $\Psi_{T-\alpha}(\bfv{R})$};
    \node [block, below of=init, node distance=2.0cm] (suggestMove) {Suggest a move};
    \node [block, below of=suggestMove, node distance=1.8cm] (evaluateAcceptance) {Compute acceptance ratio};
    \node [block, left of=evaluateAcceptance, node distance=4.5cm] (randomGenerator) {Generate an uniformly distributed variable $r$};
    \node [decision, below of=evaluateAcceptance, node distance=2.2cm] (decide) {Is\\ $R \geq r$?};
    \node [block, right of=decide, node distance=3.5 cm] (rejectMove) {Reject move: \\ $\bfv{x}^{new}_{i} = \bfv{x}^{old}_{i}$};
    \node [block, below of=decide, node distance=2.0cm] (acceptMove) {Accept move:\\$\bfv{x}^{old}_{i} = \bfv{x}^{new}_{i}$};
    \node [decision, below of=acceptMove, node distance=2.2cm] (lastMove) {Last move?};
    \node [block, below of=lastMove, node distance=2.1cm] (getLocalEnergy) {Get local\\ energy $E_L$};
    \node [decision, below of=getLocalEnergy, node distance=2.2cm] (decideMC) {Last MC step?};
    \node [block, below of=decideMC, node distance=2.2cm] (collectSamples) {Collect samples};
    \node [decision, right of=collectSamples, node distance=6.0cm] (minEnergy) {Is\\ $\langle E \rangle_{min}$?};
    \node [block, below of=minEnergy] (ending) {End};
    
%     % Draw edges
    \path [line] (init) -- (suggestMove);
    \path [line] (suggestMove) -- (evaluateAcceptance);
    \path [line] (evaluateAcceptance) -- (decide);
    \path [line] (randomGenerator) |- (decide);
    \path [line] (decide) -- node [, color=black] {yes}(acceptMove);
    \path [line] (decide) -- node [, color=black] {no}(rejectMove);
    \path [line] (acceptMove) -- (lastMove); 
    \path [line] (lastMove) -- node [, color=black] {yes}(getLocalEnergy);
    \path [line] (rejectMove) |- (lastMove);
    \path [line] (getLocalEnergy) -- (decideMC);
    \path [line] (decideMC) -- node [, color=black, node distance=5.5] {yes}(collectSamples);
   

    % Define a style for shifting a coordinate upwards
    % Note the curly brackets around the coordinate.
    \tikzstyle{s}=[shift={(0mm,\radius)}]
    \path[line] (lastMove.west) -- +(-1.0,0)  -- +(-1.0, 4.15) 
% % % %     % Draw semicircle junction to indicate that the lines are
% % % %     % not connected. Since we want the semicircle to have its center 
% % % %     % where the lines intersect, we have to shift the intersection 
% % % %     % coordinate using the 's' style to account for this.
    arc(-90:90:\radius) -- +(0.0, 3.9) -- (suggestMove.west);
    
    \path [line] (decideMC.west) -- node [, color=black]{no} +(-1.7,0) --+(-1.7,8.45) 
    arc(-90:90:\radius) --+(0.0,4.8) -- +(2.8,4.8);
         
    \path [line,dashed] (collectSamples) -- (minEnergy);
    \path [line] (minEnergy) -- node [, color=black] {yes}(ending);
   \path [line,dashed] (minEnergy) |- node [, color=black] {no}(init);

\end{tikzpicture}\caption{Optimization of the trial wave function $\Psi_{trial}(\bfv{\alpha})$ and minimization of the energy with respect to the variational parameters.}\label{chartFlowOptim}
\end{figure}

\clearemptydoublepage