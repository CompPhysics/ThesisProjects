\documentclass[a4paper,norsk,11pt,twoside]{report}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pstricks}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{epstopdf}
\usepackage{simplewick}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}
\usepackage[]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{arrows,matrix,positioning}

\date{}
\title{}
\author{}

% romersk numerering med arabisk på starten
% arabiske tall når jeg starter selve kapitelene
% \newpage foran hver kapitell ca

\begin{document}


%Forside
\thispagestyle{empty}
\begin{center}        % Sentrerer teksten
  %Tittel
  \vspace{5mm}          % Vertikalt mellomrom
  \LARGE
  \textbf{Coupled Cluster Studies in Computational Chemistry} \\
  \Large
  \vspace{5mm}
  \textbf{by} \\
  \vspace{5mm}
  %Forfatter
  \large
  \textbf{Ole Tobias B. Norli} \\
  %Avdeling for mekanikk
  \vspace{30mm}
  \Large
  {\bf{\textsl{THESIS}}} \\
  \textsl{for the degree of} \\
  \vspace{2mm}
  %%%%%%%OLD%%%%%%{\bf{\textsl{CANDIDATUS SCIENTIARUM}}} \\
  {\bf{\textsl{MASTER OF SCIENCE}}} \\
  \vspace{5mm}
  {\large \textsl {(Master i Computational Physics)}}\\
  \vspace{10mm}
  \centerline{\includegraphics[width=4cm,height=4cm]{UiO_Segl_cmyk.eps}}
  \vspace{5mm}
  % \textsl{Mechanics Division, Department of Mathematics} \\
  \textsl{Faculty of Mathematics and Natural Sciences} \\
  \textsl{University of Oslo} \\
  %Maaned, aar
  \vspace{10mm}
  \large
  \textsl{August 2014} \\
  \vspace{5mm}
  \normalsize
  % \textsl{Avdeling for mekanikk, Matematisk institutt} \\
  \textsl{Det matematisk- naturvitenskapelige fakultet} \\
  \textsl{Universitetet i Oslo} \\
\end{center}


\begin{abstract}
In this thesis we explore the Coupled Cluster method in Quantum Chemistry. We have implemented an effective Coupled Cluster Singles and Doubles code. We also explore deviations from the true ground state. For this purpose we have implemented a Coupled Cluster Singles, Doubles and Triples code. Our results are in agreement with theory that Coupled Cluster converge to the ground state when including more excitations and improving the basis set. \\

Our code performance is approaching the level of the best performing software available. Further continuations of already implemented optimizations are proposed to help development of more effective Coupled Cluster code.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
I would like to acknowledge my supervisor Morten H. Jensen. You are the best supervisor I could ask for, thank you. Also thank you to Diako Darian. 
\end{abstract}


\tableofcontents{}

\maketitle
\newpage




\chapter{Introduction}

Quantum Chemistry is a field of research where quantum mechanics is
used to describe the behaviour of atoms and molecules. This can be
used to model for example chemical reactions. A good understanding of
chemical reactions is vitally important in several fields, from
materials science to life science and medicine, with a huge potential
for industrial applications. To develop accurate many-body methods
which allow us to reproduce and predict properties of atoms and
molecules is thus extremely important for scientific progress in a
wide range of scientific fields, from basic research to industrial
applications. \\

In 2013 Martin Karplus and Michael Levitt were awarded the Nobel Prize
in chemistry for their development of multiscale models for complex
chemical systems. Their work focused on Molecular Dynamics (MD)
simulations of large chemical reactions. An important breakthrough in
their work was combing higher and lower accuracy methods to provide an
accurate and computationally efficient model. The active parts of the
molecule were described with high accuracy, while the inactive parts
were described with less accurate methods. \\

In this thesis we will focus on a high accuracy method in quantum
chemistry. We will study the Coupled Cluster method. The Coupled Cluster method is one of the highly successful so-called first-principle methods (or {\em ab initio} methods) and  
was introduced in the late 1950s by Coester and K\"ummel within the context of nuclear physics. 
It was introduced in quantum chemistry in the 1960s by Cizek and Paldus. 
It is considered
to be a highly accurate many-body method. In the 1980s through 1990s several
computational chemists predicted Coupled Cluster would be the method
of choice for most calculations in quantum chemistry today. However
the method applied in the absolute majority of publications is 
Density Functional Theory (DFT).  \\

The main reason DFT is so popular is the computational
affordability. DFT can model much larger systems than Coupled Cluster,
and in much less time. Therefore we will focus much of our attention
on implementing an optimized Coupled Cluster code. \\

In this thesis we will develop computational chemistry methods based
on quantum mechanics. These are called ab initio quantum chemistry
methods. We will implement Hartree Fock (HF) theory, Coupled Cluster Singles
and Doubles (CCSD) and Coupled Cluster Singles, Doubles and Triples
(CCSDT) from scratch. We will design parallel algorithms for HF and
CCSD. Our algorithms will focus on effective memory distribution and
high performance. Calculations will be performed on the Abel supercomputing
cluster of the University of Oslo. In particular the CCSD implementation will be greatly
optimized. We will also present an extremely optimized algorithm for
transformation of the four index integrals involved in post-HF
methods. \\

We have benchmarked our performance and results against existing
software. Since our implementation is made from scratch we will also
propose further optimizations in great detail. The proposed
optimizations combine positive features from our implementation and
existing software developed by others. The main purpose will be
working towards a more computationally affordable CCSD
implementation. One that can also run calculations on larger
molecules. \\

We will not present an optimized CCSDT implementation. CCSDT is
implemented to better study the limitations on accuracy in
CCSD. The Coupled Cluster method in theory only contains two errors. These are a
limited basis set and a truncation of excitations included. With CCSDT
implemented we will be able to study both these errors. \\

The thesis is structured for a good presentation of theory, code
development and results. Chapter 2 describes the basics of the system
we will study. The theoretical derivation of the Hartree Fock method
using Gaussian Type Orbitals is given in chapters 3 and 4. \\

Chapter 5 contains a derivation of the CCSD method. In Chapter 6 we provide
the factorized and implementation ready CCSD equations. \\

Chapter 7 contains information about the general programming
principles we will apply in our implementation. This includes
information on parallel programming and external libraries in
use. Chapter 8 discusses our actual serial and parallel
implementation of HF theory and CCSD.\\

Chapter 9 is an implementation guide to the CCSDT method. We will not
derive the equations for this method. Multiple references are included
and the equations are presented in an implementation ready form. Our
actual implementation of CCSDT is plain and simple, as presented
in this chapter. \\

In chapter 10 we present benchmark calculations to validate our
implementation. Chapter 11 presents new results and chapter 12 states our
conclusions. In chapter 13 we propose future prospects. \\

All code developed is freely available on github. Please see Ref.\cite{my_own_code_shiiiit}. 



\chapter{Definition of Hamiltonian}

In this chapter we present the Hamiltonian, with some basic definitions, for the systems we want to
study in this thesis, namely various atoms and molecules (with an emphasis on molecules) 
using first principle theories. We are mainly
interested in the ground state of atoms and molecules, and we aim at 
solving the time-independent Schr\"odringer equation
\begin{equation}
\textbf{H} |\Psi \rangle = E |\Psi \rangle,
\end{equation}
where $\textbf{H}$ is the Hamiltonian of the system, $\Psi$ the given eigenstate function and $E$ the corresponding
eigenenergy or simply energy  of the system. 

\section{Hamiltonian}
The full Hamiltonian for such atoms and molecules  is well defined, it reads
\begin{align}
\textbf{H} = &
- \sum_A^{nuc} \frac{1}{2m_A} \nabla_A^2
- \sum_i^E \frac{1}{2} \nabla_i^2
- \sum_A^{nuc} \sum_i^E \frac{Z_A}{|r_i - R_A|}
\nonumber \\ &
+ \sum_{i>j}^E \frac{1}{|r_i - r_j|}
+ \sum_{A>B}^{nuc} \frac{1}{|R_A - R_B|}.
\end{align}
The various terms represent the kinetic and potential energy terms for the electrons and the nucleus (in case of atoms) or nuclei in case of molecules. Here $R_A$ is the position of a given nucleus, $r_i$ is the position of electron $i$, $m_A$ is the mass ratio of a given given nucleus with the electron mass and $Z_A$ is the charge of that specific nucleus.


\section{The Born-Oppenheimer approximation}
Throughout this thesis we will employ a Hamiltonian where the
Born-Oppenheimer approximation is used.  In this approximation we
neglect the nuclear kinetic energy, since the time it takes for a
nucleus to move is large compared to the time it takes for the
electrons to obtain their ground state configuration. This means we
can solve the equations first with the nucleus or the nuclei at fixed
positions. When the nucleus (or nuclei in case of molecules) is (are)
at a fixed position the kinetic energy term becomes zero. We neglect
also contributions from nuclear forces since their energy scales are
in the gigaelectronvolt domain. We will refer to such a Hamiltonian as
the electronic Hamiltonian, $\textbf{H}_e$, and it reads
\begin{equation}
\textbf{H}_e = - \sum_i^E \frac{1}{2} \nabla_i^2
- \sum_A^{nuc} \sum_i^E \frac{Z_A}{|r_i - R_A|}
+ \sum_{i>j}^E \frac{1}{|r_i - r_j|}
+ \sum_{A>B}^{nuc} \frac{1}{|R_A - R_B|}.
\end{equation}
The term $\frac{1}{|R_A - R_B|}$ has no electrons in it, but it is
often included in the electronic Hamiltonian. With nuclei at fixed
positions this term reduces to a constant value. Using the
electronic Hamiltonian we can then find the potential energy of the
nuclei. \\

In this thesis we will thus only be working with the electronic
Hamiltonian, and in future chapters we will just call it $\textbf{H}$.

\section{Comments on the Wavefunction}
In this thesis we will represent the many-particle state function (or just wavefunction, $\Psi$) by single-particle basis function that solve the Hartree-Fock equations. These equations will be derived in chapter \ref{hf_chapter_reference}. The Hartree-Fock method represents an
approximation to the solution of the full Schr\"odinger equation. \\

In
practical terms, it is an algorithm which allows us to rewrite the abovementioned many-particle Schr\"odinger 
equation in terms of coupled single-particle equations. It represents perhaps the simplest approach to the 
full many-body problem and provides a so-called self-consistently solved  basis of orthogonal single-particle wavefucntions. We will call these single-particle states for spin orbitals hereafter. 
These basis functions are in turn used as input to so-called post Hartree-Fock methods like coupled-cluster theory. \\

 In the Hartree-Fock approximation we assume that the many-body wavefunction can be
written as a function of single electron wavefunctions. Each electron
will occupy its own spin orbital. Since we are aiming at the ground state, the
occupied spin orbitals are the ones with the lowest energy from the solution of the Hartree-Fock equations.\\

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{tat.jpeg}
\caption{Illustration of electrons occupying orbitals to construct a wavefunction.}
\label{fig:basic_kapitell1_fig}
\end{figure}

Figure \ref{fig:basic_kapitell1_fig} is an illustration of
this. The state $|\Psi_0 \rangle$ has six electrons. Two electrons occupy the
lowest orbital in energy, one with spin up and one with spin down. Two electrons
occupy the second lowest orbital in energy, and the last two electrons occupy the third
lowest energy level. This forms then an ansatz for  the ground state. The next wavefunction has a
configuration where one electron can be  excited to a higher energetic
orbital. This is labeled as $| \Psi_i^a \rangle$. \\

We here use a standard quantum chemistry notation, where
occupied spin orbitals are labeled by the letters $i,j,k,\dots$ and unoccupied orbitals are labeled as 
$a,b,c,\dots$. 
The occupied spin orbitals serve to define the ansatz for the ground state wave function, in our case this will be a 
so-called Slater determinant since our particles (the electrons) are fermions and need to obey the requirement that the 
total wavefunction is antisymmetric in space. 
A generic spin orbital is labeled by the letters $p,q,r,\dots$. Any electron can be
excited to any of the higher orbitals. The state $|\Psi_{ij}^{ab} \rangle$
represents two electrons excited to higher energetic orbitals. \\

To find the true electronic wavefunction we would need a perfect
description of the orbitals, and a linear combination of all the
different possible excitations. There is an unlimited number of
possible excitations, but some are more likely than others. In Coupled
Cluster theory, to be discussed later, we will include some of these excited states. 




\chapter{Hartree Fock \label{hf_chapter_reference}}
In this chapter we will discuss the Hartree-Fock (HF) method. We will
derive the HF equations. For the most part we will limit ourselves to
deal with a spin restricted HF (RHF) method, with closed shells and a
single Slater determinant, Ref.~\cite{slater_determinant_new_citation}, approximation to the wavefunction. However a
spin unrestricted version has also been implemented and will be
discussed briefly. \\

Much of the material discussed here is based on a series of summer lectures
series from the Sherill Group, see for example Ref.~\cite{ref111111}, but see also 
Ref.~\cite{ref222222} for further details. Additional references include the recent Master of Science theses from the 
Computational Physics group at the University of Oslo, see the theses of
S.~A.~Dragly \cite{sadragly}, H.~M.~Eiding \cite{hmeiding} and M.~H.~Mobarhan \cite{mhmobarhan}. 
This chapter is also closely
related to the following chapter on an optimal basis for atoms and molecules, the so-called  Gaussian Type Orbitals (GTO).

\section{Introduction}
Since our main focus is on molecules, in our exposition of  
the HF method we will describe how to approximate the Schr\"odringer equation for an arbitrary molecule. 
Our equation in atomic units reads	

\begin{equation}
\textbf{H} | \Psi \rangle = E | \Psi |\rangle,
\label{hartreefock_1}
\end{equation}
where our electronic Hamiltonian, $\textbf{H}$, is defined

\begin{equation}
\textbf{H} = 
- \sum_i \frac{1}{2} \nabla_i^2 
- \sum_{iA} \frac{Z_A}{r_{iA}}
+ \sum_{i>j} \frac{1}{r_{ij}} 
+ \sum_{AB} \frac{Z_A Z_B}{R_{AB}} .
\end{equation}
Here $Z_A$ is the atomic number of nucleus A (with charge in atomic units) 
and $r_{iA}$ is the distance from nucleus A to electron i. 
To simplify notation we will introduce two new operators, $\textbf{h}(i)$ and $\textbf{v}(i,j)$. These are defined by

\begin{equation}
\textbf{h}(i) = - \frac{1}{2} \nabla_i^2 - \sum_A \frac{Z_A}{r_{iA}}, \label{single_particle_hf}
\end{equation}
which defines the one-body (or single-particle) electron Hamiltonian and

\begin{equation}
\textbf{v}(i,j) = \frac{1}{r_{ij}},
\end{equation}
which is called the two electron part of our Hamiltonian. The quantity
$r_{ij}$ is defined as the distance between electron i and electron
j. $r_{ij} = |r_i - r_j|$. This quantity has the following symmetry
$r_{ij} = r_{ji}$, and has the constraint that $i \not= j$. We also
introduce a shorthand notation for the nucleus-nucleus repulsion, namely,

\begin{equation}
V_{NN} = \sum_{AB} \frac{Z_A Z_B}{r_{AB}}.
\end{equation}
This leaves Eq.~\eqref{hartreefock_1} as

\begin{equation}
\left( 
\sum_i \textbf{h}(i) + \frac{1}{2} \sum_{ij} \textbf{v}(i,j) + V_{NN}
\right) | \Psi(R) \rangle
= E | \Psi(R) \rangle.
\end{equation}
Here $R$ is a vector of Cartesian coordinates (x, y, z) and spin for
the different electrons. We have also included a factor $\frac{1}{2}$
since we removed the constraint $i > j$ from the sum.

\section{Slater Determinant}
The first assumption made in HF theory is that the wavefunction, $\Psi(R)$,
can be written as a single Slater determinant. A Slater determinant is
defined as

\begin{equation}
\Psi_T(R) = \frac{1}{\sqrt{N!}}
\begin{vmatrix} 

        \psi_1(x_1) & \psi_2(x_1)  & \psi_3(x_1) & \dots  & \psi_N(x_1) \\ 
        \psi_1(x_2) & \psi_2(x_2)  & \psi_3(x_2) & \dots & \psi_N(x_2) \\
        \psi_1(x_3) & \psi_2(x_3)  & \psi_3(x_3) & \dots & \psi_N(x_3) \\
        \dots & \dots & \dots & \dots & \dots \\
        \psi_1(x_N) & \psi_2(x_N)  & \psi_3(x_N) & \dots & \psi_N(x_N) \\

    \end{vmatrix}
    .
\end{equation}
Here $N$ is the number of electron and $x_i$ denotes the $x$, $y$ and
$z$ coordinates for a single electron, i = 1, 2, $\dots$. The subscript $T$ in $\Psi_T$
indicates that this is a trial wavefunction, and not the exact
one. The factor $\frac{1}{\sqrt{N!}}$ is a normalization factor.  \\

An orbital is the wavefunction for a single electron. An atomic orbital is the wavefunction for a single electron in an atom. A molecular orbital is the wavefunction of a single electron in a molecule. \\

A spacial orbital is an orbital that describes the position of an
electron. A spin orbital describes the position and the spin of an
electron. Each spacial orbital has two spin orbitals, since electrons
are fermions with spin up or spin down. The quantity $\psi_i$ represents a
molecular spin orbital, in case of molecules. \\

There are a few properties that make a Slater determinant an
attractive trial wavefunction. First, it is antisymmetric, which means
a change in sign upon interchanging two particles. Second it
incorporates the Pauli Exclusion Principle, whose consequence states
that two identical fermions cannot occupy the same state
simultaneously.  \\

For our purposes we approximate $\Psi$ with a single Slater
determinant. A single Slater determinant is a so called independent
particle approximation. This will be discussed later in more depth. \\

Another shorthand notation for $\Psi_T(R)$ we will use soon is

\begin{equation}
| \Psi_T(R) \rangle = |i j k l \dots \rangle, 
\label{hfnotat}
\end{equation}
where index i, j, k, l, $\dots$ refer to a molecular spin orbital.

\section{The Energy Expression}
We will now find an expression for the energy with this wavefunction. The energy can be found by rewriting Eq. \eqref{hartreefock_1}, namely. 

\begin{align}
E^{HF} & = \langle \Psi_T | \textbf{H} | \Psi_T \rangle \nonumber \\ &
= \langle \Psi_T | \left( \sum_i \textbf{h}(i) + \frac{1}{2} \sum_{ij} \textbf{v}(i,j) + V_{NN} \right) | \Psi_T \rangle \nonumber \\ &
= \langle \Psi_T | \sum_i \textbf{h}(i) | \Psi_T | \rangle
+ \frac{1}{2} \langle \Psi_T | \sum_{ij} \textbf{v}(i,j) | \Psi_T \rangle
+ \langle \Psi_T | V_{NN} | \Psi_T \rangle.
\label{hfref2}
\end{align}
Here we have labeled the energy  as $E^{HF}$ in order to stress that it is the Hartree Fock energy we are aiming at. 
We also split up the equations into three parts. The easiest one comes from the nucleus-nucleus repulsion,

\begin{equation}
\langle \Psi_T | V_{NN} | \Psi_T \rangle = V_{NN} \langle \Psi_T | \Psi_T \rangle
= \sum_{AB} \frac{Z_A Z_B}{r_{AB}}.
\end{equation}
This will be a constant number. For the other two terms we use the
attributes of the Slater determinant to simplify. We also insert the
alternative notation noted in Eq. \eqref{hfnotat} and have

\begin{align}
\langle \Psi_T | \textbf{h}(i) | \Psi_T \rangle
 = \langle i j k l \dots | \textbf{h}(i) | i j k l \dots \rangle .
\end{align}
The operator $\textbf{h}(i)$ acts only on one orbital at the time, namely orbital $i$. The properties of the Slater determinant are such that this simplifies to

\begin{equation}
\langle i j k l \dots | \textbf{h}(i) | i j k l \dots \rangle = \langle i | \textbf{h} | i \rangle .
\end{equation}
The expression for the two-electron operator simplifies to

\begin{equation}
\langle i j k l  \dots | \textbf{v}(i,j) | i j k l \dots \rangle = \langle ij || ij \rangle .
\end{equation}
Notice that only two electrons are involved since we only have a two-body operator at most in our Hamiltonian. 
Here $\langle ij || ij \rangle$ is a shorthand for the double bar integral, defined as

\begin{equation}
\langle ij || ij \rangle = \langle ij | ij \rangle - \langle ij | ji \rangle .
\end{equation}
with $x_i$ being the coordinates and spin of electron $i$. 
Inserting this into Eq.~\eqref{hfref2} gives us

\begin{equation}
E^{HF} = 
\langle i | \textbf{h} | i \rangle 
+ \frac{1}{2} \left( \langle ij | ij \rangle - \langle ij | ji \rangle \right)
+ V_{NN} ,
\end{equation}
with $\langle ij | ij \rangle$ being defined as

\begin{equation}
\langle ij | ij \rangle = \int dx_1 \int dx_2 \psi_i^*(x_1) \psi_j^*(x_2) \frac{1}{r_{12}} \psi_i(x_1) \psi_j(x_2) .
\end{equation}
Note that $\psi_i^*$ and $\psi_i$ takes the same electron as input. Another notation frequently used in quantum chemistry is 

\begin{equation}
\langle ij | ij \rangle = [ii|jj] ,
\end{equation}
or in the case of general spin orbitals $p, q, r, s$ as

\begin{equation}
\langle pq | rs \rangle = [pr|qs] .
\end{equation}
The two-body interaction has several symmetries that we can utilize to improve the performance  of our codes.
One symmetry is given by the relation

\begin{equation}
\langle pq | rs \rangle = \langle qp | sr \rangle . \label{interchangesym} 
\end{equation}
We will use real orbitals. This provides four more symmetries, namely

\begin{equation}
\langle pq | rs \rangle = \langle rq | ps \rangle = \langle ps | rq \rangle = \langle rs | pq \rangle . \label{interchangesym2}
\end{equation}
These four symmetries can also be applied to Eq.~\eqref{interchangesym} which means we have in total eight symmetries..

\section{The Hartree Fock Equations}

To find the lowest possible energy we must find the molecular orbitals
that produce this energy. When finding a minima in such an equation we
employ the method of Lagrangian multipliers. The method is described
in detail in Ref.~\cite{lagrange_duden}. Here we will simply present the
equations for our system, and give some brief arguments  why these terms are
present in the equation

\begin{equation}
\mathcal{L}[\{\psi_i\}] = E^{HF}[\{\psi_i\}] - \sum_{ij} \epsilon_{ij} \left( \langle i | j \rangle - \delta_{ij} \right) . \label{tweakhf}
\end{equation}
Here $\mathcal{L}$ is a functional of the set of $\psi_i$. The aim is to find the minimum of this functional. 
The set of single-particle orbitals $\psi_i$ will be varied in order to find this minimum. The condition $\langle i | j \rangle - \delta_{ij}$ is a constraint we impose to ensure the molecular spin orbitals remain orthonormal, even when we vary them. That is we require

\begin{equation}
\langle i | j \rangle = \delta_{ij} .
\end{equation}
The quantity 
$\epsilon_{ij}$ are the undetermined Lagrange multipliers. The variation in our orbitals can be described as

\begin{equation}
\psi_i \rightarrow \psi_i + \delta \psi_i .
\label{hftweaks}
\end{equation}
We want to find the minimum of the functional $\mathcal{L}$. This means its derivative must be equal to zero, that is

\begin{equation}
\delta \mathcal{L} = 
\delta E^{HF}[\{\psi_i\}] 
- \sum_{ij} \epsilon_{ij} \delta \langle i | j \rangle
= 0 . \label{hflabting}
\end{equation}
We then insert Eq. \eqref{hftweaks} into the two terms in this equation, starting with the final term and obtain

\begin{equation}
\delta \langle i | j \rangle = 
\langle \delta i | j \rangle
+ \langle i | \delta j \rangle
+ \langle \delta i | \delta j \rangle .
\end{equation}
where $\delta i$ represent the variation of a single-particle orbital. With 

\begin{equation}
\delta \langle i | j \rangle \approx 
\langle \delta i | j \rangle
+ \langle i | \delta j \rangle .
\end{equation}
we find the variation in energy $\delta E^{HF}$ as

\begin{align}
\delta E^{HF} = & 
\sum_i \left( \langle \delta i | \textbf{h} | i \rangle
 + \langle i | \textbf{h} | \delta i \rangle \right)
 + \frac{1}{2} \sum_{ij} (
 \langle \delta i j | i j \rangle
 + \langle i \delta j | i j \rangle
 + \langle  i j | \delta i j \rangle \nonumber \\ &
 + \langle  i j | i \delta j \rangle
 - \langle \delta i j | j i \rangle 
 - \langle i \delta j | j i \rangle 
 - \langle i j | \delta  j i \rangle 
 - \langle i j | j \delta i \rangle )
 \nonumber \\ &
 = \sum_i (
 \langle \delta i | \textbf{h} | i \rangle
 + \langle i | \textbf{h} | \delta i \rangle ) \nonumber \\ &
 + \sum_{ij} (
 \langle \delta i j | i j \rangle 
 + \langle  i \delta j | i j \rangle
 - \langle \delta i j | j i \rangle 
 - \langle i \delta j | j i \rangle ) .
\end{align}
Here we used the symmetries defined in Eq. \eqref{interchangesym}. We insert this in Eq. \eqref{hflabting} and get

\begin{align}
0 = & \sum_i (
 \langle \delta i | \textbf{h} | i \rangle
 + \langle i | \textbf{h} | \delta i \rangle )
 - \sum_{ij} \epsilon_{ij} ( \langle \delta i | j \rangle)
+ \langle i | \delta j \rangle )
  \nonumber \\ &
 + \sum_{ij} (
  \langle \delta i j | i j \rangle 
 + \langle  i \delta j | i j \rangle
 - \langle \delta i j | j i \rangle 
 - \langle i \delta j | j i \rangle ) .
 \label{hfeq}
\end{align}
We now examine the term $\sum_{ij} \epsilon_{ij} \langle \psi_i | \delta \psi_j \rangle$. 
We will specifically take its complex conjugate twice, resulting in

\begin{equation}
\sum_{ij} \epsilon_{ij} \langle i | \delta j \rangle
= \left[ \sum_{ij} \epsilon_{ij}^* \left( \langle i | \delta j \rangle \right)^* \right]^* .
\end{equation}
The complex conjugate of the inner product interchanges the bra and
the ket states. We insert this and then interchange the indeces $i$ and $j$. We
can make this interchange because we are summing over all possible indices $i$
and $j$, resulting in

\begin{equation}
\left[ \sum_{ij} \epsilon_{ij}^* \left( \langle i | \delta j \rangle \right)^* \right]^* 
= \left[ \sum_{ij} \epsilon_{ij}^*  \langle \delta j | i \rangle \right]^*
=  \left[ \sum_{ij} \epsilon_{ji}^*  \langle \delta i | j \rangle \right]^* .
\end{equation}
We will assume $\epsilon_{ij}$ is part of a hermitian matrix where $\epsilon_{ji}^* = \epsilon_{ij}$. 
We have then 

\begin{equation}
\left[ \sum_{ij} \epsilon_{ji}^*  \langle \delta i | j \rangle \right]^* = \left[ \sum_{ij} \epsilon_{ij} \langle \delta i | j \rangle \right]^* .
\end{equation}
The content inside the parenthesis is the same as the other term
involving $\epsilon_{ij}$ in Eq.~\eqref{hfeq}. We have just shown that
the two terms are the complex conjugate of each other. This will hold
true for all terms in Eq.~\eqref{hfeq}. One term in the equation
is the complex conjugate of another. We will mark this in our equation
as $+ c.c.$, where this represents the complex conjugate of every
single term remaining in Eq.~\eqref{hfeq}. We have then

\begin{equation}
0 =  \sum_i 
 \langle \delta i | \textbf{h} | i \rangle
 - \sum_{ij} \epsilon_{ij} \langle \delta i | j \rangle)
 + \sum_{ij} (
 \langle \delta i j | i j \rangle 
 - \langle \delta i j | j i \rangle  ) + c.c .
\end{equation}
This equation can be rewritten using the definition of the inner product and drawing the sum over i and $\delta \psi_i^*(x_1)$ outside a parenthesis, resulting in 

\begin{align}
0 = & \sum_i \int dx_1 \delta \psi_i^*(x_1) 
\Big[
\textbf{h}(x_1) \psi_i(x_1)
+ \sum_j \psi_i(x_1) 
\int dx_2
\frac{1}{r_{12}} \psi_j^*(x_2) \psi_j(x_2)
\nonumber \\ &
- \sum_j \psi_j(x_1) \int dx_2 \frac{1}{r_{12}} \psi_j^*(x_2) \psi_i(x_2)
- \sum_j \epsilon_{ij} \psi_j(x_1)
\Big]
+ c.c .
\end{align}
We should be able to insert any reasonable set of $\psi_i$ into this equation
and find a minimum of the Lagrangian. This means that the terms inside the
bracket are the ones that should be zero. If the content of the
brackets are zero, then the complex conjugate of this will also be
zero. This may not hold if the content inside the brackets are purely
imaginary. However we will not be dealing with such a situation. \\

We can thus put the content inside the bracket equal to zero, and set
$\psi_j^*(x_2) \psi_j(x_2) = |\psi_j(x_2)|^2$, yielding

\begin{align}
0 = &
\textbf{h}(x_1) \psi_i(x_1)
+ \sum_j \psi_i(x_1) 
\int dx_2
\frac{1}{r_{12}} |\psi_j(x_2)|^2
\nonumber \\ &
- \sum_j \psi_j(x_1) \int dx_2 \frac{1}{r_{12}} \psi_j^*(x_2) \psi_i(x_2)
- \sum_j \epsilon_{ij} \psi_j(x_1) .
\end{align}
We can rewrite the latter as

\begin{align}
\sum_j \epsilon_{ij} \psi_j(x_1) = &
\textbf{h}(x_1) \psi_i(x_1)
+ \sum_j \left[ 
\int dx_2
\frac{1}{r_{12}} |\psi_j(x_2)|^2
\right]
\psi_i(x_1)
\nonumber \\ &
- \sum_j \left[ \int dx_2 \frac{1}{r_{12}} \psi_j^*(x_2) \psi_i(x_2) \right] \psi_j(x_1) .
\label{hfequgogo}
\end{align}
It is common to define two operators, $\textbf{J}$ and $\textbf{K}$, to make this equation more compact. 
These operators are defined as 

\begin{equation}
\textbf{J}_j(x_1) \equiv \int dx_2
\frac{1}{r_{12}} |\psi_j(x_2)|^2 . \label{JJdefinition}
\end{equation}
and

\begin{equation}
\textbf{K}_j(x_1) \psi_i(x_1) \equiv 
\left[ \int dx_2 \frac{1}{r_{12}} \psi_j^*(x_2) \psi_i(x_2) \right] \psi_j(x_1) .
\label{KKdefinition}
\end{equation}
Using these definitions in   Eq.~\eqref{hfequgogo} results in

\begin{equation}
\sum_j \epsilon_{ij} \psi_j(x_1) = 
\left[
\textbf{h}(x_1)  
+ \sum_j \textbf{J}_j(x_1) 
- \sum_j \textbf{K}_j(x_1) 
\right]
\psi_i(x_1)  .
\label{dfjkaahagagyagyaaaaaaffaa}
\end{equation}
The content of the brackets on the right hand side of the equation will be defined as the Fock operator, namely 

\begin{equation}
\textbf{F}(x_1) \equiv \textbf{h}(x_1)  + \sum_j \textbf{J}_j(x_1) - \sum_j \textbf{K}_j(x_1) . 
\end{equation}
Since we require that our single-particle orbitals should be
orthonormal, $\epsilon$ is a diagonal matrix, that is $\epsilon_{ij}
= \delta_{ij} \times \epsilon_i$. This means we can remove the sum
over $j$. The only term to survive on the left hand side of
Eq.~\eqref{dfjkaahagagyagyaaaaaaffaa} is thus given by $i = j$,

\begin{equation}
\textbf{F}(x_1) \psi_i(x_1) = \epsilon_{i} \psi_i(x_1) . \label{insert_here}
\end{equation}
The term $\epsilon_{i}$ becomes the eigenvalues of the Fock
operator. This means we have an eigenvalue problem. The operator
$\textbf{F}$ is defined in terms of $\psi_i$, but to find $\psi_i$ we
need the operator $\textbf{F}$. This is a circular problem, which can
be solved iteratively.

\section{Restricted Hartree Fock}
The Hartree-Fock-Roothan method, Ref.~\cite{rothaan_new_citation}, is one way of
solving the HF equations when the spin is restricted (that is all spin orbitals are occupied, resulting in a total spin and angular momentum equal to zero). What we do is to
choose a basis set of predefined functions that will be our guess for the 
atomic orbitals, $\phi_{\mu}$. These will be discussed in the next
chapter. For the present discussion we simply state that the basis functions we choose are
usually not orthonormal. We want the atomic orbitals to define our
molecular orbitals $\psi_i$, 

\begin{equation}
\psi_i(x_1) = \sum_{\mu}^M C_{i \mu} \phi_{\mu}(x_1) .
\end{equation}
The lefthand side here is a molecular orbital, whereas the righthand side involves atomic orbitals $\phi$. We have $M$ such atomic orbitals. Inserting these into the Fock equation, Eq.~\eqref{insert_here}, we arrive at

\begin{equation}
\textbf{F}(x_1) \sum_{\mu}^M C_{i \mu} \phi_{\mu}(x_1) = \epsilon_{i} \sum_{\mu}^M C_{i \mu} \phi_{\mu}(x_1) .
\end{equation}
We then multiply by $\phi_v^*(x_1)$ and integrate both sides. We also pull the sum over $\mu$ and $C_{i \mu}$ 
outside the integral, resulting in

\begin{equation}
\sum_{\mu}^M C_{i \mu} \int dx_1 \phi^*_{v}(x_1) \textbf{F}(x_1) \phi_{\mu}(x_1)
= \epsilon_{i} \sum_{\mu}^M C_{i \mu} \int dx_1 \phi^*_{v}(x_1) \phi_{\mu}(x_1) .
\label{fock_to_solve}
\end{equation}
The righthand side is again not equal to $\delta_{v \mu}$ since the basis functions are usually not orthonormal. It can however be represented as a matrix element $C_{r \mu} S_{\mu v}$, where $S$ is known as the overlap. 
The integral on the left handside is equivalent to a matrix element $F_{\mu v}$, 

\begin{equation}
\sum_{\mu}^M F_{\mu v} C_{\mu i} 
= \epsilon_{i} \sum_{\mu}^M S_{\mu v} C_{\mu i}  .
\end{equation}
Here we have defined the matrix element $F_{\mu v}$ to be equal to

\begin{equation}
F_{\mu v}(x_1) = \int dx_1 \phi^*_{v}(x_1) \textbf{F}(x_1) \phi_{\mu}(x_1) ,
\label{hfreferancekk}
\end{equation}
and $S$ is defined as

\begin{equation}
S_{\mu v} = \int dx_1 \phi_{\mu}^*(x_1) \phi_v(x_1) .
\end{equation}
On matrix form the equation becomes

\begin{equation}
FC = SC \epsilon , \label{FOCK_EQUATION_STUFF}
\end{equation}
where $\epsilon$ is a diagonal matrix. This equation is a matrix
problem, and matrix problems are generally well suited to be handled on computers. \\

We should also mention briefly that $\epsilon_{i}$ physically comes to
represent how much energy is required to remove an electron out of
orbital $i$. The highest occupied molecular orbital (HOMO) will then
be the energy required to remove the most loosely bound electron from say an atom. This defines the simplest possible approximation to the  ionization
energy, according to Koopmans Theorem',
\cite{KoopmansTheorem}. Koopmans Theorem' only works for spin
restricted HF. \\

In Eq.~\eqref{hfreferancekk}, the quantity $S$ becomes the overlap
between basis functions, and does not change during iterations. The quantity $C$
becomes a coefficient, and changes in each iteration. This applies to the Fock
matrix elements as well. \\

Now we would like to make use of our spin restriction to simplify things even further. The quantity $\textbf{F}$ was defined as

\begin{equation}
\textbf{F}(x_1) = \textbf{h}(x_1)  + \sum_j^N \textbf{J}_j(x_1) - \sum_j^N \textbf{K}_j(x_1) , \label{fock_refer}
\end{equation}
with $\textbf{J}$ and $\textbf{K}$ defined in
Eqs.~\eqref{JJdefinition} and \eqref{KKdefinition}. Our molecular spin
orbitals have two possible spin orientations, either spin up or spin down. We want to
restrict spin so that total spin is zero. We also want specifically
each spacial orbital to be occupied by one spin up and one spin down
particle. This means in total that half the electrons will have spin up and
the other half spin down. \\

We will use this spin restriction to simplify our operators $\textbf{J}$ and $\textbf{K}$. If we look at the definition of $\textbf{J}$ first,

\begin{equation}
\textbf{J}_j(x_1) = \int dx_2
\frac{1}{r_{12}} |\psi_j(x_2)|^2 . \label{Jdefinition}
\end{equation}
we notice that $\textbf{J}$ depends only on the spin orbital $j$. Orbital $j$ obviously 
has the same spin orientation as itself. This means we can add a factor 2 in front of $\textbf{J}$ in Eq. \eqref{fock_refer} and only sum over $\frac{N}{2}$, resulting in

\begin{equation}
\textbf{K}_j(x_1) \psi_i(x_1) =
\left[ \int dx_2 \frac{1}{r_{12}} \psi_j^*(x_2) \psi_i(x_2) \right] \psi_j(x_1) .
\label{Kdefinition}
\end{equation}
The quantity $\textbf{K}$ depends however on orbitals $i$ and $j$. If
orbital $i$ has its spin orientation defined, then the integral will
be equal to zero whenever orbital $j$ does not have the same spin
orientation. This occurs half the time. We can still restrict the sum
to only go over $\frac{N}{2}$ and add a factor 2, but must also add a
$\frac{1}{2}$ for this reason. These two cancels out, and results in 
Eq.~\eqref{fock_refer} for the spin restricted case
to be equal to 

\begin{equation}
\textbf{F}(x_1) = \textbf{h}(x_1)  + 2 \sum_j^{\frac{N}{2}} \textbf{J}_j(x_1) - \sum_j^{\frac{N}{2}} \textbf{K}_j(x_1)  .
\end{equation}
We also insert the molecular orbitals as a linear combination of atomic orbitals in the matrix element $F_{\mu v}$, giving

\begin{equation}
F_{\mu v} = h_{\mu v} 
+ \sum_j^{\frac{N}{2}} \sum_{r s}^M
C_{r j} C_{s j}^*
\left( 2 \langle \mu r | v s \rangle 
- \langle \mu s | v r \rangle \right) ,
\label{Fock_Restricted_1}
\end{equation}
with 

\begin{equation}
h_{\mu v} = \int dx_1 \phi_{\mu}^*(x_1) \textbf{h} \phi_v(x_1) ,
\end{equation}
and

\begin{equation}
\langle \mu v | r s \rangle = \int dx_1 \int dx_2 \phi_{\mu}^*(x_1) \phi_v^*(x_2) \frac{1}{r_{12}} \phi_r(x_1) \phi_s(x_1) .
\end{equation}
The Fock matrix elements are now defined by atomic orbitals. To get
implementation ready equations we must define these atomic orbitals,
and solve the integrals using them. This will be done in the next
chapter. We note the sum over $\frac{N}{2}$ and $M$, where $N$ is the
number of electrons and M is the number of basis functions. 

\section{Unrestricted Hartree Fock}
The Hartree-Fock equations can also be solved without restricting each
spin orbital to be occupied by two electrons. In this section we will
derive the Pople-Sesbet equations, Ref.~\cite{ref222222}.  These equations achieves exactly
this. \\

In Unrestricted Hartree-Fock (UHF) case we can define two sets of
molecular spin orbitals. One set of occupied orbitals with spin up,
$\{ \psi_i^{\alpha} \}$, and another set of occupied orbitals with
spin down, $\{ \psi_i^{\beta} \}$. The total set of molecular spin
orbitals contains both of these, that is

\begin{equation}
\{ \psi_i \} = 
\left\{
	\begin{array}{ll}
		\{ \psi_j^{\alpha} \} \\
		\{ \psi_j^{\beta}  \}
	\end{array}
\right .
\end{equation}
Inserted into Eq.~\eqref{insert_here}, we obtain

\begin{equation}
\textbf{F}^{\alpha} \psi_i^{\alpha}(x_1) = \epsilon_i^{\alpha} \psi_i^{\alpha}(x_1) ,
\end{equation}
and
\begin{equation}
\textbf{F}^{\beta} \psi_i^{\beta}(x_1) = \epsilon_i^{\beta} \psi_i^{\beta}(x_1) .
\end{equation}
We applied the spin restriction in the definition of $\textbf{F}$. 
This expression will be different, otherwise our equations remain the same, that is

\begin{equation}
F^{\alpha} C^{\alpha} = S C^{\alpha} \epsilon^{\alpha} ,
\end{equation}
and
\begin{equation}
F^{\beta} C^{\beta} = S C^{\beta} \epsilon^{\beta} .
\end{equation}
The Fock operator is defined in Eq. \eqref{fock_refer} and depends on
$\textbf{h}$, $\textbf{J}$ and $\textbf{K}$. We used our spin
approximation in the expression for $\textbf{J}$ and $\textbf{K}$

\begin{equation}
\textbf{J}_j(x_1) = \int dx_2
\frac{1}{r_{12}} |\psi_j(x_2)|^2 . \label{Jdefinition}
\end{equation}
The quantity $\textbf{J}$ integrates over spin orbital $j$. This
orbital can have spin up or spin down. This will be independent of the
spin orientation of orbital $i$, meaning that  we can split this
expression in two terms, summing thereby over the occupied up spin orbitals and the
occupied down spin orbitals separately. \\

The quantity
$\textbf{K}$ from Eq. \eqref{KKdefinition} on the other hand involved spin orbitals $i$ and $j$. This will follow the same 
argument as for the spin restricted case, resulting in the expression for the Fock matrix elements to be

\begin{equation}
F_{\mu v}^{\alpha} = 
h_{\mu v}
+ \sum_j^{N_{\alpha}} 
\sum_{rs}^M C_{rj}^{\alpha} \left(C_{sj}^{\alpha}\right)^* 
(\langle \mu r | v s \rangle 
- \mu r | s v \rangle)
+ \sum_j^{N_{\beta}} 
\sum_{rs}^M C_{rj}^{\beta} \left(C_{sj}^{\beta} \right)^* 
\langle \mu r | vs \rangle .
\label{Fock_Restricted_2}
\end{equation}
and
\begin{equation}
F_{\mu v}^{\beta} = 
h_{\mu v}
+ \sum_j^{N_{\beta}} 
\sum_{rs}^M C_{rj}^{\beta} \left(C_{sj}^{\beta}\right)^* 
(\langle \mu r | v s \rangle 
- \mu r | s v \rangle)
+ \sum_j^{N_{\alpha}} 
\sum_{rs}^M C_{rj}^{\alpha} \left(C_{sj}^{\alpha} \right)^* 
\langle \mu r | vs \rangle .
\label{Fock_Restricted_3}
\end{equation}
As in the restricted Hartree-Fock case we have the Fock matrix defined by atomic orbitals. Now it is time to 
define the atomic orbitals. This is the aim of the next chapter.

\chapter{Gaussian Type Orbitals}

In 1950 Boys, \cite{boys_original_work_stuff}, proposed the use of
Gaussian Type Orbitals (GTOs) in electronic structure theory. Years
after his proposal the use of  Gaussian Type Orbitals are
now standard in computational chemistry. In this chapter we will
define what GTOs are and examine in detail how to construct them. We
will also look at the mathematical expressions required for solving
the integrals left open in the previous chapter. In total we will
present all the required programmable equations for calculating
energies with Hartree Fock. The content exposed here follows closely the work of
T.~Helgaker, P.~Jorgensen and J.~Olsen, \cite{helg1,helg2,helg3}. \\

The reader may also find additional and useful information in the 
articles by McMurcie and Davidson \cite{helg4} and Pople and Hehre \cite{helg5}.

\section{Contracted GTOs}
Two  of the  basic ingredients in the formalism exposed here are the  so-called contracted and primitive GTOs. 
A contracted GTO is used to describe an atomic orbital and is defined as

\begin{equation}
\phi(x,y,z) = \sum_{i} N_i \chi_i(x,y,z) . \label{GTO_EQUATION}
\end{equation}
Here $\phi_i$ represents a contracted GTO, $N_i$ is its normalization constant and $\chi_i$ is a primitive GTO. A primitive GTO is defined as

\begin{equation}
\chi_i(x,y,z) = c_i x^m y^n z^o e^{-\alpha_i R^2} , \label{primitivegto}
\end{equation}
where $x$, $y$ and $z$ are Cartesian coordinates and $R^2 = x^2 + y^2
+ z^2$. These coordinates represent the distance to a given nucleus, while  $m$, $n$
and $o$ depend on the angular momentum of the orbital we wish to
describe. When the primitives are defined with $x^m y^n z^o$ they are
 called Cartesian Gaussian functions. We will only be dealing
with these kind of Gaussians and $m$, $n$ and $o$ take only integer values, that is

\begin{equation}
m + n + o = l , 
\end{equation}
where $l$ is the total angular momentum. The parameters $\alpha_i$ and $c_i$ are variational parameters. \\

A contracted GTO is a linear combination of primitive GTOs. The goal
of making contracted GTOs is to mimic the behaviour of a Slater Type
Orbital (STO). An STO is considered to resemble the true atomic
orbitals. We will see later that GTOs allow us to perform
calculations much faster. For this reason we want to use GTOs, but we
want the behaviour of an STO. An STO is defined as

\begin{equation}
\Phi(r) = N R^m e^{-\alpha R} ,
\end{equation}
where $N$ is the normalization constant, $R$ is the distance from the
electron to the nucleus, $m$ depends on angular momentum and $\alpha$
is again a variational parameter. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{sto_vs_gto_1.eps}}
\caption{Illustration of the shape of a primitive GTO vs a STO}
\label{fig:STOvsGTO1}
\end{center}
\end{figure}

Figure \ref{fig:STOvsGTO1} is an illustration of the shape of a
primitive GTO side by side of an STO. We notice that they behave differently. We therefore make a linear combination of
primitive GTOs, as is shown in figure \ref{fig:STOvsGTO2}. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{sto_vs_gto_2.eps}}
\caption{Illustration of the construction of a contracted GTO from three primitives}
\label{fig:STOvsGTO2}
\end{center}
\end{figure}

The contracted GTO is made up of three primitives. The problem with
GTOs is that they fall off to quickly for increasing $R$ compared  to
the STO. Increasing $R$ is known as a long-range behaviour. Also when $R$
goes to zero GTO and STO behave differently. \\

The problems of long and short range behaviour are reduced when going
from a single primitive GTO to a contracted GTO of three
primitives. In theory we can describe an STO with increasing accuracy by
adding more primitive GTOs with different $\alpha_i$. The parameters $\alpha_i$
control the width of the primitive GTO.  \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{sto_vs_gto_3.eps}}
\caption{Illustration of how an increased number of primitives improve our contracted GTO}
\label{fig:STOvsGTO3}
\end{center}
\end{figure}

In figure \ref{fig:STOvsGTO3} we have made a comparison of a
contracted GTO made up of three primitives, versus a contracted GTO
made up of six primitives. We see that the more primitives the better our
GTO becomes relative to the original STO it is meant to represent. As mentioned previously, an STO is
considered to behave like a true atomic orbital (AO). We therefore have a
good argument for using GTOs to describe AOs. However, this requires
that we know how to get the right primitives.

\section{Variational Principle}
Constructing good primitives means defining $\alpha_i$ and
$c_i$. These are variational parameters, and in theory we can use the
variational principle. The variational principle states that

\begin{equation}
E_0 \le \frac{\int \langle \psi_T | \textbf{H} | \psi_T \rangle}{\int \langle \psi_T | \psi_T \rangle} .
\end{equation}
This means that we can optimize our variational parameters, by
minimizing the energy as a function of $\alpha_i$ and $c_i$. There is
a huge computational cost attached to this, since the number of
variational parameters scales quickly with increasing number of
primitives. 

\section{EMSL}
The software library EMSL \cite{emsl_stuffies,emsl_stuffies2,emsl_stuffies3} provides already calculated $\alpha$ and $c$. 
We will make use of these pre-computed
parameters in our calculations.

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{EMSL.eps}}
\caption{Front page of the EMSL website}
\label{fig:EMSL}
\end{center}
\end{figure}

When entering the basis set exchange we must select two options. First
what basis set, listed on the left in figure \ref{fig:EMSL}. Secondly,
which atom(s) we will study. What the different basis sets represent will
be explained later, but for now let us examine how to read data from
EMSL. If we click on the 3-21G basis set and Hydrogen and then "Get Basis
Set" we will get the basis set seen in figure \ref{fig:3-21G_H}. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{3-21G_H.eps}}
\caption{The 3-21G basis set for Hydrogen}
\label{fig:3-21G_H}
\end{center}
\end{figure}

The first line of interest is\\

\begin{equation}
\text{BASIS "ao basis" PRINT} \nonumber
\end{equation}
which means that this is a basis for atomic orbitals. Two lines down
we see two letters, H and S. H means this is a basis for the hydrogen
atom. S means this is an S orbital, which means that the angular
momentum is 0 for all primitive GTOs that define this orbital. Angular
momentum of 0 means that $m$, $n$ and $o$ are zero in
Eq. \eqref{primitivegto}. \\

The next two lines are filled with four numbers. Each line contains
one $\alpha$ value (left number) and one $c$ value (right
value). Inserting these four numbers in Eq.~\eqref{primitivegto} we
obtain our first two primitive GTOs

\begin{equation}
\chi_1(x,y,z) = 0.1562850 \times exp(-5.4471780 \times (r-R_H)^2) ,
\end{equation}
and
\begin{equation}
\chi_2(x,y,z) = 0.9046910 \times exp(-0.8245470 \times (r-R_H)^2) .
\end{equation}
These can be combined to a contracted GTO which will represent the first atomic orbital.

\begin{equation}
\phi_1(x,y,z) = N_1 \chi_1 + N_2 \chi_2 .
\end{equation}

The next line represents another atomic orbital with angular momentum zero. However this contains only one primitive GTO, 

\begin{equation}
\phi_2(x,y,z) = N_1 \times 0.1562850 \times exp(-5.4471780 \times (r-R_H)^2) .
\end{equation}

It may seem confusing why a hydrogen atom with only one electron would
need two atomic orbitals. This will be explained in a later
section. Further notation from EMSL can be noted in figure
\ref{fig:6-31G_Be}. 

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{6-31G_Be.eps}}
\caption{The 6-311G basis set for Beryllium}
\label{fig:6-31G_Be}
\end{center}
\end{figure}

Figure \ref{fig:6-31G_Be} contains the data  for Be from the 6-311G basis set. The first atomic
orbital is an S orbital with angular momentum zero and a contracted
GTO of six primitive GTOs. These six primitives together actually
construct the blue line plotted in figure \ref{fig:STOvsGTO3}. \\

The next orbital is marked as SP. This is a short notation for one S
orbital and one P. The notation means that the left column represents
the $\alpha$ values for both orbital S and P. The second column are
$c$ values for the S orbital, whereas the third column are $c$ values
for the P orbital. In this basis set the S and P orbital share
$\alpha$ values. This is a common feature. The basis set is designed
like this to allow for a more efficient
implementation. \\

However, for now our interest is the notation on the EMSL website. The P
orbital represents an angular momentum of 1, which means $m + n + o =
1$. This can be achieved by either $m=1$, $n=1$ or $o=1$. \\

When we make our basis set all of these possibilities must be available. This means a P orbital is really 3 atomic orbitals, but all 3 orbitals have the same $\alpha_i$ and $c_i$. One of them will have $m=1$, $n=0$ and $o=0$. The second will have $m=0$, $n=1$ and $o=0$. The third will have $m=0$, $n=0$ and $o=1$. The 6-311G basis set therefore has a total of 13 orbitals for an atom like beryllium.\\

Different letters represent different angular momentum on EMSL. This means the different letters also represent a different number of atomic orbitals, as indicated in table here. \\

\begin{center}
\begin{tabular}{ l c r }
\hline
  Letter & Ang mom & Nr of Orbitals \\ \hline
  S & 0 & 1 \\
  P & 1 & 3 \\
  D & 2 & 6 \\
  F & 3 & 10 \\
  G & 4 & 15 \\
  H & 5 & 21 \\
  \hline
\end{tabular}
\end{center}

The number of orbitals is increasing because of the different ways to
arrange $m$, $n$ and $o$ to achieve the given angular momentum. The
general number of orbitals for an angular momentum $l$ is
$\frac{(l+1)(l+2)}{2}$. The next table represent the different ways of
organizing $m$, $n$ and $o$ for the D orbital.\\

\begin{center}
\begin{tabular}{ l c r }
\hline
  m & n & o \\ \hline
  2 & 0 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 2 \\
  1 & 0 & 1 \\
  1 & 1 & 0 \\
  0 & 1 & 1 \\
  \hline
\end{tabular}
\end{center}

The same method can be applied to F, G, H and higher orbitals.  \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{emslillu.eps}}
\caption{Illustration of another possible EMSL notation}
\label{fig:emsl2}
\end{center}
\end{figure}

Figure \ref{fig:emsl2} shows part of the aug-cc-pCV5Z basis set for
carbon. The interesting part is that we now have three columns but only an
S orbital represented. When this notation occurs it means there are
two S orbitals with identical $\alpha$ values but different $c$
values. Some basis sets lists seven or eight columns, but the same principle
applies. The first column stands for the  $\alpha$ values. The next one represents  the $c$ values,
where each column contains $c$ values for its own orbital.

\section{Product of Gaussians \label{overlap_section}}
In this section we will derive the normalization constant. The
normalization constant is defined such that the inner product is equal
to 1, that is

\begin{equation}
|\langle \phi_i | \phi_i \rangle|^2 = 1  ,
\end{equation}
with

\begin{equation}
\phi(x,y,z) = \sum_{i} N_i \chi_i(x,y,z)  , 
\end{equation}
where $\chi_i$ are the primitive GTOs. This means we can calculate the
integral over contracted GTOs by first calculating the integral over
two primitive GTOs 

\begin{equation}
\chi_1 = c_1 x_A^i y_A^j z_A^k exp(-\alpha_1 \textbf{r}_A^2)  ,
\end{equation}
and
\begin{equation}
\chi_2 = c_2 x_B^m y_B^n z_B^o exp(-\alpha_2 \textbf{r}_B^2)  .
\end{equation}
Here $\textbf{r}_A$ = $\textbf{r} - \textbf{A}$ and $\textbf{A}$ is the position of nucleus A. The primitive $\chi_1$ is part of a contracted GTO that describes an atomic orbital in nucleus A. Similar relations apply 
 for $\textbf{r}_B$. The product of $\chi_1$ and $\chi_2$ is well defined,

\begin{equation}
\chi_1 \chi_2 = c_1 c_2 x_A^i x_B^m y_A^j y_B^n z_A^k z_B^o exp(-(\alpha_1 \textbf{r}_A^2 + \alpha_2 \textbf{r}_B^2))  .
\end{equation}
A key feature of Gaussian functions is that a product of two is equal to another Gaussian. This is shown by finding the "charge center" $P$ 

\begin{equation}
\textbf{P} = \frac{\alpha_1 \textbf{A} + \alpha_2 \textbf{B}}{\alpha_1 + \alpha_2}  .
\end{equation}
Defining $\textbf{r}_P = r - P$ we can rewrite the exponential term as

\begin{equation}
exp(-(\alpha_1 \textbf{r}_A^2 + \alpha_2 \textbf{r}_B^2)) = G_{IJ} exp(-\alpha_p \textbf{r}_P) , 
\end{equation}
where

\begin{equation}
G_{IJ} = exp(-\alpha_1 \alpha_2 \alpha_p^{-1} |\textbf{A} - \textbf{B}|^2)  ,
\end{equation}
is independent of $r$. We now realise that the product $\chi_1 \chi_2$ can be separated in its three Cartesian coordinates, $x$, $y$ and $z$ with $\textbf{r}_P = (x_P, y_P, z_P)$. This results in

\begin{equation}
\chi_1 \chi_2 = G_{IJ} \chi_X \chi_Y \chi_Z , \label{overlap}
\end{equation}
where

\begin{equation}
\chi_X = x_A^{i} x_B^m exp(-\alpha_p x_P^2) ,
\end{equation}
and similar for $\chi_Y$ and $\chi_Z$. We now define 

\begin{equation}
\Lambda_j (x_p, \alpha_p) exp(-\alpha_p x_p^2) = (\frac{\partial}{\partial P_x})^j exp(-\alpha_p x_p^2)  .
\end{equation}
This relates to a Hermite polynomial $H_j$ as such

\begin{equation}
\Lambda_j (x_p, \alpha_p) = \alpha_p^{j/2} H_j(\alpha_p^{1/2}x_P)  .
\end{equation}
The purpose of this definition is to replace $x_A^{i} x_B^m$ with derivatives of $P_x$ which can be placed outside an integral.\\

We wish to expand $x_A^{i} x_B^m$ as such:

\begin{equation}
x_A^i x_B^m = \sum_{N=0}^{i+m} E_N^{i,m} \Lambda(x_P, \alpha_p) . \label{recurslabel}
\end{equation}
The first 5 Hermite polynomials are (see also \cite{hermite_stuffies}):

\begin{equation}
H_0(x) = 1 . 
\end{equation}

\begin{equation}
H_1(x) = 2x . 
\end{equation}

\begin{equation}
H_2(x) = 4x^2 - 2 . 
\end{equation}

\begin{equation}
H_3(x) = 8x^3 - 12x . 
\end{equation}

\begin{equation}
H_4(x) = 16x^4 - 48x^2 + 12 . 
\end{equation}
We notice from these that the following recursion relation holds:

\begin{equation}
H_{N+1}(x)  = 2x H_N(x) - 2N H_{N-1}(x) .
\end{equation}
The latter results in another recursion relation:

\begin{equation}
x_A \Lambda_N(x_P,\alpha_p) = N\Lambda_{N-1} + (P_x - A_x) \Lambda_N + \frac{1}{2\alpha_p} \Lambda_{N+1} . \label{recurs}
\end{equation}
We can combine Eq. \eqref{recurslabel} with Eq. \eqref{recurs} and find the following recursion relations for $E_N^{i,m}$.

\begin{equation}
E_N^{i+1, m} = \frac{1}{2\alpha_p} E_{N-1}^{i,m} + (P_x - A_x) E_N^{i,m} + (N + 1) E_{N+1}^{i,m} ,
\end{equation}
and
\begin{equation}
E_N^{i, m+1} = \frac{1}{2\alpha_p} E_{N-1}^{i,m} + (P_x - B_x) E_N^{i,m} + (N + 1) E_{N+1}^{i,m} ,
\end{equation}
where $E_0^{0,0} = 1$ and $E_N^{i,m} = 0$ for N > i+m and N < 0. We can use similar recursive relations for $y_A^j y_B^n$ and $z_A^k z_B^o$. We have 

\begin{equation}
y_A^j y_B^n = \sum_{N=0}^{j+n} E_{N}^{j,n} \Lambda_N(y_p, \alpha_p) ,
\end{equation}
and
\begin{equation}
z_A^k z_B^o = \sum_{N=0}^{k+o} E_{N}^{k,o} \Lambda_N(z_p, \alpha_p) .
\end{equation}
These equations are then used to rewrite Eq. \eqref{overlap} as

\begin{equation}
\chi_1 \chi_2 = G_{IJ} \sum_{N,M,L} E_N^{i,m} E_L^{j,n} E_M^{k,o} \Lambda_N(x_P) \Lambda_L(y_P) \Lambda_M(z_P) exp(-\alpha_p \textbf{r}_p^2) .
\label{referequequequ}
\end{equation}
The integral over $\chi_1 \chi_2$ can be calculated easily. We will label this integral $S_{1,2}$,

\begin{equation}
S_{1,2} = \int dr \chi_1 \chi_2 = \int dr G_{IJ} \chi_X \chi_Y \chi_Z  = G_{IJ} S_{i,m} S_{j,n} S_{k,o}  . \label{Overlap_EquEqu}
\end{equation}
A general integral over an Hermitian Gaussian function is given as

\begin{equation}
\int dx \Lambda_N(x_P, \alpha_p) exp(-\alpha x_P^2) = \delta_{N,0} \left( \frac{\pi}{\alpha_p} \right)^{1/2} 
.
\end{equation}
Integrating over the $x$, $y$ and $z$ coordinates separately and placing the derivatives outside the integral results in
 the following relations

\begin{equation}
S_{i,m} = \int dx \phi_X = \sum_{N=0}^{i+m} E_N^{i,m} \int \Lambda_N(x_p) dx ,
\end{equation}
and 
\begin{equation}
S_{i,m} = \sum_{N=0}^{i+m} E_N^{i,m} \delta_{N,0} \left( \frac{\pi}{\alpha_p} \right)^{1/2} .
\end{equation}
Here only one term will survive from this sum. When N = 0, we have 

\begin{equation}
S_{i,m} = E_0^{i,m} \left( \frac{\pi}{\alpha_p} \right)^{1/2} .
\end{equation}
We get similar results for $S_{j,n}$ and $S_{k,o}$, namely

\begin{equation}
S_{1,2} = G_{IJ} E_0^{i,m} E_0^{j,n} E_0^{k,o} \left( \frac{\pi}{\alpha_p} \right)^{3/2} .
\end{equation}
It is very common to insert $G_{IJ}$ into $E_0^{0,0}$. For the $x$ coordinate we will get 

\begin{equation}
E_0^{0,0} = G_{IJ,x} = exp\left(-\frac{\alpha_1 \alpha_2}{\alpha_1 + \alpha_2} |A_x - B_x|^2\right) ,
\label{important_hf1}
\end{equation}
with similar results for the $y$ and $z$ coordinates. Collecting all our results we have

\begin{equation}
S_{1,2} = E_0^{i,m} E_0^{j,n} E_0^{k,o} \left( \frac{\pi}{\alpha_p} \right)^{3/2} , \label{overlap_integral}
\end{equation}
where we to repeat the use of these relations for E:

\begin{equation}
E_N^{i+1, m} = \frac{1}{2\alpha_p} E_{N-1}^{i,m} + (P_x - A_x) E_N^{i,m} + (N + 1) E_{N+1}^{i,m} , \label{important_hf2}
\end{equation}
and
\begin{equation}
E_N^{i, m+1} = \frac{1}{2\alpha_p} E_{N-1}^{i,m} + (P_x - B_x) E_N^{i,m} + (N + 1) E_{N+1}^{i,m} . \label{important_hf3}
\end{equation}


\section{Normalization \label{normalization_section}}
The quantity $S$ is known as the overlap between primitive
GTOs. This means we have to  calculate the inner product
as

\begin{equation}
|\langle \phi_i | \phi_i \rangle |^2 = 1  ,
\end{equation}
with

\begin{equation}
\phi_i = \sum_j N_j \chi_j  .
\end{equation}
We can now calculate the normalization constants for orbital
$\phi_i$. We have the same orbital on the bra and ket sides, meaning in
this inner product the coordinates $\textbf{A}$ and $\textbf{B}$ will be
identical, as well as $c_i$, $\alpha_i$ and the angular momentum for the
primitives. With $\textbf{A}$ = $\textbf{B}$ we get $E_0^{0,0}$ = 1
and $\alpha_p$ = 2$\alpha_i$. \\

We can now calculate the normalization constant for $\langle \chi_j|\chi_j \rangle$ for different angular momenta.

\subsection{l = 0}
For $l = 0$ all primitive GTOs are 

\begin{equation}
\chi_j = c_j exp(-\alpha_j \textbf{r}^2) . 
\end{equation} 
The normalization constant can be calculated as 

\begin{equation}
1 = N^2 c^2 S = N_j^2 c_j^2 \left(E_0^{0,0}\right)^3 \left(\frac{\pi}{2\alpha_j}\right)^{3/2} .
\end{equation}
This results in

\begin{equation}
\Rightarrow N_{0,0,0} c = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} .
\end{equation}
Here the notation $N_{0,0,0}$ means it is the normalization constant for primitives with $m=0$, $n=0$ and $o=0$.

\subsection{l = 1}
For $l = 1$ all primitive GTOs will have one $E_0^{1,1}$ term and two
$E_0^{0,0} = 1$ terms. We must now use the recursive relations to find
the $E_0^{1,1}$ term. We have

\begin{equation}
E_0^{1,1} = 0 + 0 + E_1^{1,0} = \frac{1}{4\alpha_j}  ,
\end{equation}
which results in a normalization constant 

\begin{equation}
1 = N_j^2 c_j^2 4\alpha_j \left(\frac{\pi}{2\alpha_j}\right)^{3/2} 
\end{equation}
yielding

\begin{equation}
\Rightarrow N_{1,0,0} c_j = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{4\alpha_j} .
\end{equation}
Here the notation $N_{1,0,0}$ means primitives with $(m,n,o) = (1,0,0)$, $(0,1,0)$ or $(0,0,1)$.

\subsection{l = 2}
For $l = 2$ we will have two possibilities. Either $S_{ij} = E_0^{2,2} E_0^{0,0} E_0^{0,0} \left(\frac{\pi}{2\alpha_j}\right)^{3/2}$, or $S_{ij} = E_0^{1,1} E_0^{1,1} E_0^{0,0} \left(\frac{\pi}{2\alpha_j}\right)^{3/2}$. 
This results in two different normalization constants for primitives with $l = 2$. The first case is

\begin{equation}
1 = N_j^2 c_j^2 \left(\frac{\pi}{2\alpha_j}\right)^{3/2} E_0^{2,2} E_0^{0,0} E_0^{0,0} , 
\end{equation}
with $E_0^{2,2}$

\begin{equation}
E_0^{2,2} = E_1^{1,2} = \frac{1}{4\alpha_j} E_0^{0,0} + 2E_2^{0,2} = \frac{3}{4\alpha_j} E_1^{0,1} = \frac{3}{16 \alpha_j^2} 
\end{equation}
giving

\begin{equation}
\Rightarrow N_{2,0,0} c_j =  \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{\frac{16 \alpha_j^2}{3}} .
\end{equation}
The second case is 

\begin{equation}
1 = N_j^2 c_j^2 \left(\frac{\pi}{2\alpha_j}\right)^{3/2} E_0^{1,1} E_0^{1,1} E_0^{0,0} . 
\end{equation}
where

\begin{equation}
E_0^{1,1} = \frac{1}{4\alpha_j} 
\end{equation}
with 

\begin{equation}
\Rightarrow N_{1,1,0} c_j = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{16 \alpha_j^2} .
\end{equation}

\subsection{l = 3}
For $l = 3$ we have three possibilities, $N_{3,0,0}$, $N_{2,1,0}$ and $N_{1,1,1}$. 
First,$S_{ij} = E_0^{3,3} E_0^{0,0} E_0^{0,0} \left(\frac{\pi}{2\alpha_j}\right)^{3/2}$
with

\begin{equation}
E_0^{3,3} = E_1^{2,3} = \frac{1}{4\alpha_j} E_0^{1,3} + 2 E_2^{1,3} = \frac{3}{4\alpha_j} E_1^{0,3} + 4 E_2^{0,3} 
\end{equation}

\begin{equation}
\Rightarrow E_0^{3,3} = \frac{3}{16 \alpha_j^2} E_0^{0,2} + \frac{6}{4 \alpha_j} E_2^{0,2} + \frac{4}{4\alpha_j} E_1^{0,2} 
\end{equation}

\begin{equation}
\Rightarrow E_0^{3,3} = \frac{3}{16\alpha_j^2} E_1^{0,1} + \frac{6}{16 \alpha_j^2} E_1^{0,1} + \frac{4}{16 \alpha_j^2} E_0^{0,1} 
\end{equation}

\begin{equation}
\Rightarrow E_0^{3,3} = \frac{9}{64 \alpha_j^3} 
\end{equation}
which gives 

\begin{equation}
\Rightarrow N_{3,0,0} c_j = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{\frac{64 \alpha_j^3}{9}} .
\end{equation}

The second case is $S_{ij} = E_0^{2,2} E_0^{1,1} E_0^{0,0} \left(\frac{\pi}{2\alpha_j}\right)^{3/2}$, with

\begin{equation}
E_{0}^{2,2} = \frac{3}{16 \alpha_j^2}  .
\end{equation}
and 

\begin{equation}
E_0^{1,1} = \frac{1}{4\alpha_j} .
\end{equation}
The result is

\begin{equation}
\Rightarrow N_{2,1,0} c_j = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{\frac{64 \alpha_j^3}{3}} 
\end{equation}
Finally, $S_{ij} = E_0^{1,1} E_0^{1,1} E_0^{1,1} \left(\frac{\pi}{2\alpha_j}\right)^{3/2}$ with

\begin{equation}
E_0^{1,1} = \frac{1}{4\alpha_j}  
\end{equation}
resulting in
\begin{equation}
\Rightarrow N_{1,1,1} c_j = \left( \frac{2\alpha_j}{\pi} \right)^{3/4} \sqrt{64 \alpha_j^3} .
\end{equation}

\subsection{Final normalization comments}
We placed all the normalization constants  next to the parameter c, and all the normalization constants are left as a function of $\alpha$. This means in practice we can simply multiply this normalization in with the parameter c, and combine them. \\

The derivation of normalization factors for $l = 4, 5, \dots$ are performed similarly.

\section{Calculating Integrals for Hartree Fock}
There were four integrals left untouched in the previous Hartree Fock chapter. All of these integrals involved AOs, which we will be approximating as GTOs. In this section we  provide the analytical formula for solving these integrals, using GTOs. The solutions will provide insights into why GTOs are so popular in quantum chemistry. 

\subsection{Overlap}
The overlap integrals are already solved during our quest to find the normalization constants.
They are given as

\begin{equation}
S_{pq} = \sum_I^p \sum_J^q c_I c_J N_I N_J E_0^{i,m} E_0^{j,n} E_0^{k,o} \left( \frac{\pi}{\alpha_p} \right)^{3/2}  ,
\end{equation}
with $\alpha_p = \alpha_I + \alpha_J$ and the sum over $I$ and $J$ sums over
the primitives that define the contracted GTOs, $p$ and $q$.

\subsection{Kinetic Energy}
The single-particle operator $\textbf{h}$, as seen in Eq. \eqref{single_particle_hf}, contained two terms. The kinetic energy part of this operator is calculated from the integral

\begin{equation}
-\frac{1}{2} \langle \phi_p | \nabla^2 | \phi_r \rangle .
\end{equation}
We again insert the primitives $\chi$, and sum over them later, such that we need to calculate 

\begin{equation}
- \frac{1}{2} \langle \chi_a | \nabla^2 | \chi_b \rangle . \label{tempequequ}
\end{equation}
$\nabla^2$ acts on the right GTO, which can be split into its x, y and z components.

\begin{equation}
\chi_b = x^m y^n z^o 
e^{\alpha_b R^2}
= \chi_{b,x} \chi_{b,y} \chi_{b,z} ,
\end{equation}
with

\begin{equation}
\chi_{b,x} = x^m e^{\alpha_b x^2} ,
\end{equation}
and similar for $\chi_{j,y}$ and $\chi_{j,z}$. We also define

\begin{equation}
\chi_a = x^i y^j z^k e^{\alpha_a R^2} .
\end{equation}
Here $\nabla$ can also be split into $x$, $y$ and $z$ components, with the mathematics of all three components being similar. We therefore just look at the $x$ component

\begin{align}
\nabla_x^2 \chi_{b,x} = & \nabla_x^2 
\left[
x^m e^{\alpha_b x^2}
\right] \nonumber \\
= & 
4 \alpha_b^2 x^{m+2} e^{-\alpha_b x^2}
- 2 \alpha_b (2 m + 1) x^m e^{-\alpha_b x^2} \nonumber \\ &
+ m (m-1) x^{m-2} e^{-\alpha_b x^2} .
\end{align}
Here we have taken the derivative. We notice all the terms have $e^{-\alpha_b x^2}$ present, and a few constant terms. Also there is a change in the power of $x$. We can now insert the results from section \ref{overlap_section}, resulting in 

\begin{align}
\langle \chi_{a,x} | \nabla_x^2 | \chi_{b,x} \rangle
= 4 \alpha_b^2 S_{i,m+2} 
- 2 \alpha_b (2 m + 1) S_{i,m} 
+ m (m - 1) S_{i,m-2} .
\label{EKintegralsss}
\end{align}
For the $y$ direction we will have the same result, except the index $i$ will be replaced by $j$, and $m$ will be replaced by $n$. For the $z$ coordinate $i$ will be replaced by $k$ and $m$ will be replaced by $o$. We notice the kinetic energy is simply a linear combination of quantities already calculated in the overlap. This will be used in the implementation. 

\subsection{Nuclei-Electron interaction \label{nuclei-electron-section}}
The second piece of the single particle operator, $\textbf{h}$, was a nuclei-electron interaction term. The integral to solve here is

\begin{equation}
\langle \phi_p | \sum_A \frac{Z_A}{r_{iA}} | \phi_r \rangle .
\end{equation}
The sum over $A$ and $Z_A$ may be placed outside the integral. We can
also insert the primitives and solve the integral based on them, and
sum over all primitives later. We rename the distance from the
electron to the nuclei $r_C$. This leaves

\begin{equation}
\langle \chi_a | \frac{1}{r_{C}} | \chi_b \rangle = \int dr_1 \chi_a^*(r_1) \frac{1}{r_C} \chi_b(r_1)  ,
\end{equation}
where $r_C = |r_1 - R_A|$. We first multiply $\chi_a^*$ with $\chi_b$ to get $\Omega_{ab}$. This enables us to use results from section \ref{overlap_section}. This gives

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \int dr_1 \frac{\Omega_{ab}(r)}{r_C} . \label{tata}
\end{equation}
We noted above that $\Omega_{ab}$ could be split into its $x$, $y$ and $z$ components

\begin{equation}
\Omega_{ab}(r) = \Omega_{im}(x) \Omega_{jn}(y) \Omega_{ko}(z) ,
\end{equation}
with the results from section \ref{overlap_section} staying the same, namely

\begin{equation}
\Omega_{im}(x) = \sum_{t = 0}^{i+m} E_t^{i,m} \Lambda_t(x_P) .
\end{equation}
Combining the $x$, $y$ and $z$ directions we can write this as

\begin{equation}
\Omega_{ab}(r) = \sum_{tuv} E_{tuv}^{ab} \Lambda_{tuv}(r_p) ,
\end{equation}
where $\Lambda_{tuv}(r_p)$ is defined as

\begin{equation}
\Lambda_{tuv}(r_p) = \frac{\partial^{t+u+v}}{\partial P_x^t \partial P_y^u \partial P_z^u} exp(-\alpha_p r_p^2) ,
\label{afAFAFA}
\end{equation}
as defined in section \ref{overlap_section}. We insert this into Eq. \eqref{tata} and obtain

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \int dr_1 \frac{\sum_{tuv} E_{tuv}^{ab} \Lambda_{tuv}(r_p)}{r_C}  .
\end{equation}
We can pull the sum and $E_{tuv}^{ab}$ outside the integral and get

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \sum_{tuv} E_{tuv}^{ab} \int dr_1 \frac{\Lambda_{tuv}(r_p)}{r_C}  .
\label{tetete}
\end{equation}
The next step is a substitution. We want to avoid having $r_C$ in our integral and use

\begin{equation}
\frac{1}{r_C} = \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} exp \left( -r_C^2 t^2 \right) dt .
\end{equation}
This is inserted into Eq. \eqref{tetete}, alongside the definition of $\Lambda_{tuv}(r_p)$, and gives us

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \frac{1}{\sqrt{\pi}} \sum_{tuv} E_{tuv}^{ab} \frac{\partial^{t+u+v}}{\partial P_x^t \partial P_y^u \partial P_z^u} \int dr_1 exp(-\alpha_p r_p^2)
 \int_{-\infty}^{\infty} exp \left( -r_C^2 t^2 \right) dt .
 \label{insert_boys2}
\end{equation}
This is a rather large equation now, and we will focus on the integral part of it first, that is

\begin{equation}
V_p = \int dr_1 exp(-\alpha_p r_p^2)
 \int_{-\infty}^{\infty} exp \left( -r_C^2 t^2 \right) dt . 
\end{equation}
We first multiply together the exponentials, through the Gaussian product rule.

\begin{equation}
exp(-\alpha_p r_p^2) exp \left( -r_C^2 t^2 \right) 
= exp \left( - \frac{\alpha_p t^2}{\alpha_p + t^2} R_{cP}^2 \right)
exp \left(
- (\alpha_p + t^2) r_s^2
\right) .
\end{equation}
We insert this into the integral over $r$ and $t$ and obtain

\begin{equation}
V_p = 
\int dr \int_{-\infty}^{\infty}  dt \times
exp \left(
- (\alpha_p + t^2) r_s^2
\right)
 exp \left( - \frac{\alpha_p t^2}{\alpha_p + t^2} R_{cP}^2 \right) .
\end{equation}
The integral over $r$ can now be solved analytically.

\begin{equation}
\int 
exp \left(
- (\alpha_p + t^2) r_s^2
\right) dr
= \left(
\frac{\pi}{\alpha_p + t^2}
\right)^{3/2} .
\end{equation}
This is inserted in $V_p$ and we get

\begin{equation}
V_p = \int_{-\infty}^{\infty} exp \left( - \frac{\alpha_p t^2}{\alpha_p + t^2} R_{cP}^2 \right) \left(
\frac{\pi}{\alpha_p + t^2}
\right)^{3/2} dt .
\end{equation}
We now introduce u as

\begin{equation}
u^2 = \frac{t^2}{\alpha_p + t^2} .
\end{equation}
Some rewriting results in

\begin{equation}
dt = \frac{1}{\alpha_p} \left( 
\frac{t^2}{u^2}
\right)^{3/2} du  .
\end{equation}
We insert this into $V_p$ and we obtain finally

\begin{equation}
V_p = \frac{2 \pi^{3/2}}{\alpha_p}
\int_0^1 exp(- \alpha_p R_{cP}^2 u^2) du . \label{insert_boys}
\end{equation}
Solving this integral is done using the Boys function discussed below. 

\subsubsection{The Boys Function}
For the Boys function we have used additional references \cite{boys_referanse_1} and \cite{boys_referanse_2}. The Boys function is defined as 

\begin{equation}
F_n(x) = \int_0^1 exp(-x t^2) t^{2n} dt .
\end{equation}
This can be solved analytically if $x = 0$ and gives 

\begin{equation}
F_n(0) = \int_0^1 t^{2n} dt = \frac{1}{2n+1} .
\end{equation}
If $x$ is small, we can Taylor expand around 0 and get

\begin{equation}
F_n(x) = \sum_k^{\infty} \frac{(-x)^k}{k! (2n + 2k + 1)} .
\label{boys_int_1}
\end{equation}
We cannot sum to infinity, so we must define some value $M$ to make the sum finite, where $M$ should be large. \\

If $x$ is large, the exponential function will flatten out for
increasing $t$, and will be approximately 0 for $t > 1$. This means we can make this 
approximation

\begin{equation}
F_n(x) \approx \int_0^{\infty} exp(-x t^2) t^{2n}  .
\end{equation}
This can be solved analytically and gives

\begin{equation}
F_n(x) = \frac{(2n + 1)!!}{2^{n+1}} 
\sqrt{\frac{\pi}{x^{2n+1}}} . \label{boys_int_2}
\end{equation}
Once we have calculated the Boys function for one $n$, we can use recurrence relations to find the others. These relations are defined as

\begin{equation}
F_{n-1}(x) = \frac{2x F_n(x) + exp(-x)}{2n - 1} .
\label{boys_int_3}
\end{equation}
There is also an upward recurrence relation, 

\begin{equation}
F_{n+1}(x) = \frac{(2n + 1) F_n(x) - exp(-x)}{2x} ,
\end{equation}
but this is somewhat numerically unstable.



\subsubsection{Defining $R_{tuv}$}
The Boys function should be inserted into Eq. \eqref{insert_boys}. In the equation we do not have any $t^{2n}$ term, this is equivalent to having $t^0$, meaning $n = 0$.

\begin{equation}
V_p = \frac{2 \pi}{\alpha_p}
F_0(\alpha_p R_{cP}^2) .
\end{equation}
This should further be inserted into Eq. \eqref{insert_boys2}. We also place the constant $\frac{2 \pi}{\alpha_p}$ outside the derivatives. We get

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \frac{1}{\sqrt{\pi}} \sum_{tuv} E_{tuv}^{ab}  \frac{2 \pi}{\alpha_p} \frac{\partial^{t+u+v}}{\partial P_x^t \partial P_y^u \partial P_z^u}
F_0(\alpha_p R_{cP}^2) . \label{gagagaga}
\end{equation}
We here define $R_{tuv}(\alpha_p, R_{cP})$. 

\begin{equation}
R_{tuv}(\alpha_p, R_{cP}) = 
\frac{\partial^{t+u+v}}{\partial P_x^t \partial P_y^u \partial P_z^u}
F_0(\alpha_p R_{cP}^2) .
\end{equation}
The quantity $R_{tuv}$ is known as the Hermite Coulomb integrals. How to evaluate these will be shown in a later section. We now insert this into Eq.~\eqref{gagagaga} and get

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = \frac{1}{\sqrt{\pi}} \sum_{tuv} E_{tuv}^{ab}  \frac{2 \pi^{3/2}}{\alpha_p} R_{tuv}(\alpha_p, R_{cP}) .
\end{equation}
We combine terms and rewrite the equation as

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = 
\frac{2 \pi}{\alpha_p} 
\sum_{tuv} 
E_{tuv}^{ab} 
R_{tuv}(\alpha_p, R_{cP}) .
\label{final_nuclei_electron_thang}
\end{equation}
This is a programmable expression once the equations for $R_{tuv}$ have been defined.

\subsection{Electron-Electron interaction}
The final integral to solve is $\langle a b | c d \rangle$. 

\begin{equation}
\langle a b | c d \rangle = 
\int dr_1 \int dr_2 \phi_a^*(r_1) \phi_b^*(r_2) \frac{1}{r_{12}} \phi_c(r_1) \phi_d(r_2) .
\end{equation}
This a similar situation to that of the nucleus-electron interaction, except that we now have two particle and thereby (for a three-dimensional case) in principle a six-dimensional integral. 
We can combine $\phi_a^*(r_1)$ with $\phi_c(r_1)$ and $\phi_b^*(r_2)$ with $\phi_d(r_2)$ and have 

\begin{equation}
\langle a b | c d \rangle = \int dr_1 \int dr_2 \frac{\Omega_{ac}(r_1) \Omega_{bd}(r_2)}{r_{12}} .
\end{equation}
Here we can insert $\Omega$ as we did in section \ref{nuclei-electron-section}. 

\begin{equation}
\langle a b | c d \rangle = 
\sum_{tuv} E_{tuv}^{ac} \sum_{\tau \mu \theta} E_{\tau \mu \theta}^{bd}
\int dr_1 \int dr_2 \frac{\Lambda_{tuv}(r_{1P}) \Lambda_{\tau \mu \theta}(r_{2Q})}{r_{12}} .
\end{equation}
We insert the definition of $\Lambda$ as we did in Eq. \eqref{afAFAFA}. 

\begin{align}
\langle a b | c d \rangle = &
\sum_{tuv} E_{tuv}^{ac} \sum_{\tau \mu \theta} E_{\tau \mu \theta}^{bd} 
\frac{\partial^{t+u+v}}{\partial P_x^t \partial P_y^u \partial P_z^v}
\frac{\partial^{\tau+\mu+\theta}}{\partial Q_x^\tau \partial Q_y^\mu \partial Q_z^\theta}
\int dr_1 \times \nonumber \\  & 
\int dr_2 
\frac{exp(-\alpha_p r_{1P}^2) exp(-\alpha_q r_{2Q}^2)}{r_{12}} .
\end{align}
The terms inside the integral are similar to what we had for the nucleus-electron interaction, except we now have $r_{12}$. Recalling 

\begin{equation}
r_{12} = \sqrt{(r_1 - r_2)^2} .
\end{equation}
Taking the derivative with respect to $r_2$ will change the sign
of the equation. We will take this derivative $\tau + \mu + \theta$
times. Using the same technique applied to the nucleus-electron
interaction this can be shown to reduce to

\begin{equation}
\langle a b | c d \rangle = 
\frac{2 \pi^{5/2}}{\alpha_p \alpha_q \sqrt{\alpha_p + \alpha_q}}
\sum_{tuv} E_{tuv}^{ac} \sum_{\tau \mu \theta} E_{\tau \mu \theta}^{bd} 
\left( -1 \right)^{\tau + \mu + \theta}
R_{t + \tau, u + \mu, v + \theta}(\alpha, R_{PQ}) ,
\label{electron_electron_int_1_1}
\end{equation}
where $\alpha$ without any index is equal to

\begin{equation}
\alpha = \frac{\alpha_p \alpha_q}{\alpha_p + \alpha_q}
. \label{electron_electron_int_1_2}
\end{equation}

\subsection{Calculating $R_{tuv}$}
The quantity $R_{tuv}$ is in use in both the nucleus-electron interaction and the electron-electron interaction. Here we will find programmable equations for $R_{tuv}(a,A)$ 

\begin{equation}
R_{tuv}(a,A) = \frac{\partial^{t+u+v}}{\partial A_x^t \partial A_y^u \partial A_z^v} F_0(a \times A^2) .
\end{equation}
To obtain practical equations  we introduce the auxiliary hermite integrals 

\begin{equation}
R_{tuv}^n (a, A) = (-2a)^n
 \frac{\partial^{t+u+v}}{\partial A_x^t \partial A_y^u \partial A_z^v} F_n(a \times A^2) .
\end{equation}
We first get a starting value when $t = u = v = 0$, 

\begin{equation}
R_{000}^n (a,A) = (-2a)^n F_n(a \times A^2) .
\label{nucelec_0_int}
\end{equation}
From this we use the following recurrence relations which has been proven in \cite{helg2}, namely 

\begin{equation}
R^n_{t+1,u,v} = 
t R^{n+1}_{t-1, u, v} 
+ A_x R^{n+1}_{tuv} ,
\label{nucelec_1_int}
\end{equation}

\begin{equation}
R^n_{t,u+1,v} = u R^{n+1}_{t, u-1, v} 
+ A_y R^{n+1}_{tuv} ,
\label{nucelec_2_int}
\end{equation}
and

\begin{equation}
R^n_{t,u,v+1} = 
t R^{n+1}_{t, u, v-1} 
+ A_z R^{n+1}_{tuv} .
\label{nucelec_3_int}
\end{equation}
Since we do not have any $(-2a)^n$ term in any of our equations for the nucleus-electron or the 
electron-electron interactions, we make a small tweak to these equations when we use the auxiliary hermite integrals, 

\begin{equation}
\langle \chi_a | \frac{1}{r_C} | \chi_b \rangle = 
\frac{2 \pi}{\alpha_p} 
\sum_{tuv} 
E_{tuv}^{ab} 
R^0_{tuv}(\alpha_p, R_{cP}) ,
\end{equation}
and

\begin{equation}
\langle a b | c d \rangle = 
\frac{2 \pi^{5/2}}{\alpha_p \alpha_q \sqrt{\alpha_p + \alpha_q}}
\sum_{tuv} E_{tuv}^{ac} \sum_{\tau \mu \theta} E_{\tau \mu \theta}^{bd} 
\left( -1 \right)^{\tau + \mu + \theta}
R^0_{t + \tau, u + \mu, v + \theta}(\alpha, R_{PQ}) .
\end{equation}

\section{Choosing Basis Set}
We now have plenty of programmable equations for solving the HF
equations, and we will pull them all together in the implementation
chapter. But before we can use any of them we must choose some basis
functions. We will use basis sets from EMSL. In this section we will
discuss the differences between the different basis sets. \\

There are several basis sets in EMSL. Some of these are 6-31G, 3-21G,
6-311++G**, STO-3G, cc-pVDZ etc. In general, Pople type basis sets are
the ones with numbers inside the name, such as the 6-31G and
3-21G sets. The Pople type basis sets use cartesian GTOs, which are the same
ones that we use. We can also work with STO-3G and other versions of this
basis set. \\

It is possible to use basis sets designed for spherical GTOs with a program that use Cartesian GTOs. In this situation we usually get an energy slightly lower than if our program did actually use spherical GTOs. Also it is less effective in terms of program performance. For these reasons we will not make use of basis sets designed for spherical GTOs. These are basis sets such as cc-pVTZ, aug-cc-pVDZ etc. 

\subsection{STO-nG}
First we look at the simplest, smallest basis set, the STO-nG
family. These are single-zeta basis sets. Up till now we have usually
talked about contracted GTOs and Atomic Orbitals (AOs) as the same thing,
and for our purposes they are the same. \\

However, strictly speaking, the terminology of Atomic Orbitals in this
context are the solutions of the HF equations for one atom. In this
scenario we have no molecular orbitals since we have no molecule. An
atomic orbital would then be a linear combination of contracted
GTOs. Early on in the literature the terms atomic orbital meant basis
function. The term basis function was introduced somewhat later where
appropriate. This distinction is more important if we are constructing
new basis sets. Since we will be citing old articles we will, with
exception of this section, use the term atomic orbital as used in the old context. \\

A single zeta basis set means we have just enough contracted GTOs to
contain the electrons of a neutral charged atom and retain its
spherical symmetry. \\

The STO part of the name signifies that we are trying to mimic an STO,
but we are still using GTOs. An STO-nG basis set will have n
primitives for each contracted GTO.\\

For the hydrogen atom with only one electron, we will only have one
contracted GTO to describe the 1s orbital. For the atoms Li to Ne STO-nG
will have five contracted GTOs to describe the orbitals 1s, 2s, 2p$_x$
2p$_y$ and 2p$_z$. All these orbitals need to be described since we want to retain the spherical symmetry of the atom. 

\subsection{Double Zeta Basis Sets}
STO-nG is a small and poor basis set, regardless of the value of n. It
should never be used, unless we are performing program tests. This
simplifies things further, since we are left with the option of using
the Pople style basis sets. These are the ones named 4-31G, 6-31G and
so on. \\

The smallest Pople style basis sets are known as double zeta. The
smallest of these are 3-21G. This was originally called STO-3-21G, but
the STO part is omitted. 3-21G is a basis set with one contracted GTO
for the core orbitals. This GTO consists of 3 primitives, hence the
number 3 in the first position. \\

The numbers 2 and 1 mean that we have two contracted GTOs to
represent the valence orbitals. These contracted GTOs consist of 2 and
1 primitives, respectively. The name double zeta comes from the fact that the valence
orbitals are now represented by two contracted GTOs. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{Be_6-31G.eps}}
\caption{2p$_x$ Basis Functions for Be with the 6-31G Basis Set}
\label{fig:basis_set_6_31G}
\end{center}
\end{figure}

The set 6-31G is another example of a double zeta Pople basis
set. Here we have six primitives for the contracted GTO describing the
core orbitals, and two contracted GTOs describing the valence
orbitals. These have 3 and 1 primitives, respectively. \\

For Li to Ne the valence orbitals will be 2s, 2p$_x$, 2p$_y$ and
2p$_z$. The core orbital will be 1s. This makes in total nine
contracted GTOs. Figure \ref{fig:basis_set_6_31G} illustrates the two
contracted GTOs in the 6-31G basis set that both illustrate the 2p$_x$
orbital in the Beryllium atom. Our solver can use both of these in the
linear combination to make the molecular orbitals.

\subsection{Tripple Zeta Basis Sets}
Another Pople basis set is the 6-311G basis set. Here we have three
numbers after the dash, meaning the valence electrons will be
represented by three contracted GTOs. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{Be_6-311G.eps}}
\caption{2p$_x$ Basis Functions for Be with the 6-311G Basis Set}
\label{fig:basis_set_6_311G}
\end{center}
\end{figure}

The core orbitals will have one contracted, that consists of six
primitives. The valence electrons will have three contracted GTOs,
where each of them consist of 3, 1 and 1 primitives. Figure
\ref{fig:basis_set_6_311G} illustrates the three contracted GTOs in
the 6-311G basis set that share quantum numbers with the 2p$_x$
orbital. Our solver can use all these three GTOs in the linear
combination to make the molecular orbitals.

\subsection{Polarized Basis Set}
Polarized basis sets are usually marked by a star. The 6-311G basis
set, with added polarization functions, is called 6-311G*. The
difference here is that we add orbitals of higher angular momentum. If
an atom has electrons where the P orbital contains the valence
electrons, the P orbital has $l = 1$. Adding polarization will mean adding
D orbitals, with $l = 2$.

\subsection{Diffuse Basis Set}
We can also choose a diffuse basis set. The largest basis sets are
always diffuse, and the smaller diffuse basis sets are usually marked
by a +. For example, adding diffuse functions to a 6-311G basis set
will make 6-311+G. \\

Adding diffuse functions means that we add another set of contracted GTOs. If we have a basis set of S and P orbitals, we add one more contracted GTO for the S and P orbitals. If our basis set is also polarized there are also D orbitals present. However a diffuse basis set does not add another D orbital. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{Be_6-311++G.eps}}
\caption{2p$_x$ Basis Functions for Be with the 6-311++G(2d,2p) Basis Set}
\label{fig:basis_set_6_311++G}
\end{center}
\end{figure}

Figure \ref{fig:basis_set_6_311++G} illustrates the additional GTOs
available for the linear combination. We can combine also diffuse and
polarized basis functions. If we add diffuse and polarization
functions to the 6-311G basis set we get the 6-311+G* basis set. \\

\subsection{Reasons for Larger Basis Set}
The main reason for a larger basis set is the ability for our HF
solver to respond to a changing molecular environment. The electrons
are likely to behave differently in an atom  compared to in a
larger molecule. We do construct molecular orbitals as a linear
combination of contracted GTOs. Since we minimize the energy with
respect to this linear combination, having more contracted GTOs should
produce a better result. \\

By adding diffuse functions, we make it possible that the electrons
are pulled further away from their positions in a single atom. Double and
triple zeta also helps with this. This is illustrated in figures
\ref{fig:basis_set_6_31G}, \ref{fig:basis_set_6_311G} and
\ref{fig:basis_set_6_311++G}. \\

By introducing polarization functions, we make it possible for
electrons to redistribute themselves in our molecule.  \\

All this helps our HF solver in finding the best suited wavefunction, for any system.

\section{HF Limitations \label{imporatant_ccsd_convergence_ting}}
However if we add too many orbitals, some of them will get coefficients
of zero in the linear combination. Once this happens we have reached a
converged basis set. Increasing the size of the basis set at this
point will not improve our results further. \\

When we have a converged basis set we still do not have the exact
wavefunction. This is because we used a single Slater determinant. In
a single Slater determinant the electrons feel a mean interaction with
the other electrons. This is an approximation. \\

We can imagine two electrons in close proximity. Two electrons side by
side will feel a strong and repulsive interaction, possibly even temporarily pushing
both electrons into orbitals previously unoccupied. HF theory does not
account for this at all. As such the energy cannot converge to the
exact ground state energy. It can only converge to the so called
Hartree Fock Limit. \\

Polarized basis sets include orbitals of higher angular momentum than
the electrons occupy in a single atom. With a single Slater
determinant, polarized basis sets are not as effective as they could
be. If we go back to the example of the two electrons in close
proximity, we stated that it was possible that they were temporarily
excited into orbitals previously unoccupied. To describe this we need
a description of these orbitals that are unoccupied. This is where the
polarized basis sets are important. Coupled cluster theory, unlike HF theory, accounts for this
situation. As such it is likely to see more basis functions required
for the CCSD energy to be converged relative to HF, since Coupler Cluster can make better use of the polarized basis set.  

\section{DIIS \label{diis_section_po_g}}
Direct Inversion of the Iterative Subspace is a method to help the
convergence of our HF solution. This method can potentially reduce the
number of iterations required for self consistency dramatically. This
content is explained in detail in Refs.~\cite{diis_citation1,diis_citation2,diis_citation3}.
\\

DIIS is a method to reduce the number of iterations required to solve
any iterative problem. In HF theory this means updating the Fock matrix
elements in between iterations. In our implementation we define an
error given by

\begin{equation}
\Delta p = FPS - SPF .
\end{equation}
The definition of the error is somewhat optional. We used the density matrix P defined as

\begin{equation}
P_{ij} = \sum_{m}^N C_{im} C_{jm} .
\end{equation}
However the error can also be defined differently. Another option is simply using the current and the prior Fock matrix,

\begin{equation}
\Delta p_k = F_k - F_{k-1} .
\end{equation}
In DIIS we want to make a linear combination of Fock matrices in prior iterations. We want

\begin{equation}
F = \sum_k^M c_k F_k .
\end{equation}
Here we want to have $\sum_k^M c_k = 1$. We want to minimize the norm

\begin{equation}
\langle \Delta p | \Delta p \rangle = \sum_i^M \sum_j^M c_i^* c_j \langle \Delta p_i | \Delta p_j \rangle .
\end{equation}
We define the overlap of the errors as a matrix B

\begin{equation}
B_{ij} = \langle \Delta p_i | \Delta p_j \rangle .
\end{equation}
This matrix will be symmetric, since $\Delta p$ is real. We can then use the method of Lagrangian multipliers, since we want to minimize the energy with the constraint that the sum of coefficients is 1, that is

\begin{equation}
\mathcal{L} = c^{\dag} B c - \lambda \left( 1 - \sum_i^M c_i \right) .
\end{equation}
We find the minimum of this Lagrangian by minimizing the coefficients $c$,

\begin{equation}
\frac{\partial \mathcal{L}}{\partial c_k} = 0 .
\end{equation} 
This gives

\begin{equation}
\sum_i^M c_i B_{ik} + \sum_j^M c_j B_{kj} - \lambda = 0 .
\end{equation}
We can use the symmetry in $B$ to combine the two sums and obtain

\begin{equation}
2 \sum_i^M c_i B_{ki} - \lambda = 0 .
\end{equation}
Since $\lambda$ is a constant, we can place the factor two into $\lambda$ and get

\begin{equation}
\sum_i^M c_i B_{ki} - \lambda = 0 .
\end{equation}
This results in  Eq.~(6) in \cite{diis_citation2}, restated here

\[ \left( \begin{array}{ccccc}
B_{11} & B_{12} & \dots & B_{1M} & -1 \\
B_{21} & B_{22} & \dots & B_{2M} & -1 \\
\dots & \dots & \dots & \dots & \dots \\
B_{M1} & B_{M2} & \dots & B_{MM} -1 \\
-1 & -1 & \dots & -1 & 0
\end{array} \right) \left( \begin{array}{c}
c_1 \\
c_2 \\
\dots \\
c_M \\
\lambda
\end{array} \right) =\left( \begin{array}{c}
0 \\
0 \\
\dots \\
0 \\
-1 
\end{array} \right)\] 

\section{Four Index Integral, from AO to MO}
The four index integral $\langle i j | k l \rangle$ is defined in
terms of AOs. However, we want a procedure to recalculate these in
terms of Molecular Orbitals (MOs). The MOs is defined as

\begin{equation}
\psi_a = \sum_i C_i^a \phi_i .
\end{equation}
Our integral using MOs is describes as

\begin{equation}
\langle a b | c d \rangle = 
\int dr_1 \int dr_2 \psi_a^*(r_1) \psi_b^*(r_2) | \frac{1}{r_{12}} | \psi_c(r_1) \psi_d(r_2) .
\end{equation}
We insert the definition of an MO into this equation. We assume the coefficients, $C_i^a$, are real

\begin{equation}
\langle a b | c d \rangle = 
\sum_{ijkl} \int dr_1 \int dr_2 C_i^a \phi_i^*(r_1) C_j^b \phi_j^*(r_2) | \frac{1}{r_{12}} | C_k^c \phi_k(r_1) C_l^d \phi_l(r_2) .
\end{equation}
We then pull all the coefficients outside the integral.

\begin{equation}
\langle a b | c d \rangle = 
\sum_{ijkl} C_i^a C_j^b C_k^c C_l^d \int dr_1 \int dr_2 \phi_i^*(r_1) \phi_j^*(r_2) | \frac{1}{r_{12}} | \phi_k(r_1)  \phi_l(r_2) .
\end{equation}
This is our two electron or four index integral based on AOs. We insert this and get

\begin{equation}
\langle ab | cd \rangle = \sum_{ijkl} C_i^a C_j^b C_k^c C_l^d \langle ij||kl \rangle .
\end{equation} 
We will make use of this equation when starting with the coupled
cluster method. In coupled cluster theory the four index integral is
the only term where molecular orbitals are involved. For this reason
the four index molecular orbital integrals are usually just called MOs
for short. Also, the four index atomic orbital integrals are named AOs
for short in the context of coupled cluster. 



\chapter{Coupled Cluster Singles and Doubles}
Coupled Cluster is an important ab initio technique in computational chemistry. It is considered the most reliable and also computational affordable method for solving the electronic Schr\"{o}dinger equation. It was introduced in Quantum Chemistry by Paldus and Cizek in the late 1960s. Then it was derived using Feynman-like diagrams, however about ten years later Hurley re-derived the equations in terms more familiar to most physicists. In this chapter we will look at the derivation of coupled cluster singles and doubles (CCSD), using the results from Hartree Fock. \\

This chapter is almost solely based on a book by T. Daniel Crawford and Henry S. Schaeffer III, Ref.\cite{ccsdbook11}. 

\section{Creation and Annihilation operators}
In Coupled Cluster, CC, we will solve the Schr\"{o}dinger equation.

\begin{equation}
\textbf{H} | \Psi \rangle = E | \Psi \rangle \label{SE} .
\end{equation}
From Hartree Fock calculations we have created a $|\Psi\rangle_{HF}$ which contains MOs in a Slater determinant. Dirac notation provides a simple representation of this. In Dirac notation only the diagonal terms in the slater determinant are listed. If $|\Psi_0 \rangle$ has four electrons Dirac notation would be

\begin{equation}
|\Psi_0 \rangle =  |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_l(r_4) \rangle . \label{dirac_not} 
\end{equation}
Eq. \eqref{dirac_not} will be used to introduce a few new operators needed. The creation operator $\textbf{a}^{\dag}_m$ creates a new electron in orbital m.

\begin{equation}
\textbf{a}^{\dag}_m |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_l(r_4) \rangle = |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_l(r_4), \psi_m(r_5) \rangle .
\end{equation}
The annihilation operator, $\textbf{a}_n$, destroys an electron in orbital n.

\begin{equation}
\textbf{a}_n |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_l(r_4), \psi_n(r_5) \rangle = |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_l(r_4) \rangle  .
\end{equation}
These two operators working together can destroy one electron in orbital n, and create another in orbital m. The result is that one electron now occupies a different orbital, as such

\begin{equation}
\textbf{a}^{\dag}_m \textbf{a}_n |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_n(r_4) \rangle = |\psi_i(r_1), \psi_j(r_2), \psi_k(r_3), \psi_m(r_4) \rangle .
\end{equation}
These operators have a few interesting features. Such as the annihilation operator acting on the vacuum state produces 0.

\begin{equation}
a_n | \rangle = 0  .
\end{equation}
Interchanging two rows in the Slater determinant introduce a change in the sign. Hence we have

\begin{equation}
\textbf{a}^{\dag}_m \textbf{a}^{\dag}_n | \rangle = |\psi_m, \psi_n \rangle = - |\psi_n, \psi_m \rangle = -\textbf{a}^{\dag}_n \textbf{a}^{\dag}_m | \rangle  .
\end{equation}

\begin{equation}
\Rightarrow \textbf{a}^{\dag}_m \textbf{a}^{\dag}_n + \textbf{a}^{\dag}_n \textbf{a}^{\dag}_m = 0 .
\end{equation}
The same applies to the annihilation operator.

\begin{equation}
\textbf{a}_m \textbf{a}_n + \textbf{a}_n \textbf{a}_m = 0 .
\end{equation}
These are known as anti commutation relations. It can be shown that the anti commutation relation when mixing $\textbf{a}$ and $\textbf{a}^{\dag}$ is

\begin{equation}
\textbf{a}^{\dag}_m \textbf{a}_n + \textbf{a}_n \textbf{a}^{\dag}_m = \delta_{mn} . \label{ccsd_anni_creato_operator_combo}
\end{equation}

\section{CCSD Wavefunction}
The first step in coupled cluster is rewriting the wavefunction,

\begin{equation}
|\Psi_{CC} \rangle \equiv e^{\textbf{T}} | \Psi_{HF} \rangle .
\end{equation} 
$\textbf{T}$ is known as the cluster operator. This includes all possible excitations. $|\Psi_{CC} \rangle$ is thus a linear combination of Slater determinants of all possible excitations and is an exact solution to Eq. \eqref{SE}. $\textbf{T}$ can be defined in terms of a one-orbital excitation operator, a two-orbital excitation operator and so on.

\begin{equation}
\textbf{T} \equiv \textbf{T}_1 + \textbf{T}_2 + \textbf{T}_3 + \textbf{T}_4 \dots .
\end{equation}
When doing CCSD only single excitations, $\textbf{T}_1$, and double excitations, $\textbf{T}_2$, are included. 

\begin{equation}
\textbf{T} = \textbf{T}_1 + \textbf{T}_2 .
\end{equation}
Other CC methods include more terms. If $\textbf{T}_3$ is included the method is called CCSDT. CCSDTQ includes also $\textbf{T}_4$. \\

$\textbf{T}_1$ is defined using one creation and one annihilation operator, because we will have one single electron excited. Also defining $\textbf{T}_1$ is an amplitude $t_i^a$ and a summation over all possible excitations.  

\begin{equation}
\textbf{T}_1 \equiv \sum_{a,i} t_i^a \textbf{a}^{\dag}_a \textbf{a}_i . \label{t1defi}
\end{equation}
$\textbf{T}_2$ is defined by two creation and two annihilation operators and an amplitude $t_{ij}^{ab}$.

\begin{equation}
\textbf{T}_2 \equiv \frac{1}{4} \sum_{a,b,i,j} t_{ij}^{ab} \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_i \textbf{a}_j . \label{t2defi}
\end{equation}

\section{Derivation of Equations}
This section contains the formal derivation of coupled cluster theory, starting from Eq. \eqref{SE} and using the CCSD wavefunction. 

\begin{equation}
\textbf{H} e^{\textbf{T}} |\Psi \rangle_{HF} = E e^{\textbf{T}} |\Psi \rangle_{HF} .
\end{equation}
For this derivation $|\Psi \rangle_{HF}$ will be shortened to $|\Psi_0 \rangle$. 

\begin{equation}
E = \langle \Psi_0 |e^{-\textbf{T}} \textbf{H} e^{\textbf{T}} |\Psi_0 \rangle .
\end{equation}
We also assume an orthonormal basis, meaning

\begin{equation}
\langle \Psi_m |e^{-\textbf{T}} \textbf{H} e^{\textbf{T}} |\Psi_0 \rangle = 0 .
\end{equation}

\subsection{Baker-Campbell-Hausdorff formula}
The Baker-Campbell-Hausdorff formula is used to expand $e^{-\textbf{T}} \textbf{H} e^{\textbf{T}}$.

\begin{equation}
\begin{split}
e^{-\textbf{T}} \textbf{H} e^{\textbf{T}} = 
\textbf{H} 
+ \left[ \textbf{H}, \textbf{T} \right] 
+ \frac{1}{2} \left[ [\textbf{H}, \textbf{T}], \textbf{T} \right]  + \frac{1}{6} \left[ [ [\textbf{H}, \textbf{T}], \textbf{T}], \textbf{T} \right] \\
+ \frac{1}{24} \left[ [ [ [\textbf{H}, \textbf{T}], \textbf{T}],\textbf{T}], \textbf{T} \right] \dots .
\end{split} \label{baker}
\end{equation}
$\textbf{T}$ is expressed in terms of $\textbf{a}^{\dag}$ and $\textbf{a}$. $\textbf{H}$ contains a maximum of two orbital interactions. It can be showed that $\textbf{H}$ can also be expressed in terms of these operators. 

\begin{equation}
\textbf{H} = \sum_{a,i} h_{a,i}
\textbf{a}^{\dag}_a 
\textbf{a}_i 
+ \frac{1}{4} \sum_{a,b,i,j} \langle ab||ij \rangle
\textbf{a}^{\dag}_a  
\textbf{a}^{\dag}_b
\textbf{a}_i 
\textbf{a}_j . \label{annih}
\end{equation}
Where $h_{a,i} = \langle \psi_a | \textbf{h} | \psi_i \rangle$. With $\textbf{h}$ the one-particle part of $\textbf{H}$. This is the same Hamiltonian as before expressed slightly different and will be discussed further later on. Eq. \eqref{baker} can be simplified using commutators.

\begin{equation}
[\textbf{a}^{\dag}_a  \textbf{a}_i 
, \textbf{a}^{\dag}_b \textbf{a}_j] =
\textbf{a}^{\dag}_a  \textbf{a}_i \textbf{a}^{\dag}_b \textbf{a}_j
- \textbf{a}^{\dag}_b \textbf{a}_j \textbf{a}^{\dag}_a  \textbf{a}_i .
\end{equation} 
Using the anti commutator relations this commutator itself can be simplified.

\begin{equation}
[\textbf{a}^{\dag}_a  \textbf{a}_i 
, \textbf{a}^{\dag}_b \textbf{a}_j] =
\textbf{a}^{\dag}_a  \delta_{ib} \textbf{a}_j
- \textbf{a}^{\dag}_b \delta_{ja} \textbf{a}_i .
\end{equation}
This simplification reduces the number of indices from 4 to 3, replacing two operators with a Kronecker delta. Each nested commutator in Eq. \eqref{baker} will reduce the number of indexes by 1. The maximum number of creation/annihilation operators in $\textbf{H}$ was 4. This means Eq. \eqref{baker} will naturally truncate after exactly 4 terms, and we can remove the dots.

\begin{equation}
\begin{split}
e^{-\textbf{T}} \textbf{H} e^{\textbf{T}} = 
\textbf{H} 
+ \left[ \textbf{H}, \textbf{T} \right] 
+ \frac{1}{2} \left[ [\textbf{H}, \textbf{T}], \textbf{T} \right] 
+ \frac{1}{6} \left[ [ [ \textbf{H},[\textbf{T}], \textbf{T}], \textbf{T} \right] \\
+ \frac{1}{24} \left[ [ [ [\textbf{H}, \textbf{T}], \textbf{T}], \textbf{T}], \textbf{T}  \right] .
\end{split}  \label{variationalccsd}
\end{equation}

\subsection{Normal Order and Contractions}
When deriving CCSD equations it is common to introduce a concept called normal ordering of second quantized operators. This means that all creation operators are placed to the left and the annihilation operators to the right. The mathematics of swapping the order of creation and annihilation operators are well defined. To show this we define an example operator $\textbf{O}$ and use the anti commutator relations.

\begin{align}
\textbf{O} & = 
\textbf{a}_i 
\textbf{a}^{\dag}_a 
\textbf{a}_j 
\textbf{a}^{\dag}_b \label{normalorder} \\ &
= \delta_{ia} 
\textbf{a}_j 
\textbf{a}^{\dag}_b - 
\textbf{a}^{\dag}_a 
\textbf{a}_i  
\textbf{a}_j 
\textbf{a}^{\dag}_b \nonumber \\ &
= \delta_{ia} \delta_{jb} -
\delta_{ia} 
\textbf{a}^{\dag}_b
\textbf{a}_j -
\delta_{jb}
\textbf{a}^{\dag}_a
\textbf{a}_i +
\textbf{a}^{\dag}_a
\textbf{a}_i
\textbf{a}_j
\textbf{a}^{\dag}_b \nonumber \\ &
= \delta_{ia} \delta_{jb} - 
\delta_{ia} 
\textbf{a}^{\dag}_b
\textbf{a}_j +
\delta_{ib} 
\textbf{a}^{\dag}_a
\textbf{a}_j -
\delta_{jb} 
\textbf{a}^{\dag}_a
\textbf{a}_i -
\textbf{a}^{\dag}_a
\textbf{a}^{\dag}_b
\textbf{a}_i
\textbf{a}_j .
\end{align}
The final expression of $\textbf{O}$ is in normal order since all creation operators are to the left and all annihilation operators to the right. Notice that we now have five terms, four of which have a reduced number of operators compared to our first definition of $\textbf{O}$. Any combination of annihilation and creation operators can be expressed as a linear combination of normal ordered combinations of these operators. \\

The four terms with reduced number of operators arise from contractions between operators. A contraction between two operators $\textbf{A}$ and $\textbf{B}$, that each contain an arbitrary number of creation or/and annihilation operators, can be defined as such:

\begin{equation}
\contraction{}{\textbf{A}}{}{\textbf{B}}
\textbf{A}\textbf{B}
 \equiv \textbf{A}\textbf{B} - \left\{ \textbf{A}\textbf{B} \right\}_{\nu} .
\end{equation}
Here $\left\{ \textbf{A}\textbf{B} \right\}_{\nu}$ is the normal ordered form of $\textbf{A}\textbf{B}$. $\contraction{}{\textbf{A}}{}{\textbf{B}}
\textbf{A}\textbf{B}$ is called the contraction between $\textbf{A}$ and $\textbf{B}$. As an example $\textbf{A} = \textbf{a}_i$ and $\textbf{B} = \textbf{a}_j$ will give a contraction of

\begin{equation}
\contraction{}{\textbf{a}_i}{}{\textbf{a}_j}
\textbf{a}_i\textbf{a}_j
= \textbf{a}_i \textbf{a}_j - \left\{ \textbf{a}_i \textbf{a}_j \right\}_{\nu} = \textbf{a}_i \textbf{a}_j - \textbf{a}_i \textbf{a}_j = 0 . \label{wickexample1}
\end{equation}
From the example above we see the contraction of all annihilation operators will be zero since there will be no swapping of operators when creating the normal ordered form. The same will apply to contractions between operators formed from only creation operators. Also the contraction between already normal ordered operators will be zero. \\

However the contraction between different operators not in normal order will not be zero. The simplest example is one annihilation operator in front of one creation operator.

\begin{equation}
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\textbf{a}_i\textbf{a}^{\dag}_a
= \textbf{a}_i \textbf{a}^{\dag}_a - \left\{ \textbf{a}_i \textbf{a}^{\dag}_a \right\}_{\nu} = \textbf{a}_i \textbf{a}^{\dag}_a + \textbf{a}^{\dag}_a \textbf{a}_i = \delta_{ia} .
\end{equation}
Here we used Eq. \eqref{ccsd_anni_creato_operator_combo}.

\subsection{Wick's Theorem}
Wick's Theorem provides a schematic way of defining any string of annihilation and creation operators in terms of these contractions. A string of annihilation and creation operators can be defined as $ABC \dots XYZ$ where $A, B, C, X, Y, Z$ $\dots$ represent either a creation or an annihilation operator.  Wick's Theorem is defined as such

\begin{align}
\textbf{A} \textbf{B} \textbf{C} \dots \textbf{X} \textbf{Y} \textbf{Z} = & \left\{\textbf{A} \textbf{B} \textbf{C} \dots \textbf{X} \textbf{Y} \textbf{Z} \right\}_{\nu} \label{wicks} \\
& + \sum_{singles} \{
\contraction{}{\textbf{A}}{}{\textbf{B}}
\textbf{A}\textbf{B}
\textbf{C} \dots \textbf{X} \textbf{Y} \textbf{Z}
\}_{\nu} \nonumber \\
& + \sum_{doubles} \{
\contraction{}{\textbf{A}}{\textbf{B}}{\textbf{C}}
\contraction[2ex]{\textbf{A}}{\textbf{B}}{\textbf{C} \dots \textbf{X} \textbf{Y}}{\textbf{Z}}
\textbf{A} \textbf{B} \textbf{C} \dots \textbf{X} \textbf{Y} \textbf{Z} 
\}_{\nu} \nonumber \\
& \dots \nonumber
\end{align}
The right side of Eq. \eqref{wicks} should represent every possible contraction of $\textbf{A} \textbf{B} \textbf{C} \dots \textbf{X} \textbf{Y} \textbf{Z}$. To specify the notation we apply Wick's theorem as an example to the operator $\textbf{O}$ defined in Eq. \eqref{normalorder} and repeated here.

\begin{equation}
\textbf{O} = \textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b \nonumber
\end{equation}
Applying Wick's Theorem provides

\begin{align}
\textbf{O} = & 
\{
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \\ &
+ \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\textbf{a}_i \textbf{a}^{\dag}_a
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \\ &
+ \{
\textbf{a}_i
\contraction{}{\textbf{a}^{\dag}_a}{}{\textbf{a}_j}
\textbf{a}^{\dag}_a \textbf{a}_j
\textbf{a}^{\dag}_b
\}_{\nu} \label{wickex1} \\ &
+ \{
\textbf{a}_i \textbf{a}^{\dag}_a
\contraction{}{\textbf{a}^j}{}{\textbf{a}^{\dag}_b}
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \\ &
+ \{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a}{\textbf{a}_j}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j
\textbf{a}^{\dag}_b
\}_{\nu} \label{wickex2} \\ &
+ \{
\textbf{a}_i
\contraction{}{\textbf{a}^{\dag}_a}{\textbf{a}_j}{\textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \label{wickex3} \\ &
+ \{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a \textbf{a}_j}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \label{wickex4} \\ &
+ \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\textbf{a}_i \textbf{a}^{\dag}_a 
\contraction{}{\textbf{a}_j}{}{\textbf{a}^{\dag}_b}
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} . \label{wicksdoubles}
\end{align}
Eq. \eqref{wicksdoubles} is from the doubles summation. The other terms are from the singles summation. Eqs. \eqref{wickex1}, \eqref{wickex2} and \eqref{wickex3} are zero when using the rules such as \eqref{wickexample1}. This leaves the terms

\begin{align}
\textbf{O} = & 
\{
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu}
+ \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\textbf{a}_i \textbf{a}^{\dag}_a
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu}
+ \{
\textbf{a}_i \textbf{a}^{\dag}_a
\contraction{}{\textbf{a}^j}{}{\textbf{a}^{\dag}_b}
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} \nonumber \\ &
+ \{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a \textbf{a}_j}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu}
+ \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\textbf{a}_i \textbf{a}^{\dag}_a 
\contraction{}{\textbf{a}_j}{}{\textbf{a}^{\dag}_b}
\textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu} .
\end{align}
These must be evaluated. One trick needed is when swapping two operators inside a contraction the sign is changed. Eq. \eqref{wickex4} is used as an example.

\begin{equation}
\{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a \textbf{a}_j}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu}
= - \{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b 
\textbf{a}_j 
\}_{\nu}
= \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_b}
\textbf{a}_i  \textbf{a}^{\dag}_b 
\textbf{a}^{\dag}_a \textbf{a}_j 
\}_{\nu} . \label{wikssign}
\end{equation}
Remembering $\{\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_b}
\textbf{a}_i  \textbf{a}^{\dag}_b \}_{\nu} = \delta_{ib}$ then the terms in $\textbf{O}$ reduces in order to

\begin{equation}
\textbf{O} =  \{ 
\textbf{a}_i \textbf{a}^{\dag}_a
\textbf{a}_j \textbf{a}^{\dag}_b
\} +
\delta_{ia} \{
\textbf{a}_j \textbf{a}^{\dag}_b
\} +
\delta_{jb} \{
\textbf{a}_i \textbf{a}^{\dag}_a
\} +
\delta_{ib} \{
\textbf{a}^{\dag}_a \textbf{a}_j
\} +
\delta_{ia} \delta_{ib} . 
\end{equation}

Using $\{
\textbf{a} \textbf{a}^{\dag}
\} = -\textbf{a}^{\dag} \textbf{a}$ and $\{
\textbf{a}^{\dag} \textbf{a} 
\} = \textbf{a}^{\dag} \textbf{a}$ we get

\begin{equation}
\textbf{O} = 
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_i \textbf{a}_j  
- \delta_{ia} \textbf{a}^{\dag}_b \textbf{a}_j
- \delta_{jb} \textbf{a}^{\dag}_a \textbf{a}_i
+ \delta_{ib} \textbf{a}^{\dag}_a \textbf{a}_j 
+ \delta_{ia} \delta_{ib} ,
\end{equation}
which is identical to Eq. \eqref{normalorder}. The sign rules can sometimes be complicated, when there is more than one contraction present. Swapping two operators can fulfil the positioning for two contractions at once, as seen in example Eq. \eqref{examplebullshit}. This provides a minus sign which must not be neglected.

\begin{equation}
\{
\contraction{}{\textbf{a}_i}{\textbf{a}^{\dag}_a}{\textbf{a}_j}
\contraction[2ex]{\textbf{a}_i}{\textbf{a}^{\dag}_a}{\textbf{a}_j}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\}_{\nu}
= - \{
\contraction{}{\textbf{a}_i}{}{\textbf{a}^{\dag}_a}
\contraction{\textbf{a}_i \textbf{a}^{\dag}_a}{\textbf{a}_j}{}{\textbf{a}^{\dag}_b}
\textbf{a}_i \textbf{a}^{\dag}_a \textbf{a}_j \textbf{a}^{\dag}_b
\} . \label{examplebullshit}
\end{equation}

\subsection{Fermi Vacuum and Particle Holes}

When using creation and annihilation operators it is common they work on the vacuum state, $| \rangle$. Eq. \eqref{dirac_not} would commonly be represented as such

\begin{equation}
|\Psi_0 \rangle = \textbf{a}^{\dag}_i
\textbf{a}^{\dag}_j
\textbf{a}^{\dag}_k
\textbf{a}^{\dag}_l |\rangle .
\end{equation}
An excited state $|\Psi_m\rangle$ would for example be noted as the following

\begin{equation}
|\Psi_m \rangle = \textbf{a}^{\dag}_m
\textbf{a}^{\dag}_i
\textbf{a}^{\dag}_j
\textbf{a}^{\dag}_k |\rangle .
\end{equation}
Where the m orbital is occupied and the l orbital is not occupied. In our CCSD derivation we will not use this kind of notation. The Fermi Vacuum is introduced and is later defined as the Hartree Fock result. However continuing this example the Fermi Vacuum could be defined as $|\Psi_0\rangle$, and the excited state would be

\begin{equation}
|\Psi_m\rangle = \textbf{a}^{\dag}_m \textbf{a}_l |\Psi_0\rangle .
\end{equation}
This creates a "hole state" in orbital l, since an occupied orbital is now unoccupied. It also creates a "particle state" in orbital m, since this is now occupied and was unoccupied in the Fermi Vacuum. \\

This definition will bring new features to Wick's theorem. Indexes $a, b, c \dots$ will denote newly occupied orbitals. Indexes $i, j, k \dots$ will denote newly formed holes. The operator $\textbf{a}^{\dag}_i$ can then be thought of as annihilating a hole. $\textbf{a}_a$ can be thought of annihilating a particle. Likewise $\textbf{a}^{\dag}_a$ and $\textbf{a}_i$ can be thought of as creating a particle or creating a hole. \\

This differs from the concept of $\textbf{a}^{\dag}$ always being a creation operator, since $\textbf{a}^{\dag}_i$ can be thought of as annihilating a hole state. This changes our Wick's Theorem calculations, since we still have the only terms not zero being those with one annihilation operator followed by one creation operator. There are only two possibilities of this happening.

\begin{equation}
\contraction{}{\textbf{a}^{\dag}_i}{}{\textbf{a}_j}
\textbf{a}^{\dag}_i \textbf{a}_j
 = \textbf{a}^{\dag}_i \textbf{a}_j
 - \{\textbf{a}^{\dag}_i \textbf{a}_j \}_{\nu} = \textbf{a}^{\dag}_i \textbf{a}_j
 +  \textbf{a}_j \textbf{a}^{\dag}_i = \delta_{ij} . \label{fermi1}
\end{equation}

\begin{equation}
\contraction{}{\textbf{a}_a}{}{\textbf{a}^{\dag}_b}
\textbf{a}_a \textbf{a}^{\dag}_b = \textbf{a}_a \textbf{a}^{\dag}_b - \{\textbf{a}_a \textbf{a}^{\dag}_b\}_{\nu} = \textbf{a}_a \textbf{a}^{\dag}_b + \textbf{a}^{\dag}_b \textbf{a}_a = \delta_{ab} . \label{fermi2}
\end{equation}
Any other contraction will be 0 using rules analogous to Eq. \eqref{wickexample1}

\subsection{Normal Ordered $\textbf{H}$}
As noted in Eq. \eqref{annih} $\textbf{H}$ can be expressed in terms of creation and annihilation operators. This expression is known as the secound-quantized form of the electronic Hamiltionan and will be repeated here, but the indices will be changed because $a,b,i,j$ have now been denoted a new meaning.

\begin{equation}
\textbf{H} = \sum_{pq} <p|\textbf{h}|q> \textbf{a}^{\dag}_q \textbf{a}_p + 
\frac{1}{4} \sum_{pqrs} \langle pq||rs \rangle \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r .
\end{equation}
We now wish to use Wick's theorem on this operator to simplify. From the one-electron term we use:

\begin{equation}
\textbf{a}^{\dag}_p \textbf{a}_q = \{\textbf{a}^{\dag}_p \textbf{a}_q \} + \{\contraction{}{\textbf{a}^{\dag}_p}{}{\textbf{a}_q}
 \textbf{a}^{\dag}_p \textbf{a}_q 
\} . 
\end{equation}
Eq. \eqref{fermi1} states that $\{\contraction{}{\textbf{a}^{\dag}_p}{}{\textbf{a}_q}
\textbf{a}^{\dag}_p \textbf{a}_q \}$ is not equal to zero only if both operators act on a hole space, then $\{\contraction{}{\textbf{a}^{\dag}_p}{}{\textbf{a}_q}
\textbf{a}^{\dag}_p \textbf{a}_q \}$ = $\delta_{pq}$. This means 

\begin{equation}
\sum_{pq}
\{\contraction{}{\textbf{a}^{\dag}_p}{}{\textbf{a}_q}
\textbf{a}^{\dag}_p \textbf{a}_q \} = \sum_i \langle i|h|i \rangle .
\end{equation}
Inserting this in $\textbf{H}$ we get:

\begin{equation}
\textbf{H} = \sum_{pq} <p|\textbf{h}|q> 
\{\textbf{a}^{\dag}_p \textbf{a}_q \}
+ \sum_i \langle i|h|i \rangle
 + 
\frac{1}{4} \sum_{pqrs} \langle pq||rs \rangle \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r . \label{temp_h}
\end{equation}
The Wick's theorem will also be applied to the two electron part, $\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r$. Included here are only the non-zero terms.

\begin{align}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r = & \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\} 
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s 
\textbf{a}_r
\}
+ \{
\textbf{a}^{\dag}_p
\contraction{}{\textbf{a}^{\dag}_q}{}{\textbf{a}_s}
\textbf{a}^{\dag}_q \textbf{a}_s 
\textbf{a}_r
\} \nonumber \\ &
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q \textbf{a}_s}{\textbf{a}_r}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s 
\textbf{a}_r
\}
+ \{\textbf{a}^{\dag}_p
\contraction{}{\textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}
\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r
\}
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s 
\textbf{a}_r
\} \nonumber \\ &
+ \{
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q \textbf{a}_s}{\textbf{a}_r}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s 
\textbf{a}_r
\} . \nonumber
\end{align}
These can be simplified using Eq. \eqref{fermi1}, and the rules for index swapping within a contraction noted in Eq. \eqref{wikssign}.

\begin{align}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r = &\{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\} 
+ \delta_{pi} \delta_{ps} \{ \textbf{a}^{\dag}_q \textbf{a}_r \}
\nonumber \\ & 
+ \delta_{qi} \delta_{qs}
\{ \textbf{a}^{\dag}_p \textbf{a}_r \}
+ \delta_{pi} \delta_{pr} 
\{ \textbf{a}^{\dag}_q \textbf{a}_s \}
+ \delta_{qi} \delta_{qr}
\{ \textbf{a}^{\dag}_p \textbf{a}_s \} \nonumber \\ &
- \delta_{pi} \delta_{ps} \delta_{qj} \delta_{qr}
+ \delta_{pi} \delta_{pr} \delta_{qj} \delta_{qs} .
\end{align}
The two electron part can now be replaced.

\begin{align}
\frac{1}{4} \sum_{pqrs} \langle pq||rs \rangle \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r = &
\frac{1}{4}
\sum_{pqrs} \langle pq||rs \rangle \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\}
\nonumber \\ &
- \frac{1}{4} \sum_{iqr} \langle iq||ri \rangle \{\textbf{a}^{\dag}_q \textbf{a}_r \} \nonumber \\ &
+ \frac{1}{4} \sum_{ipr} \langle pi||ri \rangle \{\textbf{a}^{\dag}_p \textbf{a}_r \} \nonumber \\ &
+ \frac{1}{4} \sum_{iqs} \langle iq||is \rangle \{\textbf{a}^{\dag}_q \textbf{a}_s \} \nonumber \\ &
- \frac{1}{4} \sum_{ips} \langle pi||is \rangle \{\textbf{a}^{\dag}_p \textbf{a}_s \} \nonumber \\ &
- \frac{1}{4} \sum_{ij} \langle ij||ji \rangle \nonumber \\ &
+ \frac{1}{4} \sum_{ij} \langle ij||ij \rangle . \nonumber
\end{align}
From the symmetry in the single bar four index integrals it can be shown that these symmetries hold for the double bar integrals:

\begin{equation}
\langle pq||rs \rangle =
\langle qp||sr \rangle =
- \langle pq||sr \rangle =
- \langle qp || rs \rangle , \label{dirac_symetry}
\end{equation}
and 

\begin{equation}
\langle pq || rs \rangle = \langle rs || pq \rangle ,
\end{equation}

making eightfold symmetry. Using Eq. \eqref{dirac_symetry}, re indexing terms and combining leaves the two electron part as the following:

\begin{align}
\frac{1}{4} \sum_{pqrs} \langle pq||rs \rangle \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r = & \frac{1}{4}
\sum_{pqrs} \langle pq||rs \rangle \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\}
 \\ &
+ \sum_{ipr} \langle pi||ri \rangle \{\textbf{a}^{\dag}_p \textbf{a}_r \} \nonumber \\ &
+ \frac{1}{2} \sum_{ij} \langle ij||ij \rangle . \nonumber
\end{align}
This can be inserted in Eq. \eqref{temp_h}.

\begin{align}
\textbf{H} = & \sum_{pq} <p|\textbf{h}|q> 
\{\textbf{a}^{\dag}_p \textbf{a}_q \}
+ \sum_i \langle i|h|i \rangle
 + \frac{1}{4}
\sum_{pqrs} \langle pq||rs \rangle \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\}
 \\ &
+ \sum_{ipr} \langle pi||ri \rangle \{\textbf{a}^{\dag}_p \textbf{a}_r \}
+ \frac{1}{2} \sum_{ij} \langle ij||ij \rangle . \nonumber
\end{align}
The first and fourth term on the right hand side are the normal ordered form of the Fock operator. If we also include the second term we have the HF energy.

\begin{equation}
\textbf{H} = \sum_{pq} f_{pq} 
\{\textbf{a}^{\dag}_p \textbf{a}_q \}
 + \frac{1}{4}
\sum_{pqrs} \langle pq||rs \rangle \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r\}
+ \langle \Psi_{HF} | \textbf{H} |\Psi_{HF} \rangle .
\end{equation}
We rename these terms.

\begin{equation}
\textbf{H} = \textbf{F}_N + \textbf{V}_N + \langle \Psi_{HF} | \textbf{H} |\Psi_{HF} \rangle .
\end{equation}
The normal ordered Hamiltonian is defined from this:

\begin{equation}
\textbf{H}_N \equiv 
\textbf{H} - \langle \Psi_{HF} | \textbf{H} |\Psi_{HF} \rangle = 
\textbf{F}_N + \textbf{V}_N . \label{normal_order_hamiltonian}
\end{equation}

\subsection{CCSD Hamiltonian}

The CCSD Hamiltonian is now defined as such

\begin{equation}
\bar{H} \equiv e^{-\textbf{T}} \textbf{H}_N e^{\textbf{T}}  .
\end{equation}
Using the CCSD cluster operator, $\textbf{T} = \textbf{T}_1 + \textbf{T}_2$. This can be inserted in equation Eq. \eqref{variationalccsd}. 

\begin{align}
\bar{H} = & 
\textbf{H}_N 
+ \left[ \textbf{H}_N, \textbf{T}_1 \right] 
+ \left[ \textbf{H}_N, \textbf{T}_2 \right] 
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_1 \right]  \\ &
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_2  \right]
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_2], \textbf{T}_1 \right]
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_2], \textbf{T}_2 \right] \dots \nonumber
\end{align}
$\textbf{T}_1$ and $\textbf{T}_2$ does commute, so we can combine terms. The full $\bar{H}$ then becomes:

\begin{align}
\bar{H} = & 
\textbf{H}_N 
+ \left[ \textbf{H}_N, \textbf{T}_1 \right] 
+ \left[ \textbf{H}_N, \textbf{T}_2 \right] 
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_1 \right] \label{temp_hamil_ccsd} \\ &
+ \left[ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_2  \right]
+ \frac{1}{2} \left[ [\textbf{H}_N, \textbf{T}_2], \textbf{T}_2 \right] \nonumber \\ &
+ \frac{1}{6} \left[ [ [ \textbf{H}_N,[\textbf{T}_1], \textbf{T}_1], \textbf{T}_1 \right]
+ \frac{1}{6} \left[ [ [ \textbf{H}_N,[\textbf{T}_2], \textbf{T}_2], \textbf{T}_2 \right] \nonumber \\ &
+ \frac{1}{2} \left[ [ [ \textbf{H}_N,[\textbf{T}_1], \textbf{T}_1], \textbf{T}_2 \right]
+ \frac{1}{2} \left[ [ [ \textbf{H}_N,[\textbf{T}_1], \textbf{T}_2], \textbf{T}_2 \right] \nonumber \\ &
+ \frac{1}{24} \left[ [ [ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_1], \textbf{T}_1], \textbf{T}_1  \right]
+ \frac{1}{24} \left[ [ [ [\textbf{H}_N, \textbf{T}_2], \textbf{T}_2], \textbf{T}_2], \textbf{T}_2  \right] \nonumber \\ &
+ \frac{1}{6} \left[ [ [ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_1], \textbf{T}_1], \textbf{T}_2  \right]
+ \frac{1}{6} \left[ [ [ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_1], \textbf{T}_2], \textbf{T}_2  \right] \nonumber \\ &
+ \frac{1}{4} \left[ [ [ [\textbf{H}_N, \textbf{T}_1], \textbf{T}_2], \textbf{T}_2], \textbf{T}_2  \right] . \nonumber
\end{align}
$\bar{H}$ will still analytically truncates after up to and including four nested commutators. When using $\textbf{H}_N$ it is better to rewrite Eqs. \eqref{t1defi} and \eqref{t2defi} using contractions.

\begin{equation}
\textbf{T}_1 \equiv \sum_{ai} t_i^a \textbf{a}^{\dag}_a \textbf{a}_i = \sum_{ai} \left( t_i^a \{\textbf{a}^{\dag}_a \textbf{a}_i\} + \{
\contraction{}{\textbf{a}^{\dag}_a}{}{\textbf{a}_i}
\textbf{a}^{\dag}_a \textbf{a}_i
\} \right) = \sum_{ai} t_i^a \{\textbf{a}^{\dag}_a \textbf{a}_i\} .
\end{equation}
The contraction term will be 0 based the discussion before Eq. \eqref{fermi1}. Also similarly for $\textbf{T}_2$

\begin{equation}
\textbf{T}_2 = \frac{1}{4} \sum_{abij} \{
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_i \textbf{a}_j \} .
\end{equation}
The commutators can then be calculated, starting with $[\textbf{H}_N, \textbf{T}_1]$.

\begin{equation}
[\textbf{H}_N, \textbf{T}_1] = \textbf{H}_N \textbf{T}_1 - \textbf{T}_1 \textbf{H}_N .
\end{equation}
Using the definition of contractions on both these terms we can simplify this expression.

\begin{align}
[\textbf{H}_N, \textbf{T}_1] & =
\left( \{\textbf{H}_N\textbf{T}_1\} + \{
\contraction{}{\textbf{H}_N}{}{\textbf{T}_1}
\textbf{H}_N\textbf{T}_1\} \right) - \left(\{\textbf{T}_1\textbf{H}_N\} + \{
\contraction{}{\textbf{T}_1}{}{\textbf{H}_N}
\textbf{T}_1\textbf{H}_N\} \right) \nonumber \\ &
= \{
\contraction{}{\textbf{H}_N}{}{\textbf{T}_1}
\textbf{H}_N\textbf{T}_1\}
- \{
\contraction{}{\textbf{T}_1}{}{\textbf{H}_N}
\textbf{T}_1\textbf{H}_N\} .
\end{align}
Eqs. \eqref{fermi1} and \eqref{fermi2} explains the only terms that will not be 0 when calculating the contractions. $\{ \contraction{}{\textbf{T}_1}{}{\textbf{H}_N}
\textbf{T}_1\textbf{H}_N\}$ will be 0 since $\textbf{T}_1$ does not contain any creation or annihilation operator that when placed on the left creates a non-zero contraction when using Wick's Theorem. The same argument applies to $\textbf{T}_2$. 

\begin{equation}
[\textbf{H}_N, \textbf{T}_1] = 
\{
\contraction{}{\textbf{H}_N}{}{\textbf{T}_1}
\textbf{H}_N\textbf{T}_1\} 
= \left( \textbf{H}_N \textbf{T}_1 \right)_C
.
\end{equation}

\begin{equation}
[\textbf{H}_N, \textbf{T}_2] = 
\{
\contraction{}{\textbf{H}_N}{}{\textbf{T}_2}
\textbf{H}_N\textbf{T}_2\} 
= \left( \textbf{H}_N \textbf{T}_2 \right)_C
.
\end{equation}
It then becomes clear that the only surviving terms when calculating all the commutators will be terms with $\textbf{H}_N$ in the leftmost position. \\

A new notation is also introduced, $()_C$. This notations means that each cluster operator inside the parentheses should have at least one contraction each to $\textbf{H}_N$ when applying Wick's Theorem. This holds for up to four cluster operators, which makes the truncation even more sensible. \\

The final form of $\bar{H}$ becomes

\begin{equation}
\begin{split}
\bar{H} = 
\big( \textbf{H}_N + \textbf{H}_N \textbf{T}_1 + \textbf{H}_N \textbf{T}_2
+ \frac{1}{2} \textbf{H}_N \textbf{T}_1^2
+ \frac{1}{2} \textbf{H}_N \textbf{T}_2^2
+ \textbf{H}_N \textbf{T}_1 \textbf{T}_2 \\
+ \frac{1}{6} \textbf{H}_N \textbf{T}_1^3
+ \frac{1}{6} \textbf{H}_N \textbf{T}_2^3
+ \frac{1}{2} \textbf{H}_N \textbf{T}_1^2 \textbf{T}_2
+ \frac{1}{2} \textbf{H}_N \textbf{T}_1 \textbf{T}_2^2 \\ 
+ \frac{1}{24} \textbf{H}_N \textbf{T}_1^4
+ \frac{1}{24} \textbf{H}_N \textbf{T}_2^4
+ \frac{1}{4} \textbf{H}_N \textbf{T}_1^2 \textbf{T}_2^2
+ \frac{1}{6} \textbf{H}_N \textbf{T}_1^3 \textbf{T}_2
+ \frac{1}{6} \textbf{H}_N \textbf{T}_1 \textbf{T}_2^3 \big)_C  .
\end{split} \label{CCSDHamiltonian}
\end{equation}

\subsection{CCSD Energy}
Using the definition of $\textbf{H}_N$, Eq. \eqref{normal_order_hamiltonian}, and $\bar{H}$, Eq. \eqref{CCSDHamiltonian}, we can now construct a programmable expression for the energy. 

\begin{equation}
E_{CCSD} - E_0 = \langle \Psi_0 | \bar{H} | \Psi_0 \rangle .
\end{equation}
The terms in Eq. \eqref{CCSDHamiltonian} are here calculated separately. 

\begin{equation}
\langle \Psi_0 | \textbf{H}_N | \Psi_0 \rangle = 0 .
\end{equation}
From the construction of the normal ordered Hamiltonian this term will be 0.

\begin{align}
\langle \Psi_0 | (\textbf{H}_N \textbf{T}_1)_C | \Psi_0 \rangle & = \langle \Psi_0 | \left((\textbf{F}_N + \textbf{V}_N ) \textbf{T}_1 \right)_C | \Psi_0 \rangle \nonumber \\ &
= \langle \Psi_0 | (\textbf{F}_N \textbf{T}_1)_C | \Psi_0 \rangle + \langle \Psi_0 | (\textbf{V}_N \textbf{T}_1)_C | \Psi_0 \rangle . \label{ene_1}
\end{align}

\begin{equation}
(\textbf{F}_N \textbf{T}_1)_C  = \sum_{pq} \sum_{ai} f_{pq} t_i^a  \{ \textbf{a}^{\dag}_p \textbf{a}_q\}  \{\textbf{a}^{\dag}_a \textbf{a}_i \} . 
\end{equation}
Wick's Theorem is applied to simplify the expression. Only non zero terms are included.

\begin{align}
\{ \textbf{a}^{\dag}_p \textbf{a}_q\}  \{\textbf{a}^{\dag}_a \textbf{a}_i \} & = &
\{ \textbf{a}^{\dag}_p \textbf{a}_q \textbf{a}^{\dag}_a \textbf{a}_i \} + \{ \textbf{a}^{\dag}_p
\contraction{}{\textbf{a}_q}{}{\textbf{a}^{\dag}_a}
\textbf{a}_q \textbf{a}^{\dag}_a
\textbf{a}_i \}  \\ & &
+  \{
\contraction{}{\textbf{a}^{\dag}_p}{ \textbf{a}_q \textbf{a}^{\dag}_a}{\textbf{a}_i}
\textbf{a}^{\dag}_p \textbf{a}_q \textbf{a}^{\dag}_a \textbf{a}_i \} 
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}_q \textbf{a}^{\dag}_a}{\textbf{a}_i}
\contraction{\textbf{a}^{\dag}_p}{\textbf{a}_q}{} {\textbf{a}^{\dag}_a}
\textbf{a}^{\dag}_p \textbf{a}_q \textbf{a}^{\dag}_a \textbf{a}_i \} \nonumber \\ & = &
\{ \textbf{a}^{\dag}_p \textbf{a}_q \textbf{a}^{\dag}_a \textbf{a}_i \} + \delta_{pi} \{\textbf{a}_q \textbf{a}_a^{\dag} \} \nonumber \\ & &
+ \delta_{qa} \{\textbf{a}_p^{\dag} \textbf{a}_i \} 
+ \delta_{pi} \delta_{qa} . \nonumber
\end{align}
Inserting this gives 

\begin{equation}
(\textbf{F}_N \textbf{T}_1)_C = \sum_{pq} \sum_{ai} f_{pq} t_i^a \left(\{ \textbf{a}^{\dag}_p \textbf{a}_q \textbf{a}^{\dag}_a \textbf{a}_i \} + \delta_{pi} \{\textbf{a}_q \textbf{a}_a^{\dag} \} 
+ \delta_{qa} \{\textbf{a}_p^{\dag} \textbf{a}_i \} 
+ \delta_{pi} \delta_{qa} \right) .
\end{equation}
When calculating $\langle \Psi_0 | (\textbf{F}_N \textbf{T}_1)_C | \Psi_0 \rangle$ only terms that solely consists of $\delta$'s will be non 0, since our basis is orthogonal. 

\begin{equation}
\langle \Psi_0 | (\textbf{F}_N \textbf{T}_1)_C | \Psi_0 \rangle =  \sum_{pq} \sum_{ai} f_{pq} t_i^a\delta_{pi} \delta_{qa} = \sum_{ai} f_{ai} t_i^a . \label{f_t1}
\end{equation}
$\langle \Psi_0 | (\textbf{V}_N \textbf{T}_1)_C | \Psi_0 \rangle$ must also be calculated.

\begin{equation}
(\textbf{V}_N \textbf{T}_1 )_C = \frac{1}{4} \sum_{pqrs} \sum_{ai} \langle pq||rs \rangle t_i^a \{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \} \{\textbf{a}_a^{\dag} \textbf{a}_i \} .
\end{equation}

\begin{align}
\{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \} \{\textbf{a}_a^{\dag} \textbf{a}_i \} & = \{
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag} \textbf{a}_i 
\} 
+ 
\{ \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\contraction{}{\textbf{a}_s}{\textbf{a}_r}{\textbf{a}_a^{\dag}}
 \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag}
  \textbf{a}_i 
\}  \\ &
+ 
\{ \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s
\contraction{}{\textbf{a}_r}{}{\textbf{a}_a^{\dag}}
\textbf{a}_r \textbf{a}_a^{\dag}
\textbf{a}_i
\}
+ \{ \textbf{a}^{\dag}_p
\contraction{}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag}}{\textbf{a}_i}
\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag} \textbf{a}_i \} \nonumber \\ &
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag}}{\textbf{a}_i}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag} \textbf{a}_i
\}
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag}}{\textbf{a}_i}
\contraction{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}_a^{\dag}}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag} \textbf{a}_i
\} \nonumber \\ &
+ \{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag}}{\textbf{a}_i}
\contraction{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}{\textbf{a}_a^{\dag}}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}_a^{\dag} \textbf{a}_i
\} . \nonumber
\end{align}
From the derivation of $\langle \Psi_0 | (\textbf{F}_N \textbf{T}_1)_C | \Psi_0 \rangle$ we noticed that the only term that survived was the term where every construction/annihilation operator was linked by a contraction. In this case we have no such terms. Hence the contribution from $\langle \Psi_0 | (\textbf{V}_N \textbf{T}_1)_C | \Psi_0 \rangle$ will be 0. \\

Inserting Eq. \eqref{f_t1} and $\langle \Psi_0 | (\textbf{V}_N \textbf{T}_1)_C | \Psi_0 \rangle = 0$ into Eq. \eqref{ene_1} gives 

\begin{equation}
\langle \Psi_0 | (\textbf{H}_N \textbf{T}_1)_C | \Psi_0 \rangle = \sum_{ai} f_{ai} t_i^a . \label{Energy_Contribution_1}
\end{equation}
Here the contribution from $\langle \Psi_0 | (\textbf{H}_N \textbf{T}_2)_C | \Psi_0 \rangle$ is calculated.

\begin{equation}
\langle \Psi_0 | (\textbf{F}_N \textbf{T}_2)_C | \Psi_0 \rangle = \frac{1}{4} \sum_{pq} \sum_{abij} f_{pq} t_{ij}^{ab} \{ \textbf{a}^{\dag}_p \textbf{a}_q \}
\{ \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_i \textbf{a}_j \} .
\end{equation}
This is again a similar situation that will result in 0 contribution. The reason is that any two operators $\textbf{A}$ and $\textbf{B}$ that contain a different number of annihilation/creation operators will not create any fully contracted terms (terms that solely consists of $\delta$'s) when applying Wick's Theorem. This means because of orthogonality the contribution to $E_{CCSD}$ from terms like this will always be 0. \\

$\langle \Psi_0 | (\textbf{V}_N \textbf{T}_2)_C | \Psi_0 \rangle$ however has an equal number of operators. From this we will have a contribution.

\begin{equation}
\langle \Psi_0 | (\textbf{V}_N \textbf{T}_2)_C | \Psi_0 \rangle = \frac{1}{16} \sum_{pqrs} \sum_{abij} \langle pq||rs \rangle t_{ij}^{ab} \langle \Psi_0|
\{
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r \}
\{
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i \}
| \Psi_0 \rangle \nonumber
\end{equation}
Wick's Theorem is applied. Only the four terms that are fully contracted and non-zero are listed.

\begin{align}
\{
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r \}
\{
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_i \textbf{a}_j \} & = &
\{
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_a}
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r
\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_j}{\textbf{a}_i}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i
\}
+
\{
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_j}{\textbf{a}_i}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i
\} \nonumber \\ & &
\{
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_a}
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r
\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b\textbf{a}_j}{\textbf{a}_i}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b}{\textbf{a}_j}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i
\}
+
\{
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b\textbf{a}_j}{\textbf{a}_i}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b}{\textbf{a}_j}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i
\} \nonumber \\
& = & \delta_{pi} \delta_{qj} \delta_{sb} \delta_{ra} 
- \delta_{pi} \delta_{gj} \delta_{rb} \delta_{sa}
+ \delta_{pj} \delta_{qi} \delta_{rb} \delta_{sa}
- \delta_{pj} \delta_{qi} \delta_{ra} \delta_{sb} .
\end{align}
Inserting this provides

\begin{align}
\langle \Psi_0 | (\textbf{V}_N \textbf{T}_2)_C | \Psi_0 \rangle & =  \frac{1}{16} \sum_{pqrs} \sum_{abij}  \langle pq||rs \rangle t_{ij}^{ab} \langle \Psi_0|
\delta_{pi} \delta_{qj} \delta_{sb} \delta_{ra} 
- \delta_{pi} \delta_{gj} \delta_{rb} \delta_{sa}\nonumber \\ & 
+ \delta_{pj} \delta_{qi} \delta_{rb} \delta_{sa}
- \delta_{pj} \delta_{qi} \delta_{ra} \delta_{sb}
| \Psi_0 \rangle \nonumber \\ &
= \frac{1}{16} \sum_{abij} ( 
\langle ij || ab \rangle
- \langle ij || ba \rangle
+ \langle ji || ba \rangle
- \langle ji || ab \rangle ) t_{ij}^{ab} \nonumber \\ &
= \frac{1}{4} \sum_{abij} t_{ij}^{ab} \langle ij||ab \rangle .
\end{align}
Here symmetry considerations was used. This means 

\begin{equation}
\langle \Psi_0 | (\textbf{H}_N \textbf{T}_2)_C | \Psi_0 \rangle = \frac{1}{4} \sum_{abij} t_{ij}^{ab} \langle ij||ab \rangle . \label{Energy_Contribution_2}
\end{equation}
Next contribution from $\langle \Psi_0 | (\textbf{H}_N \textbf{T}_1^2)_C | \Psi_0 \rangle$. From the expression of $\bar{H}$ there is a $\frac{1}{2}$ in front of this term.

\begin{align}
\frac{1}{2} \langle \Psi_0 | (\textbf{H}_N \textbf{T}_1^2)_C | \Psi_0 \rangle = & \frac{1}{8} \sum_{pqrs} \sum_{ai} \sum_{bj} \langle pq || rs \rangle t_i^a t_j^b \nonumber \\ & 
\langle \Psi_0| 
 \{\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_i \textbf{a}_j \}
\{\textbf{a}^{\dag}_a \textbf{a}_i \}
\{\textbf{a}^{\dag}_b \textbf{a}_j \}
| \Psi_0 \rangle .
\end{align}
Again Wick's Theorem is used. We note that from $\bar{H}$ there are four creation/annihilation operators. From each $\textbf{T}_1$ there are two, and combined from all there are four. This means we will have non-zero terms, these are listed here.

\begin{align}
\{\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b \textbf{a}_i \textbf{a}_j \}
\{\textbf{a}^{\dag}_a \textbf{a}_i \}
\{\textbf{a}^{\dag}_b \textbf{a}_j \}
 = &
\{
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a}{\textbf{a}_i}
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i}{\textbf{a}^{\dag}_b}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_a}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b \textbf{a}_j 
\}
+ 
\{
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i}{\textbf{a}^{\dag}_b}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_a}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a}{\textbf{a}_i}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b \textbf{a}_j 
\} \nonumber \\ & 
\{
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i}{\textbf{a}^{\dag}_b}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_a}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a}{\textbf{a}_i}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}_i
\textbf{a}^{\dag}_b \textbf{a}_j 
\}
+
\{
\contraction[4ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}
\contraction[5ex]{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s}{\textbf{a}_r}{\textbf{a}^{\dag}_a}{\textbf{a}^{\dag}_b}
\contraction[2ex]{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b\textbf{a}_j}{\textbf{a}_i}
\contraction{}{\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_a \textbf{a}^{\dag}_b}{\textbf{a}_j}
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_a \textbf{a}^{\dag}_b
\textbf{a}_j \textbf{a}_i
\} \nonumber \\
 = & \delta_{pi} \delta_{qj} \delta_{sb} \delta_{ra} 
- \delta_{pi} \delta_{gj} \delta_{rb} \delta_{sa}
+ \delta_{pj} \delta_{qi} \delta_{rb} \delta_{sa}
- \delta_{pj} \delta_{qi} \delta_{ra} \delta_{sb} .
\end{align}
Which is a result we have seen before in Eq. \eqref{Energy_Contribution_2}, the only difference is the amplitudes and the factor $\frac{1}{2}$.

\begin{equation}
\langle \Psi_0 | (\textbf{H}_N \textbf{T}_1^2)_C | \Psi_0 \rangle = \frac{1}{2} \sum_{abij} t_{i}^{a} t_j^b \langle ij||ab \rangle . \label{Energy_Contribution_3}
\end{equation}
The next term is $\langle \Psi_0 | (\textbf{H}_N \textbf{T}_2^2)_C | \Psi_0 \rangle$. $\textbf{H}_N$ still only have four creation/annihilation operators. However now we have 8 in total from the cluster operators. This means we cannot have fully contracted terms, which means the entire contribution to $E_{CCSD}$ will be 0. \\

This argument will hold true for every single remaining term. Meaning we now have an expression for the energy from Eqs. \eqref{Energy_Contribution_1}, \eqref{Energy_Contribution_2} and \eqref{Energy_Contribution_3}.

\begin{equation}
E_{CCSD} = E_0 + \sum_{ai} f_{ai} t_i^a + \frac{1}{4} \sum_{abij} \langle ij||ab \rangle t_{ij}^{ab} + \frac{1}{2} \sum_{abij} \langle ij || ab \rangle t_i^a t_j^b . \label{CCSD_TOTAL_ENERGY}
\end{equation}
Here all factors are known except for $t_i^a$ and $t_{ij}^{ab}$. These must be determined.

\subsection{$t_i^a$ amplitudes}
We can find expressions for $t_i^a$ by calculating $\langle \Psi_i^a | \bar{H} | \Psi_0 \rangle = 0$. The notation $\Psi_i^a$ means a state with one hole state and one orbital state. This will be an excited state and we did assume orthogonality. The mathematics of this excited state can be described as such

\begin{equation}
\langle \Psi_i^a | = \langle \Psi_0 | \textbf{a}^{\dag}_i \textbf{a}_a . \label{first_excited_stats}
\end{equation}
A creation operator working to the left, on a bra, becomes an annihilation operator. $\langle \Psi_i^a | \bar{H} | \Psi_0 \rangle = 0$ can be solved in the same manner as we did for the energy. Starting with the first term $\langle \Psi_i^a | \textbf{H}_N | \Psi_0 \rangle$.

\subsubsection{$\langle \Psi_i^a | \textbf{H}_N | \Psi_0 \rangle$}

\begin{align}
\langle \Psi_i^a | \textbf{H}_N | \Psi_0 \rangle & = 
\sum_{pq} f_{pq} \langle \Psi_0| \{ \textbf{a}^{\dag}_i \textbf{a}_a \} \{ \textbf{a}^{\dag}_p \textbf{a}_q \} | \Psi_0 \rangle \nonumber \\ &
+ \frac{1}{4} \sum_{pqrs} \langle pq||rs \rangle  \langle \Psi_0| \{ \textbf{a}^{\dag}_i \textbf{a}_a \} \{ \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_r \textbf{a}_s \} | \Psi_0 \rangle .
\end{align}
Here the first term is from $\textbf{F}_N$ and the second term from $\textbf{V}_N$. The second term will be zero. Eq. \eqref{first_excited_stats} is inserted and Wick's Theorem applied.

\begin{align}
\Rightarrow \langle \Psi_i^a | \textbf{H}_N | \Psi_0 \rangle & = \sum_{pq} f_{pq} \langle \Psi_0| \{ \textbf{a}^{\dag}_i \textbf{a}_a \} \{ \textbf{a}^{\dag}_p \textbf{a}_q \} | \Psi_0 \rangle \nonumber \\ &
= \sum_{pq} f_{pq} \langle \Psi_0| \{
\contraction{}{\textbf{a}^{\dag}_i}{\textbf{a}_a \textbf{a}^{\dag}_p}{\textbf{a}_q}
\contraction[3ex]{\textbf{a}^{\dag}_i}{\textbf{a}_a}{}{\textbf{a}^{\dag}_p}
\textbf{a}^{\dag}_i \textbf{a}_a \textbf{a}^{\dag}_p \textbf{a}_q 
 \} | \Psi_0 \rangle \nonumber \\ &
= \sum_{pq} f_{pq} \delta_{iq} \delta_{ap} \nonumber \\ &
= f_{ai} . \label{t1amp_1}
\end{align}

\subsubsection{$\langle \Psi_i^a | \textbf{H}_N \textbf{T}_1 | \Psi_0 \rangle$}

The next term includes $(\textbf{H}_N \textbf{T}_1)_c = (\textbf{F}_N \textbf{T}_1)_c + (\textbf{V}_N \textbf{T}_1)_c$. The $()_c$ notation is here applied to specify that there must be at least one contraction reaching from $\textbf{H}_N$ to $\textbf{T}_1$.

\begin{align}
\langle \Phi_i^a | (\textbf{F}_N \textbf{T}_1)_c | \Phi_0 \rangle  & = 
\sum_{pq} \sum_{jb} f_{pq} t_j^b \langle \Phi_0 | 
\{ \textbf{a}^{\dag}_i \textbf{a}_a \} (\{ \textbf{a}^{\dag}_p \textbf{a}_q \} \{
\textbf{a}^{\dag}_b \textbf{a}_j \})_c | \Phi_0 \rangle \nonumber \\ &
= \sum_{pq} \sum_{jb} f_{pq} t_j^b \left(
\{
\contraction{\textbf{a}^{\dag}_i
\textbf{a}_a}{\textbf{a}^{\dag}_p}{\textbf{a}_q
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[3ex]{}{\textbf{a}^{\dag}_i}{\textbf{a}_a
\textbf{a}^{\dag}_p}{\textbf{a}_q}
\contraction[3ex]{\textbf{a}^{\dag}_i}{\textbf{a}_a}{\textbf{a}_q}{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i
\textbf{a}_a
\textbf{a}^{\dag}_p
\textbf{a}_q
\textbf{a}^{\dag}_b
\textbf{a}_j
\}
+
\{
\contraction[2ex]{}{\textbf{a}^{\dag}_i}{\textbf{a}_a
\textbf{a}^{\dag}_p
\textbf{a}_q
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction{\textbf{a}^{\dag}_i}{\textbf{a}_a}{}{\textbf{a}^{\dag}_p}
\contraction{\textbf{a}^{\dag}_i
\textbf{a}_a
\textbf{a}^{\dag}_p}{\textbf{a}_q}{}{\textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i
\textbf{a}_a
\textbf{a}^{\dag}_p
\textbf{a}_q
\textbf{a}^{\dag}_b
\textbf{a}_j
\} \right) \nonumber \\ &
= \sum_{pq} \sum_{jb} f_{pq} t_j^b \left(
\delta_{iq} \delta_{ab} \delta_{pj} + 
\delta_{ij} \delta_{ap} \delta_{qb} \right) \nonumber \\ &
= - \sum_j f_{ji} t_j^a + \sum_b f_{ab} t_i^b
.
\end{align}

\begin{align}
\langle \Phi_i^a | (\textbf{V}_N \textbf{T}_1)_c | \Phi_0 \rangle  & = \frac{1}{4} \sum_{pqrs} \sum_{jb} \langle pq||rs\rangle  t_j^b \langle \Phi_0 | 
\{ \textbf{a}^{\dag}_i \textbf{a}_a \} (\{ \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r \} \{
\textbf{a}^{\dag}_b \textbf{a}_j \})_c | \Phi_0 \rangle \nonumber \\ &
= \frac{1}{4} \sum_{pqrs} \sum_{jb} \langle pq||rs \rangle t_j^b 
(
\{
\contraction[2ex]{}{\textbf{a}^{\dag}_i}{i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}{\textbf{a}_s}
\contraction{\textbf{a}^{\dag}_i}{\textbf{a}_a}{}{\textbf{a}^{\dag}_p}
\contraction[3ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[5ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b \textbf{a}_j
\} \nonumber \\ &
+ 
\{
\contraction[2ex]{}{\textbf{a}^{\dag}_i}{\textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q \textbf{a}_s}{\textbf{a}_r}
\contraction{\textbf{a}^{\dag}_i}{\textbf{a}_a}{}{\textbf{a}^{\dag}_p}
\contraction[3ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p}{\textbf{a}^{\dag}_q}{\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[5ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
}{\textbf{a}_s}{}{\textbf{a}_r \textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b \textbf{a}_j
\}
+ 
\{
\contraction[2ex]
{}
{\textbf{a}^{\dag}_i}
{i \textbf{a}_a \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}
{\textbf{a}_s}
\contraction
{\textbf{a}^{\dag}_i}
{\textbf{a}_a}
{\textbf{a}^{\dag}_p}
{\textbf{a}^{\dag}_q}
\contraction[3ex]
{\textbf{a}^{\dag}_i \textbf{a}_a}
{\textbf{a}^{\dag}_p}
{\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[5ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s}{\textbf{a}_r}{}{\textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b \textbf{a}_j
\} \nonumber \\ &
+ 
\{
\contraction[2ex]
{}
{\textbf{a}^{\dag}_i}
{i \textbf{a}_a \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q}
{\textbf{a}_s}
\contraction
{\textbf{a}^{\dag}_i}
{\textbf{a}_a}
{\textbf{a}^{\dag}_p}
{\textbf{a}^{\dag}_q}
\contraction[3ex]
{\textbf{a}^{\dag}_i \textbf{a}_a}
{\textbf{a}^{\dag}_p}
{\textbf{a}^{\dag}_q \textbf{a}_s \textbf{a}_r \textbf{a}^{\dag}_b}{\textbf{a}_j}
\contraction[5ex]{\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
}{\textbf{a}_s}{}{\textbf{a}_r \textbf{a}^{\dag}_b}
\textbf{a}^{\dag}_i \textbf{a}_a 
\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r
\textbf{a}^{\dag}_b \textbf{a}_j
\} ) \nonumber \\ &
=  \frac{1}{4} \sum_{pqrs} \sum_{jb} \langle pq||rs \rangle t_j^b (
\delta_{pj} \delta_{qa} \delta_{rb} \delta_{si} \nonumber \\ &
+ \delta_{pa} \delta_{qj} \delta_{ri} \delta_{sb}
- \delta_{pa} \delta_{qj} \delta_{rb} \delta_{si}
- \delta_{pj} \delta_{qa} \delta_{ri} \delta_{sb} ) \nonumber \\ &
= \sum_{jb} \langle ja||bi \rangle t_j^b .
\end{align}

In total the contribution to the amplitudes from $\langle \Psi_i^a | \textbf{H}_N \textbf{T}_1 | \Psi_0 \rangle$ is

\begin{equation}
\langle \Psi_i^a | (\textbf{H}_N \textbf{T}_1)_c | \Psi_0 \rangle = - \sum_j f_{ji} t_j^a + \sum_b f_{ab} t_i^b
\nonumber +  \sum_{jb} \langle ja||bi \rangle t_j^b . \label{t1amp_2}
\end{equation}

\subsubsection{$\frac{1}{2} \langle \Psi_i^a | (\textbf{H}_N \textbf{T}_1^2)_c | \Psi_0 \rangle$}
Still contributions from $(\textbf{F}_N \textbf{T}_1^2)_c$ and $(\textbf{V}_N \textbf{T}_1^2)_c$ calculated individually. The number of steps in the calculation is now reduced since $\delta_{ij}$ is understood to be a result of a non zero contraction between indices i and j.

\begin{align}
\frac{1}{2} \langle \Psi_i^a | (\textbf{F}_N \textbf{T}_1^2)_c | \Psi_0 \rangle & = 
\frac{1}{2} \sum_{pq} \sum_{jb} \sum_{kc} f_{pq} t_j^b t_k^c \langle \Psi_0 | \{\textbf{a}_i^{\dag} \textbf{a}_a \} \{\textbf{a}_p^{\dag} \textbf{a}_q \} \{\textbf{a}_b^{\dag} \textbf{a}_j \} \{\textbf{a}_c^{\dag} \textbf{a}_k \} | \Psi_0 \rangle \nonumber \\ &
= \frac{1}{2} \sum_{pq} \sum_{jb} \sum_{kc} f_{pq} t_j^b t_k^c
\left( -\delta_{pk} \delta_{qb} \delta_{ij} \delta_{ac}
- \delta_{pj} \delta_{qc} \delta_{ik} \delta_{ab} \right) \nonumber \\ &
= - \sum_{kc} f_{kc} t_i^c t_k^a .
\end{align}

\begin{align}
\frac{1}{2} \langle \Psi_i^a | (\textbf{V}_N \textbf{T}_1^2)_c | \Psi_0 \rangle &
= \frac{1}{8} \sum_{pqrs} \sum_{jb} \sum_{kc} 
\langle pq || rs \rangle t_j^b t_k^c \langle \Psi_0 | 
\{ \textbf{a}^{\dag}_i \textbf{a}_a \} \nonumber \\ &
\{\textbf{a}^{\dag}_p \textbf{a}^{\dag}_q
\textbf{a}_s \textbf{a}_r \} \{\textbf{a}^{\dag}_b \textbf{a}_j \} \{\textbf{a}^{\dag}_c \textbf{a}_k \}
| \Psi_0 \rangle \nonumber \\ &
= \frac{1}{8} \sum_{pqrs} \sum_{jb} \sum_{kc} 
\langle pq || rs \rangle t_j^b t_k^c (
\delta_{pa} \delta_{br} \delta_{sc} \delta_{qk} \delta_{ij} + \dots \nonumber \\ & 
= \sum_{jbs} \langle ja || bc \rangle t_j^b t_i^c
- \sum_{jbk} \langle jk || bi \rangle t_j^b t_k^a .
\end{align}

\subsubsection{Total}
The remaining terms are calculated similarly. For our purposes we list the result. A more complete derivation is available in Ref.\cite{non_refer_numba1}.

\begin{align}
0 = & f_{ai} + \sum_c f_{ac} t_i^c - \sum_k f_{ki} t_k^a + \sum_{kc} \langle ka||ci \rangle t_k^c + \sum_{kc} f_{kc} t_{ik}^{ac} + \frac{1}{2} \sum_{kcd} \langle ka || cd \rangle t_{ki}^{cd} \label{T1equation} \\ &
- \frac{1}{2} \sum_{klc} \langle kl||ci\rangle t_{kl}^{ca} - \sum_{kc} f_{kc} t_i^c t_k^a - \sum_{klc} \langle kl || ci \rangle t_k^c t_l^a + \sum_{kcd} \langle ka||cd \rangle t_k^c t_i^d \nonumber \\ & 
- \sum_{klcd} \langle kl || cd \rangle t_k^c t_i^d t_l^a + \sum_{klcd} \langle kl||cd\rangle t_k^c t_{li}^{da} \nonumber \\ &
 - \frac{1}{2} \sum_{klcd} \langle kl || cd \rangle t_{ki}^{cd} t_l^a 
- \frac{1}{2} \sum_{klcd} \langle kl||cd \rangle t_{kl}^{ca} t_i^d . \nonumber
\end{align}

\subsection{$t_{ij}^{ab}$ amplitudes}
$t_{ij}^{ab}$ amplitudes are generated in a similar fashion. These amplitudes are calculated by solving the equation

\begin{equation}
\langle \Psi_{ij}^{ab} | \bar{H} | \Psi_0 \rangle = 0 . \label{T2equationtosolve}
\end{equation}
This holds true because of the orthogonality. We can describe the state $\langle \Psi_{ij}^{ab}|$ in terms of creation and annihilation operators.

\begin{equation}
\langle \Psi_{ij}^{ab}| = \langle \Psi_0 | \{ \textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b \} .
\end{equation}
We can solve Eq. \eqref{T2equationtosolve}. Starting with the contribution from $\textbf{H}_N$. 

\begin{align}
\langle \Psi_{ij}^{ab} | \textbf{F}_N + \textbf{V}_N | \Psi_0 \rangle & = 
\langle \Psi_{0} | \{\textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b\} \left( \textbf{F}_N + \textbf{V}_N \right) | \Psi_0 \rangle \nonumber \\ &
= \sum_{pq} f_{pq} \langle \Psi_{0} | \{\textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b\} \left( \{ \textbf{a}^{\dag}_p \textbf{a}_q \} \right) | \Psi_0 \rangle \nonumber \\ & 
+ \frac{1}{4} \sum_{pqrs} \langle pq || rs \rangle \langle \Psi_{0} | \{\textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b\} \left( \{ \textbf{a}^{\dag}_p \textbf{a}^{\dag}_q 
\textbf{a}_s \textbf{a}_r\} \right) | \Psi_0 \rangle \nonumber \\ &
= \frac{1}{4} \sum_{pqrs} \langle pq || rs \rangle
( \delta_{pa} \delta_{qb} \delta_{ri} \delta_{sj} - \nonumber \\ &
\delta_{pb} \delta_{qa} \delta_{ri} \delta_{sj} - 
\delta_{pa} \delta_{qb} \delta_{si} \delta_{rj} +
\delta_{qa} \delta_{pb} \delta_{si} \delta_{rj} ) \nonumber \\ &
= \sum_{pqrs} \delta_{pb} \delta_{qa} \delta_{ri} \delta_{sj} \nonumber \\ &
= \langle ab || ij \rangle .
\end{align}
The contribution from $\textbf{F}_N$ will be zero. The contribution from $\textbf{H}_N \textbf{T}_1$ is more complicated.

\begin{equation}
\langle \Psi_{ij}^{ab} | (\textbf{F}_N + \textbf{V}_N)\textbf{T}_1 | \Psi_0 \rangle = \langle \Psi_{0} | \{\textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b\} \left( \textbf{F}_N + \textbf{V}_N \right) \textbf{T}_1 | \Psi_0 \rangle .
\end{equation}
These will be calculated individually. Here we will only calculate the contribution from $\textbf{V}_N \textbf{T}_1$.

\begin{align}
\langle \Psi_{0} | \{\textbf{a}^{\dag}_i \textbf{a}^{\dag}_j \textbf{a}_a \textbf{a}_b\}  \textbf{V}_N \textbf{T}_1 | \Psi_0 \rangle & = 
\frac{1}{4}
\sum_{pqrs}
\sum_{kc}
\langle pq|| rs \rangle t_k^c \langle \Phi_0 | \{
\textbf{a}^{\dag}_i
\textbf{a}^{\dag}_j
\textbf{a}_a
\textbf{a}_b \} \nonumber \\ & 
\left(
\{
\textbf{a}^{\dag}_p
\textbf{a}^{\dag}_q
\textbf{a}_s
\textbf{a}_r
\}
\{
\textbf{a}^{\dag}_c
\textbf{a}_k
\}
\right)_c | \Phi_0 \rangle
\nonumber \\ & = 
\frac{1}{4} \sum_{pqrs} \sum_{kc} \langle pq||rs\rangle t_k^c ( \delta_{pa} \delta_{qb} \delta_{rc} \delta_{sj} \delta_{ik}  \nonumber \\ & 
- \delta_{pa} \delta_{qb} \delta_{rc} \delta_{si} \delta_{jk}
- \delta_{pa} \delta_{qb} \delta_{rj} \delta_{sc} \delta_{ik}
+ \delta_{pa} \delta_{qb} \delta_{ri} \delta_{sc} \delta_{jk}
\nonumber \\ & 
+ \delta_{pa} \delta_{qk} \delta_{rj} \delta_{si} \delta_{bc}
- \delta_{pa} \delta_{qk} \delta_{ri} \delta_{sj} \delta_{bc}
+ \delta_{pb} \delta_{qa} \delta_{rj} \delta_{sc} \delta_{ik}
 \nonumber \\ & 
- \delta_{pb} \delta_{qa} \delta_{ri} \delta_{sc} \delta_{jk}
+ \delta_{pb} \delta_{qa} \delta_{rc} \delta_{si} \delta_{jk} 
+ \delta_{pb} \delta_{qk} \delta_{ri} \delta_{sj} \delta_{ac} \nonumber \\ & 
- \delta_{pb} \delta_{qk} \delta_{rj} \delta_{si} \delta_{ac}
- \delta_{pb} \delta_{qa} \delta_{rc} \delta_{sj} \delta_{ik}
+ \delta_{pk} \delta_{qa} \delta_{ri} \delta_{sj} \delta_{bc} \nonumber \\ & 
- \delta_{pk} \delta_{qb} \delta_{ri} \delta_{sj} \delta_{ac}
- \delta_{pk} \delta_{qa} \delta_{rj} \delta_{si} \delta_{bc}
+ \delta_{pk} \delta_{qb} \delta_{rj} \delta_{si} \delta_{ac}) \nonumber \\ &
= \sum_c \left( \langle ab || cj \rangle t_i^c - \langle ab || ci \rangle t_j^c \right) \nonumber \\ & 
+ \sum_k \left( \langle ij || bk \rangle t_k^a - \langle ij||ak\rangle t_k^b \right) .
\end{align}
The rest of Eq. \eqref{T2equationtosolve} can be solved in a similar manner. This is a task testing stamina and determination. Here we will simply state the result, however there is a more complete derivation using "Feynman Diagrams" available in Appendix A. \\

Before we state the final result we must define a permutation operator, $\textbf{P}$.

\begin{equation}
\textbf{P}(ab) f(a,b) = f(a,b) - f(b,a) .
\end{equation}
An example of this would be:

\begin{equation}
\textbf{P}(ab) \sum_{abij} t_i^a t_j^b f_{ai} = \sum_{abij} \left( t_i^a t_j^b f_{ai} - t_i^b t_j^a f_{bi} \right) .
\end{equation} 
Using this definition and solving the rest of Eq. \eqref{T2equationtosolve} the expression becomes the following:

\begin{align}
0 = & \langle ab || ij \rangle
+ \textbf{P}(ab) \sum_c f_{bc} t_{ij}^{ac}
- \textbf{P}(ij) \sum_k f_{kj} t_{ik}^{ab}
+ \frac{1}{2} \sum_{kl} \langle kl||ij \rangle t_{kl}^{ab} \label{T2equation} \\ &
+ \frac{1}{2} \sum_{cd} \langle ab || cd \rangle t_{ij}^{cd}
+ \textbf{P}(ij) \textbf{P}(ab) \sum_{kc}
\langle kb||cj \rangle t_{ik}^{ac} \nonumber \\ &
+ \textbf{P}(ij) \sum_c \langle ab || cj \rangle t_i^c
- \textbf{P}(ab) \sum_k \langle kb || ij \rangle t_k^a
\nonumber \\ &
+ \frac{1}{2} \textbf{P}(ij) \textbf{P}(ab) \sum_{klcd}
\langle kl || cd \rangle t_{ik}^{ac} t_{lj}^{db} 
+ \frac{1}{4} \sum_{klcd} \langle kl || cd \rangle
t_{ij}^{cd} t_{kl}^{ab} \nonumber \\ &
-  \frac{1}{2} \textbf{P}(ab)\sum_{klcd} \langle kl || cd \rangle t_{ij}^{ac} t_{kl}^{bd}
- \frac{1}{2} \textbf{P}(ij) \sum_{klcd} \langle kl || cd \rangle t_{ik}^{ab} t_{jl}^{cd} \nonumber \\ &
+ \frac{1}{2} \textbf{P}(ab) \sum_{kl}
\langle kl || ij \rangle t_k^a t_l^b 
+ \frac{1}{2} \textbf{P}(ij) \sum_{cd} \langle ab || cd \rangle t_i^c t_j^d \nonumber \\ &
- \textbf{P}(ij) \textbf{P}(ab) \sum_{kc} \langle kb || ic \rangle t_k^a t_j^c
+  \textbf{P}(ab) \sum_{kc} f_{kc} t_k^a t_{ij}^{bc} 
\nonumber \\ &
+ \textbf{P}(ij) \sum_{kc} f_{kc} t_i^c t_{jk}^{ab}
- \textbf{P}(ij) \sum_{klc} \langle kl || ci \rangle t_k^c t_{lj}^{ab}  \nonumber \\ &
+ \textbf{P} (ab) \sum_{kcd} \langle ka || cd \rangle t_k^c t_{ij}^{db} 
+ \textbf{P}(ij) \textbf{P}(ab) \sum_{kcd} \langle ak || dc \rangle t_i^d t_{jk}^{bc} \nonumber \\ &
+ \textbf{P} (ij) \textbf{P}(ab) \sum_{klc} \langle kl || ic \rangle t_l^a t_{jk}^{bc} 
+ \frac{1}{2} \textbf{P}(ij) \sum_{klc} \langle kl || cj \rangle t_i^c t_{kl}^{ab} \nonumber \\ &
- \frac{1}{2} \textbf{P}(ab) \sum_{kcd} \langle kb || cd \rangle t_k^a t_{ij}^{cd} 
- \frac{1}{2} \textbf{P}(ij) \textbf{P}(ab) \sum_{kcd} \langle kb||cd \rangle t_i^c t_k^a t_j^d \nonumber \\ &
+ \frac{1}{2} \textbf{P}(ij) \textbf{P}(ab) \sum_{klc} \langle kl || cj \rangle t_i^c t_k^a t_l^b
- \textbf{P}(ij) \sum_{klcd} \langle kl || cd \rangle t_k^c t_i^d t_{lj}^{ab} \nonumber \\ &
- \textbf{P}(ab) \sum_{klcd} \langle kl||cd \rangle t_k^c t_l^a t_{ij}^{db}
+ \frac{1}{4} \textbf{P}(ij) \sum_{klcd} \langle kl || cd \rangle t_i^c t_j^d t_{kl}^{ab} \nonumber \\ &
+ \frac{1}{4} \textbf{P}(ab) \sum_{klcd} \langle kl || cd \rangle t_k^a t_l^b t_{ij}^{cd}
+ \textbf{P}(ij) \textbf{P}(ab) \sum_{klcd} \langle kl || cd \rangle t_i^c t_l^b t_{kj}^{ad} \nonumber \\ &
+ \frac{1}{4} \textbf{P}(ij) \textbf{P} (ab) \sum_{klcd} \langle kl || cd \rangle t_i^c t_k^a t_j^d t_l^b . \nonumber
\end{align}
See Ref.\cite{non_refer_numba1} for the full derivation.

\section{Introducing denominators}
The expressions for $t_i^a$ and $t_{ij}^{ab}$ are complex and it is not easy to understand how to implement these equations effectively. The rest of this chapter and the next will be dedicated to simplifying Eqs. \eqref{T1equation} and \eqref{T2equation}.

\subsection{$t_i^a$}
Eq. \eqref{T1equation} should be rewritten considerably before it is programmable. Starting with a definition of $D_i^a$.

\begin{equation}
D_i^a \equiv f_{ii} - f_{aa} . \label{D_i_a_def} 
\end{equation}
Remembering Eq. \eqref{T1equation} we know it starts like this:

\begin{align}
0 = f_{ai} + \sum_c f_{ac} t_i^c - \sum_k f_{ki} t_k^a + \sum_{kc} \langle ka||ci \rangle t_k^c + \sum_{kc} f_{kc} t_{ik}^{ac} + \dots . \label{t1equationstart11}
\end{align}
The term $\sum_c f_{ac} t_i^c$ can be rewritten.

\begin{equation}
\sum_c f_{ac} t_i^c = f_{aa} t_i^a + \sum_c 
(1 - \delta_{ca} ) f_{ac} t_i^c .
\end{equation}
Doing the same with the term $\sum_k f_{ki} t_k^a$ and inserting in Eq. \eqref{t1equationstart11} we get:

\begin{align}
0 = f_{ai} + f_{aa} t_i^a + \sum_c 
(1 - \delta_{ca} ) f_{ac} t_i^c - f_{ii} t_i^a - \sum_k (1 - \delta_{ki}) f_{ki} t_k^a + \dots \nonumber
\end{align}
The two terms $f_{aa} t_i^a$ and $f_{ii} t_i^a$ are combined using the definition Eq. \eqref{D_i_a_def}. 

\begin{equation}
f_{aa} t_i^a - f_{ii} t_i^a = -D_i^a t_i^a .
\end{equation}
This is inserted into Eq. \eqref{T1equation}, and moved to the other side of the equation.

\begin{equation}
D_i^a t_i^a = f_{ai} + \sum_c 
(1 - \delta_{ca} ) f_{ac} t_i^c - \sum_k (1 - \delta_{ki}) f_{ki} t_k^a + \dots
\end{equation}
If we perform the same procedure for $t_{ij}^{ab}$ we can solve this iteratively until self consistency is reached.

\subsection{$t_{ij}^{ab}$}
We also implement a denominator $D_{ij}^{ab}$ in Eq. \eqref{T2equation}. 

\begin{equation}
D_{ij}^{ab} \equiv f_{ii} + f_{jj} - f_{aa} - f_{bb}
\end{equation}
Here we want to create the term $D_{ij}^{ab} t_{ij}^{ab}$. This is done with the same procedure with the two terms $\textbf{P}(ab) \sum_c f_{bc} t_{ij}^{ac}
- \textbf{P}(ij) \sum_k f_{kj} t_{ik}^{ab}$ from Eq. \eqref{T2equation}. These two terms can be expressed in a different manner.

\begin{align}
\textbf{P}(ab) \sum_c f_{bc} t_{ij}^{ac}
- \textbf{P}(ij) \sum_k f_{kj} t_{ik}^{ab} = & 
f_{aa} t_{ij}^{ab} + f_{bb} t_{ij}^{ab} + 
\textbf{P}(ab) \sum_c (1-\delta_{bc}) f_{bc} t_{ij}^{ac} \nonumber \\ &
- \textbf{P}(ij) \sum_k (1-\delta_{kj}) f_{kj} t_{ik}^{ab}
- f_{ii} t_{ij}^{ab}
- f_{jj} t_{ij}^{ab} . \nonumber
\end{align}
Then the four terms where no sums are present are combined into $D_{ij}^{ab} t_{ij}^{ab}$ and moved to the other side of the equation. This leaves another problem which we can solve iteratively.

\begin{equation}
D_{ij}^{ab} t_{ij}^{ab} = \langle ab||ij \rangle + + 
\textbf{P}(ab) \sum_c (1-\delta_{bc}) f_{bc} t_{ij}^{ac} - \dots \nonumber
\end{equation}
Here those three dots represent the rest of Eq. \eqref{T2equation}.

\subsection{Initial guess}
The initial guess from where to start the iterative process can be anything. However it is common to start an initial guess where all the amplitudes on the right side are 0. This leaves

\begin{equation}
t_i^a = \frac{f_{ai}}{D_i^a} .
\end{equation}

\begin{equation}
t_{ij}^{ab} = \frac{\langle ab || ij \rangle}{D_{ij}^{ab}} .
\end{equation}
However it is also common to simply guess $t_i^a = 0$. We will make this initial guess to make benchmarking the number of iterations easier. \\

The iterative procedure is one where $t_i^a$ and $t_{ij}^{ab}$ are updated simultaneously, and in theory we have converged once these amplitudes stop changing. However in practice we define a convergence criteria. This is then compared to the change in energy each iteration with the newly updated amplitudes. We then define convergence to when the energy stop changing, which should for all intents and purposes be an equivalent criteria. 

\section{Variational Principle}
The energy expression in CC contains the operator $e^{-\textbf{T}} \textbf{H} e^{\textbf{T}}$, which is not Hermitian. This means the variational principle no longer applies. It is possible to use the variational principle with CC, but this is a huge complication. This also means it is possible with coupled cluster to get energies lower than the true ground state energy.

\chapter{CCSD Factorization}
CCSD with a closed shell spin restricted Hartree Fock (RHF) basis can simplify our CCSD calculations considerably. In this chapter we will find an algorithm for a serial program to solve these equations using the RHF basis. A serial program is one which does not run in parallel. A lot of work have been done to reduce the number of calculations needed for each iteration. Most of this work involves the definition of intermediate variables that can be calculated separately. \\

The factorization is based on the work of J. F. Stanton and J. Gauss and is listed in Ref.\cite{ccsd_fac1}. However at the time of this thesis there where some typos in the factorized equations, so we will repeat the calculations. Also Ref.\cite{ccsd_fac3} holds information on factorization. Additional information on symmetry is found in an article by P. Carsky, Ref.\cite{ccsd_fac2}. H. Koch et al. also published a paper on CCSD using AOs in the equation, and partly avoiding the AO to MO transformation, Ref.\cite{ccsd_fac4}. This is an alternative to our solution. \\

One simplification we have already discussed is the introduction of $D_i^a$ and $D_{ij}^{ab}$. These denominators are independent of the amplitudes, hence they can be calculated and stored outside any iterative procedure.

\newpage

\section{Constructing an algorithm}
When constructing this CCSD serial algorithm we want a few things in mind. First we wish to minimize the number of FLOPS. Second we want to utilize external linear algebra libraries to perform the calculations. Third we want the option to easily modify the algorithm to work in parallel. This third brings up a few new concerns. One of which is that effective parallel programs are ones that minimize the communication between nodes, in part by  symmetry considerations. This issue will be dealt with in a later chapter in greater detail.

\subsection{Inserting denominators}

\subsubsection{$t_i^a$}
First we insert $D_i^a$ into Eq. \eqref{t1amp_1}. We also insert our notation for $\langle ab || ij \rangle$ which is

\begin{equation}
I_{ij}^{ab} = \langle ab || ij \rangle .
\end{equation}

\begin{align}
D_i^a t_i^a = & 
f_{ai} 
+ \sum_{a\not= c} f_{ac} t_i^c 
- \sum_{i \not= k} f_{ki} t_k^a 
+ \sum_{kc} I_{ka}^{ci} t_k^c 
+ \sum_{kc} f_{kc} t_{ik}^{ac} \nonumber \\ &
+ \frac{1}{2} \sum_{kcd} I_{ka}^{cd} t_{ki}^{cd} 
- \frac{1}{2} \sum_{klc} I_{kl}^{ci} t_{kl}^{ca} 
- \sum_{kc} f_{kc} t_i^c t_k^a 
- \sum_{klc} I_{kl}^{ci} t_k^c t_l^a \nonumber \\ & 
 + \sum_{kcd} I_{ka}^{cd} t_k^c t_i^d 
- \sum_{klcd} I_{kl}^{cd} t_k^c t_i^d t_l^a 
+ \sum_{klcd} I_{kl}^{cd} t_k^c t_{li}^{da} \nonumber \\ &
 - \frac{1}{2} \sum_{klcd} I_{kl}^{cd} t_{ki}^{cd} t_l^a 
- \frac{1}{2} \sum_{klcd} I_{kl}^{cd} t_{kl}^{ca} t_i^d
. 
\end{align}
The calculation of $t_i^a$ scales as $n^6$.

\subsubsection{$t_{ij}^{ab}$}
We do the exact same procedure for $t_{ij}^{ab}$ amplitudes. Insert $D_{ij}^{ab}$ and combine terms into the same sums.

\begin{align}
D_{ij}^{ab} t_{ij}^{ab} = & 
I_{ab}^{ij}
+ \frac{1}{2} \sum_{kl} I_{kl}^{ij} t_{kl}^{ab} 
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_{ij}^{cd}
+ \frac{1}{4} \sum_{klcd} I_{kl}^{cd}
t_{ij}^{cd} t_{kl}^{ab} 
 \nonumber \\ &
- \sum_{k \not= j} f_{kj} t_{ik}^{ab} 
+ \sum_{k \not= i} f_{ki} t_{jk}^{ab}
+ \sum_{c \not= b} f_{bc} t_{ij}^{ac}
- \sum_{c \not= a} f_{ac} t_{ij}^{bc}
 \nonumber \\ &
+ \textbf{P}(ab) 
\{
\sum_{kc} f_{kc} t_k^a t_{ij}^{bc}
- \sum_k I_{kb}^{ij} t_k^a
+ \frac{1}{2} \sum_{kl} I_{kl}^{ij} t_k^a t_l^b 
\nonumber \\ &
+ \sum_{kcd} 
(
I_{ka}^{cd} t_k^c t_{ij}^{db} 
- \frac{1}{2} I_{kb}^{cd} t_k^a t_{ij}^{cd} 
)
+ \sum_{klcd} I_{kl}^{cd} (\frac{1}{4} t_k^a t_l^b t_{ij}^{cd} - t_k^c t_l^a t_{ij}^{db} - \frac{1}{2} t_{ij}^{ac} t_{kl}^{bd})
\}
\nonumber \\ &
+ \textbf{P}(ij)
\{
\sum_c I_{ab}^{cj} t_i^c
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_i^c t_j^d 
+ \sum_{kc} f_{kc} t_i^c t_{jk}^{ab}
\nonumber \\ &
+ \sum_{klc}
( 
\frac{1}{2} 
I_{kl}^{cj} t_i^c t_{kl}^{ab}
- I_{kl}^{ci} t_k^c t_{lj}^{ab}
)
+ \sum_{klcd} I_{kl}^{cd}
(
\frac{1}{4} t_i^c t_j^d t_{kl}^{ab} 
- t_k^c t_i^d t_{lj}^{ab} 
- \frac{1}{2} t_{ik}^{ab} t_{jl}^{cd}
)
\}
\nonumber \\ &
+ \textbf{P}(ab) \textbf{P}(ij)
\{
\sum_{kc}
(
I_{kb}^{cj} t_{ik}^{ac}
- I_{kb}^{ic} t_k^a t_j^c
)
+ \sum_{kcd}
(
I_{ak}^{dc} t_i^d t_{jk}^{bc}
- I_{kb}^{cd} t_i^c t_k^a t_j^d
)
\nonumber \\ &
+ \sum_{klc}
(
\frac{1}{2} I_{kl}^{cj} t_i^c t_k^a t_l^b
+ I_{kl}^{ic} t_l^a t_{jk}^{bc}
)
+ \sum_{klcd} I_{kl}^{cd}
(
t_i^c t_l^b t_{kj}^{ad}
+ \frac{1}{4} t_i^c t_k^a t_j^d t_l^b
+ \frac{1}{2} t_{ik}^{ac} t_{lj}^{db} 
)
\} . 
\end{align}
Calculating this scales as $n^8$ and can go faster with the definition of intermediates. Much of the material presented is based on the work of John F. Stanton and Jurgen Gauss.

\subsection{$[W_1]$}

We first factor out and rewrite the following terms that as they stand now are calculated for all $a,b,i,j$ but only change when $i$ or $j$ changes.

\begin{align}
& 
\frac{1}{2} \sum_{kl} I_{kl}^{ij} t_{kl}^{ab} + \frac{1}{2} \textbf{P}(ij) \sum_{klc} I_{kl}^{cj} t_i^c t_{kl}^{ab} + \textbf{P}(ij) \frac{1}{4} \sum_{klcd} I_{kl}^{cd}  t_i^c t_j^d t_{kl}^{ab}
+ \frac{1}{4} \sum_{klcd} I_{kl}^{cd} t_{ij}^{cd} t_{kl}^{ab}
\nonumber \\ 
= &
\frac{1}{2} \sum_{kl} t_{kl}^{ab} \left[ I_{kl}^{ij} +  \sum_c \left(I_{kl}^{cj} t_i^c - I_{kl}^{ci} t_j^c + \frac{1}{2} \sum_{d}  I_{kl}^{cd} (t_i^c t_j^d - t_j^c t_i^d + t_{ij}^{cd})
 \right) \right]
\nonumber \\ 
= & \frac{1}{2} \sum_{kl} t_{kl}^{ab} [W_1]_{ij}^{kl} 
.
\end{align}
Meaning a new intermediate is now defined.

\begin{equation}
[W_1]^{kl}_{ij} = I_{kl}^{ij} +  \sum_c \left(I_{kl}^{cj} t_i^c - I_{kl}^{ci} t_j^c + \frac{1}{2} \sum_{d}  I_{kl}^{cd} (t_i^c t_j^d - t_j^c t_i^d + t_{ij}^{cd})
 \right) . \label{intermedw1}
\end{equation}
The calculation of this intermediate scales as $n^6$. $[W_1]$ appear one more time in our equations. These terms can also be combined and factorized.

\begin{align}
& \textbf{P}(ab) \frac{1}{2} \sum_{kl} I_{kl}^{ij} t_k^a t_l^b
+ \frac{1}{2} \sum_{klc} \textbf{P}(ij) \textbf{P}(ab) I_{kl}^{cj} t_i^c t_k^a t_l^b 
\nonumber \\ &
+ \frac{1}{4} \textbf{P}(ab) \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} t_i^c t_k^a t_j^d t_l^b
+ \textbf{P}(ab) \sum_{klcd} I_{kl}^{cd} \frac{1}{4} t_k^a t_l^b t_{ij}^{cd}
\nonumber \\ 
= &
\frac{1}{2} \sum_{kl} (t_k^a t_l^b - t_k^b t_l^a) \left[ I_{kl}^{ij} + \sum_c \left( \textbf{P}(ij) I_{kl}^{cj} t_i^c +
\frac{1}{2} \sum_d I_{kl}^{cd} ( t_i^c t_j^d - t_j^c t_i^d + t_{ij}^{cd}
\right) \right] \nonumber \\ 
= &
\frac{1}{2} \sum_{kl} (t_k^a t_l^b - t_k^b t_l^a) [W_1]_{ij}^{kl} .
\end{align}

\subsection{$[W_2]$}
Now we combine the following terms:

\begin{align}
& - \textbf{P}(ab) \sum_k t_k^a I_{kb}^{ij}
- \textbf{P}(ab) \frac{1}{2} \sum_{kcd} I_{kb}^{cd} t_{ij}^{cd} t_k^a
- \textbf{P}(ab) \textbf{P}(ij) \sum_{kc} t_k^a I_{kb}^{ic} t_j^c 
\nonumber \\
& - \textbf{P}(ab) \textbf{P}(ij) \sum_{kcd} I_{kb}^{cd} t_i^c t_j^d t_k^a
\nonumber \\
= &
- \textbf{P}(ab) \sum_k t_k^a \left[
I_{kb}^{ij} 
+ \sum_c \left( I_{kb}^{ic} t_j^c
- I_{kb}^{jc} t_i^c
+ \frac{1}{2} \sum_{d} I_{kb}^{cd} (t_{ij}^{cd}
+ t_i^c t_j^d - t_j^c t_i^d
) \right) \right]
\nonumber \\
= &
- \textbf{P}(ab) \sum_k t_k^a [W_2]_{ij}^{kb} \nonumber \\
= &
- \textbf{P}(ab) \sum_k t_k^b [W_2]_{ij}^{ak} .
\end{align}
We have now defined another intermediate.

\begin{equation}
[W_2]_{ij}^{ak} = I_{ak}^{ij} 
+ \sum_c \left( I_{ak}^{ic} t_j^c
- I_{ak}^{jc} t_i^c
+ \frac{1}{2} \sum_{d} I_{ak}^{cd} (t_{ij}^{cd}
+ t_i^c t_j^d - t_j^c t_i^d
) \right) . \label{intermedW2}
\end{equation}
This term scales as $n^6$.

\subsection{$[W_3]$}
Our next intermediate is defined by the following two terms:

\begin{equation}
\textbf{P}(ab) \textbf{P}(ij) \sum_{klc} I_{kl}^{ic} t_l^a t_{jk}^{bc}
+ \textbf{P}(ab) \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} t_i^c t_l^b t_{kj}^{ad} .
\end{equation}

Since c and d are arbitrary indices we can relabel the second term. We can also use a trick with the permutation operators where

\begin{equation}
\textbf{P}(ab) \textbf{P}(ij) f(a,b,i,j) = 
- \textbf{P}(ab) \textbf{P}(ij) f(b,a,i,j) ,
\end{equation}
and also the symmetry of I where

\begin{equation}
I_{kl}^{dc} = - I_{kl}^{cd} ,
\end{equation}

\begin{equation}
\Rightarrow =
\textbf{P}(ab) \textbf{P}(ij) \left(
\sum_{klc} I_{kl}^{ic} t_l^a t_{jk}^{bc}
+ \sum_{klcd} I_{kl}^{cd} t_i^c t_l^a t_{kj}^{bc}
\right) .
\end{equation}
Here we insert some more symmetry considerations.

\begin{equation}
\Rightarrow =
\textbf{P}(ab) \textbf{P}(ij) \left[ \sum_{klc} t_l^a \left(
- I_{kl}^{ci} t_{jk}^{bc} - \sum_d I_{kl}^{cd} t_i^c t_{jk}^{bc} \right) \right] .
\end{equation}
Factorizing out and defining new intermediate.

\begin{align}
\Rightarrow = &
- \textbf{P}(ab) \textbf{P}(ij) \left[ \sum_{klc} t_{jk}^{bc} t_l^a \left(
 I_{kl}^{ci} + \sum_d I_{kl}^{cd} t_i^c \right) \right] \nonumber \\
= &
- \textbf{P}(ab) \textbf{P}(ij) \sum_{klc}
t_{jk}^{bc} t_l^a
 [W_3]_{ci}^{kl} .
\end{align}

\begin{equation}
[W_3]_{ci}^{kl} = I_{kl}^{ci} + \sum_d I_{kl}^{cd} t_i^c  . \label{intermedW3}
\end{equation}
This term scales as $n^5$.

\subsection{$[F_1]$}
Until now all intermediates have been four dimensional. Now we define our first two dimensional intermediate. This is defined from the terms

\begin{align}
& \textbf{P}(ab) \sum_{kc} f_{kc} t_k^a t_{ij}^{bc}
- \textbf{P}(ab) \sum_{klcd} I_{kl}^{cd} t_k^c t_l^a t_{ij}^{db} \nonumber \\ 
= & 
\textbf{P}(ab) \sum_{kc} f_{kc} t_k^a t_{ij}^{bc}
+ \textbf{P}(ab) \sum_{klcd} I_{kl}^{cd} t_l^d t_k^a t_{ij}^{bc} \nonumber \\
= &
\textbf{P}(ab) \sum_{kc} t_k^a t_{ij}^{bc} \left[ f_{kc} + \sum_{ld} I_{kl}^{cd} t_l^d \right] \nonumber \\
= &
\textbf{P}(ab) \sum_{kc} t_k^a t_{ij}^{bc} [F_1]_k^c .
\end{align}

\begin{equation}
[F_1]_k^c = f_{kc} + \sum_{ld} I_{kl}^{cd} t_l^d . \label{intermedF1}
\end{equation}
This intermediate is also repeated when combining the terms

\begin{align}
& \textbf{P}(ij) \sum_{kc} f_{kc} t_i^c t_{jk}^{ab}
- \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} t_k^c t_i^d t_{lj}^{ab} \nonumber \\ 
= &
- \textbf{P}(ij) \sum_{kc} f_{kc} t_i^c t_{kj}^{ab}
- \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} t_l^d t_i^c t_{kj}^{ab} \nonumber \\
= &
- \textbf{P}(ij) \sum_{kc} t_i^c t_{kj}^{ab} \left[
f_{kc} + \sum_{cd} I_{kl}^{cd} t_l^d \right] \nonumber \\ 
= &
- \textbf{P}(ij) \sum_{kc} t_i^c t_{kj}^{ab} [F_1]_k^c
.
\end{align}
This term scales as $n^4$. Inserting this into the equations for $t_{ij}^{ab}$ leaves the following:

\begin{align}
D_{ij}^{ab} t_{ij}^{ab} = & 
I_{ab}^{ij}
+ \frac{1}{2} \sum_{kl} (t_k^a t_l^b - t_k^b t_l^a + t_{kl}^{ab}) [W_1]_{ij}^{kl}
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_{ij}^{cd}
 \nonumber \\ &
- \sum_{k \not= j} f_{kj} t_{ik}^{ab} 
+ \sum_{k \not= i} f_{ki} t_{jk}^{ab}
- \sum_{c \not= b} f_{bc} t_{ij}^{ac}
+ \sum_{c \not= a} f_{ac} t_{ij}^{bc}
 \nonumber \\ &
+ \textbf{P}(ab) 
\{
- \sum_k t_k^b [W_2]_{ij}^{ak}
+ \sum_{kc} t_k^a t_{ij}^{bc} [F_1]_k^c
+ \frac{1}{2} \sum_{kl} I_{kl}^{ij} t_k^a t_l^b 
\nonumber \\ &
+ \sum_{kcd} 
(
I_{ka}^{cd} t_k^c t_{ij}^{db} 
)
+ \sum_{klcd} I_{kl}^{cd} 
(
\frac{1}{4} t_k^a t_l^b t_{ij}^{cd} 
- \frac{1}{2} t_{ij}^{ac} t_{kl}^{bd}
)
\}
\nonumber \\ &
+ \textbf{P}(ij)
\{
- \sum_{kc} t_i^c t_{kj}^{ab} [F_1]_k^c
+ \sum_c I_{ab}^{cj} t_i^c
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_i^c t_j^d 
\nonumber \\ &
+ \sum_{klc}
( 
- I_{kl}^{ci} t_k^c t_{lj}^{ab}
)
+ \sum_{klcd} I_{kl}^{cd}
(
- \frac{1}{2} t_{ik}^{ab} t_{jl}^{cd}
)
\}
\nonumber \\ &
+ \textbf{P}(ab) \textbf{P}(ij)
\{
\sum_{kc}
(
I_{kb}^{cj} t_{ik}^{ac}
)
+ \sum_{kcd}
(
I_{ak}^{dc} t_i^d t_{jk}^{bc}
)
\nonumber \\ &
+ \sum_{klc}
(
\frac{1}{2} I_{kl}^{cj} t_i^c t_k^a t_l^b
- t_{jk}^{bc} t_l^a [W_3]_{ci}^{kl}
)
+ \sum_{klcd} I_{kl}^{cd}
(
\frac{1}{4} t_i^c t_k^a t_j^d t_l^b
+ \frac{1}{2} t_{ik}^{ac} t_{lj}^{db} 
)
\} .
\end{align}

\subsection{$[F_2]$}
We now combine the terms:

\begin{align}
& \sum_{k \not= i} f_{ki} t_{jk}^{ab}
- \sum_{k \not= j} f_{kj} t_{ik}^{ab} 
- \textbf{P}(ij) \sum_{kc} t_i^c t_{kj}^{ab} [F_1]_k^c
\nonumber \\ &
- \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} \frac{1}{2} t_{ik}^{ab} t_{jl}^{cd}
- \textbf{P}(ij) \sum_{klc} I_{kl}^{ci} t_k^c t_{lj}^{ab} .
\end{align}
We notice that the two terms $\sum_{k \not= i} f_{ki} t_{jk}^{ab}$ and $\sum_{k \not= j} f_{kj} t_{ik}^{ab}$ can be written in terms of $\textbf{P}(ij)$ and $\delta_{ki}$.

\begin{align}
\Rightarrow = &
\textbf{P}(ij) \left[
\sum_{k} (1 - \delta_{ki}) f_{ki} t_{jk}^{ab}
- \sum_{kc} t_i^c t_{kj}^{ab} [F_1]_k^c
- \sum_{klcd}  I_{kl}^{cd} \frac{1}{2} t_{ik}^{ab} t_{jl}^{cd}
- \sum_{klc} I_{kl}^{ci} t_k^c t_{lj}^{ab}
\right]
\nonumber \\
= &
\textbf{P}(ij) \left[
\sum_{k} (1 - \delta_{ki}) f_{ki} t_{jk}^{ab}
+ \sum_{kc} t_i^c t_{jk}^{ab} [F_1]_k^c
+ \sum_{klcd}  I_{kl}^{cd} \frac{1}{2} t_{jk}^{ab} t_{il}^{cd}
+ \sum_{klc} I_{kl}^{ic} t_l^c t_{jk}^{ab}
\right]
\nonumber \\
= &
\textbf{P}(ij) \sum_k t_{jk}^{ab} \left[
(1 - \delta_{ki}) f_{ki}
+ \sum_c t_i^c [F_1]_k^c
+ \frac{1}{2} \sum_{lcd} I_{kl}^{cd} t_{il}^{cd}
+ \sum_{lc} I_{kl}^{ic} t_l^c
\right]
\nonumber \\
= &
\textbf{P}(ij) \sum_k t_{jk}^{ab} [F_2]_i^k .
\end{align}

\begin{equation}
[F_2]_i^k = (1 - \delta_{ki}) f_{ki}
+ \sum_c \left[t_i^c [F_1]_k^c
+ \sum_{l} \left( I_{kl}^{ic} t_l^c 
+ \frac{1}{2} \sum_{d} I_{kl}^{cd} t_{il}^{cd}
\right)
\right] .
\label{intermedF2}
\end{equation}
This term scales as $n^5$.

\subsection{$[F_3]$}
We now combine terms within the $\textbf{P}(ab)$ operator.

\begin{align}
& - \sum_{c \not= b} f_{bc} t_{ij}^{ac}
+ \sum_{c \not= a} f_{ac} t_{ij}^{bc}
- \textbf{P}(ab) \sum_{kc} t_k^a t_{ij}^{bc} [F_1]_k^c
- \textbf{P}(ab) \sum_{klcd} \frac{1}{2} I_{kl}^{cd} t_{ij}^{ac} t_{kl}^{bd}
\nonumber \\ &
+ \textbf{P}(ab) \sum_{kcd} I_{ka}^{cd} t_k^c t_{ij}^{db} \nonumber \\
= &
\textbf{P}(ab) \left[
+ \sum_c (1-\delta_{ca}) f_{ac} t_{ij}^{bc}
- \sum_{kc} t_k^a t_{ij}^{bc} [F_1]_k^c
- \sum_{klcd} \frac{1}{2} I_{kl}^{cd} t_{ij}^{ac} t_{kl}^{bd}
+ \sum_{kcd} I_{ka}^{cd} t_k^c t_{ij}^{db} 
\right]
\nonumber \\
= &
\textbf{P}(ab) \left[
+ \sum_c (1-\delta_{ca}) f_{ac} t_{ij}^{bc}
- \sum_{kc} t_k^a t_{ij}^{bc} [F_1]_k^c
- \sum_{klcd} \frac{1}{2} I_{kl}^{cd} t_{ij}^{bc} t_{kl}^{ad}
+ \sum_{kcd} I_{ka}^{dc} t_k^d t_{ij}^{bc} 
\right]
\nonumber \\
= &
\textbf{P}(ab) \sum_c t_{ij}^{bc}
\left[
(1-\delta_{ca}) f_{ac}
+ \sum_{k} \left( - t_k^a [F_1]_k^c
+ \sum_{d} \left( I_{ka}^{dc} t_k^d 
- \sum_{l} \frac{1}{2} I_{kl}^{cd} t_{kl}^{ad} \right) \right)
\right] \nonumber \\
= &
\textbf{P}(ab) \sum_c t_{ij}^{bc} [F_3]_c^a .
\end{align}

\begin{equation}
[F_3]_c^a = (1-\delta_{ca}) f_{ac}
- \sum_{k} \left[ t_k^a [F_1]_k^c
+ \sum_{d} \left( I_{ka}^{cd} t_k^d 
- \frac{1}{2} \sum_{l} I_{kl}^{cd} t_{kl}^{ad} \right) \right] . \label{intermedF3}
\end{equation}
This term scales as $n^5$.

\subsection{$[W_4]$}
Now we combine the terms inside $\textbf{P}(ab) \textbf{P}(ij)$. 

\begin{align}
& \textbf{P}(ab) \textbf{P}(ij) \sum_{kc} I_{kb}^{cj} t_{ik}^{ac}
+ \textbf{P}(ab) \textbf{P}(ij) \sum_{kcd} I_{ak}^{dc} t_i^d t_{jk}^{bc}
\nonumber \\ &
- \textbf{P}(ab) \textbf{P}(ij) \sum_{klc} t_{jk}^{bc} t_l^a [W_3]_{ci}^{kl}
+ \textbf{P}(ab) \textbf{P}(ij) \sum_{klcd} I_{kl}^{cd} \frac{1}{2} t_{ik}^{ac} t_{lj}^{db}
\nonumber \\
= & \textbf{P}(ab) \textbf{P}(ij) t^{bc}_{jk} \left[
\sum_{kc} I_{ka}^{ci} 
+ \sum_{kcd} I_{ak}^{dc} t_i^d
- \sum_{klc} t_l^a [W_3]_{ci}^{kl}
+ \frac{1}{2} \sum_{klcd} I_{kl}^{cd} t_{lj}^{db}
\right] \nonumber \\ 
= &
\textbf{P}(ab) \textbf{P}(ij)  \sum_{kc} t^{bc}_{jk} \left[
I_{ak}^{ic} 
+ \sum_{d} I_{ak}^{dc} t_i^d
- \sum_{l} t_l^a [W_3]_{ci}^{kl}
+ \frac{1}{2} \sum_{ld} I_{kl}^{cd} t_{il}^{ad}
\right] .
\end{align}

\begin{equation}
[W_4]_{ic}^{ak} = 
I_{ak}^{ic} 
+ \sum_{d} I_{ak}^{dc} t_i^d
- \sum_{l} t_l^a [W_3]_{ci}^{kl}
+ \frac{1}{2} \sum_{ld} I_{kl}^{cd} t_{il}^{ad} .
\label{intermedW4}
\end{equation}
In this formulation symmetries where used extensively, the term scales as $n^6$.

\subsection{Inserting intermediates}
Inserting Eqs. \eqref{intermedF1}, \eqref{intermedF2}, \eqref{intermedF3}, \eqref{intermedW2}, \eqref{intermedW3}, \eqref{intermedW4} and \eqref{intermedw1} then leaves the following:

\begin{align}
D_{ij}^{ab} t_{ij}^{ab} = & 
I_{ab}^{ij} +
\frac{1}{2} \sum_{kl} (t_{kl}^{ab} + t_k^a t_l^b - t_l^a t_k^b) [W_1]_{ij}^{kl}
- \textbf{P}(ab) \sum_k t_k^b [W_2]_{ij}^{ak}
\nonumber \\ &
+ \textbf{P}(ij) \sum_k t_{jk}^{ab} [F_2]_i^k
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_{ij}^{cd}
+ \textbf{P}(ab) \sum_c t_{ij}^{bc} [F_3]_c^a
\nonumber \\ &
+ \textbf{P}(ij) \sum_c I_{ab}^{cj} t_i^c
+ \textbf{P}(ij) \frac{1}{2} \sum_{cd} I_{ab}^{cd} t_i^c t_j^d 
+ \textbf{P}(ab) \textbf{P}(ij) \sum_{kc} t_{jk}^{bc} [W_4]_{ic}^{ak} .
\end{align}
We can also define an intermediate $\tau_{ij}^{ab}$.

\begin{equation}
\tau_{ij}^{ab} = t_{ij}^{ab} + t_i^a t_j^b - t_j^a t_i^b . \label{intermedtau}
\end{equation}
This will reduce the number of calculations required, but not by any factor of $n$. Hence using this is a debate of speed versus memory. With this intermediate the equation for $\textbf{T}_2$ amplitudes look like this:

\begin{align}
D_{ij}^{ab} t_{ij}^{ab} = & 
I_{ab}^{ij} +
\frac{1}{2} \sum_{kl} \tau_{kl}^{ab} [W_1]_{ij}^{kl}
- \textbf{P}(ab) \sum_k t_k^b [W_2]_{ij}^{ak}
\nonumber \\ &
+ \textbf{P}(ij) \sum_k t_{jk}^{ab} [F_2]_i^k
+ \frac{1}{2} \sum_{cd} I_{ab}^{cd} \tau_{ij}^{cd}
+ \textbf{P}(ab) \sum_c t_{ij}^{bc} [F_3]_c^a
\nonumber \\ &
+ \textbf{P}(ij) \sum_c I_{ab}^{cj} t_i^c
+ \textbf{P}(ab) \textbf{P}(ij) \sum_{kc} t_{jk}^{bc} [W_4]_{ic}^{ak} . \label{LINK_THIS_SHIT_1_T2}
\end{align}

\subsection{Inserting into $t_i^a$}

If we combine the terms in the following manner

\begin{align}
D_i^a t_i^a = &
- \sum_{k} t_k^a
\left[
(1 - \delta_{ki}) f_{ki}
+ \sum_c t_i^c
f_{kc}
+ \sum_{lc} I_{kl}^{ic} t_l^c
+ \sum_{lcd} I_{kl}^{cd} (\frac{1}{2} t_{il}^{cd} + t_l^d)
\right]
\nonumber \\ &
+ f_{ai} 
+ \sum_{c \not= a} f_{ac} t_i^c
+ \sum_{kc} I_{ka}^{ci} t_k^c 
- \frac{1}{2} \sum_{klc} t_{kl}^{ca}
\left[
I_{kl}^{ci} + \sum_d I_{kl}^{cd} t_i^d
\right]
\nonumber \\ &
+ \sum_{kc} t_{ik}^{ac} 
\left[
f_{kc} + \sum_{ld} I_{kl}^{cd} t_l^d
\right]
+ \frac{1}{2} \sum_{kcd} I_{ka}^{cd} t_{ki}^{cd} 
+ \sum_{kcd} I_{ka}^{cd} t_k^c t_i^d 
 .
\end{align}
They reduce to the following:

\begin{align}
D_i^a t_i^a = &
- \sum_{k} t_k^a
[F_2]_i^k
+ f_{ai} 
+ \sum_{c \not= a} f_{ac} t_i^c
+ \sum_{kc} I_{ka}^{ci} t_k^c 
- \frac{1}{2} \sum_{klc} t_{kl}^{ca} [W_3]_{kl}^{ic}
\nonumber \\ &
+ \sum_{kc} t_{ik}^{ac} [F_1]_k^c
+ \frac{1}{2} \sum_{kcd} I_{ka}^{cd} t_{ki}^{cd} 
+ \sum_{kcd} I_{ka}^{cd} t_k^c t_i^d 
 .
\end{align}
This now scales as $n^6$. The largest scaling factor throughout our algorithm now is $n^6$ whereas prior to intermediates it was $n^8$. This is significantly faster, but we do have 8 (or 7 if $\tau_{ij}^{ab}$ is excluded) intermediates which must be stored. It should be noted that $t_k^c t_i^d = \frac{1}{2} (t_k^c t_i^d - t_i^c t_k^d)$. This can be used to insert $\tau_{ij}^{ab}$ in the equations for $t_i^a$ and the energy. 

\begin{align}
D_i^a t_i^a = &
- \sum_{k} t_k^a
[F_2]_i^k
+ f_{ai} 
+ \sum_{c \not= a} f_{ac} t_i^c
+ \sum_{kc} I_{ka}^{ci} t_k^c 
- \frac{1}{2} \sum_{klc} t_{kl}^{ca} [W_3]_{kl}^{ic}
\nonumber \\ &
+ \sum_{kc} t_{ik}^{ac} [F_1]_k^c
+ \frac{1}{2} \sum_{kcd} I_{ka}^{cd} \tau_{ki}^{cd} 
.  \label{LINK_THIS_SHIT_1_T1}
\end{align}

\section{SSLRS}

The science team of Scuseria, Scheiner, Lee, Rice and Schaefer are usually refered to as SSLRS. This team have developed some of the most efficient algorithms for our purposes, Ref.\cite{sslrs_citation2}. They have defined quite different intermediates, and I would like to also present their algorithm in short for comparison. If the reader want to develop their own CCSD algorithm this would be a good alternative. See also Ref.\cite{sslrs_citation1}.

\subsection{Description of algorithm}

In their algorithm the amplitude equations are defined with intermediates as the following:

\begin{align}
- D_i^a t_i^a = &
- f_{ai} 
- \sum_{c \not= a} f_{ac} t_i^c 
+ \sum_{k \not= i} f_{ki} t_k^a
+ \sum_{kc} f_{kc} (2t_{ik}^{ac} - \tau_{ik}^{ca})
+ \sum_k g_i^k t_k^a \label{SSRS1} \\ &
- \sum_c g_c^a t_i^c 
- \sum_{klc} \left( 2 [D_1]_{li}^{ck} - [D_1]_{ki}^{cl} \right) t_l^c t_k^a \nonumber \\ &
- \sum_{kc} [2(D_{2A} - D_{2B}) + D_{2C}]_{ci}^{ka} 
- 2 \sum_k [D_1]_{ik}^{ak}  \nonumber \\ &
+ \sum_{kc} v_{ic}^{ka} t_k^c
- \sum_{klc} v_{cl}^{ak} ( 2 t_{ki}^{cl} - t_{ik}^{cl} )
+ \sum_{ljc} v_{ci}^{jl} ( 2 t_{lj}^{ac} - t_{jl}^{ac} ) .
\nonumber
\end{align}

\begin{equation}
- D_{ij}^{ab} t_{ij}^{ab} = v_{ij}^{ab} + J_{ij}^{ab} + J_{ji}^{ba} + S_{ij}^{ab} + S_{ji}^{ba} . \label{SSRS2}
\end{equation}

\begin{equation}
E_{CCSD} = E_{HF} + 2\sum_{ia} f_{ia} t_a^i 
+ \sum_{abij} v_{ij}^{ab} ( 2 \tau_{ij}^{ab}
+ \tau_{ji}^{ab} ) . \label{SSRS3}
\end{equation}
The minus signs on the left side of the equation are present due to the fact that SSLRS use a different definition of the denominators, noted here in the equations are our definitions. Their definitions is equal to ours except for this minus sign. \\

$v_{ij}^{ab}$ is a short notation used by SSLRS for $\langle ab || ij \rangle$. It should be mentioned that the brackets surrounding for example $[D_1]$ is a notation where the brackets are a part of the variable. The rest of the intermediates are now defined.

\begin{align}
\tau_{ij}^{ab} = & t_{ij}^{ab} + t_i^a t_j^b
\\ 
J_{ij}^{ab} = &
\sum_{c \not= a} f_{ca} t_{ij}^{cb}
- \sum_{k \not= i} f_{ik} t_{kj}^{ab}
+ \sum_{kc} f_{kc} (t_{ij}^{cb} t_k^b + t_{ik}^{ab} t_j^c )
+ \sum_c g_c^b t_{ij}^{ac} - \sum_k g_j^k t_{ik}^{ab} .
\end{align}

\begin{align}
S_{ij}^{ab} = & 
\frac{1}{2} [B_2]_{ij}^{ab}
- [E_1^*]_{ij}^{ab}
+ [D_{2A}]_{ab}^{ij}
+ [F_{12}]_{ab}^{ij} 
 \\ &
+ \sum_{kc} (
[D_{2A}]_{kj}^{cb} - [D_{2B}]_{kj}^{cb}
+ 2[F_{12}]_{kj}^{cb}
- [E_1^*]_{cj}^{kb} )
( t_{ik}^{ac} - \frac{1}{2} t_{ki}^{ac} )
\nonumber \\ &
+ \sum_{kc} (\frac{1}{2} [D_{2C}]_{kj}^{cb}
- v_{kj}^{bc} - [F_{11}]_{kj}^{bc} + [E_{11}]_{kj}^{bc}) t_{ic}^{ak}
\nonumber \\ &
+ \sum_{kc} (\frac{1}{2} [D_{2C}]_{ki}^{cb} - v_{ki}^{bc} - [F_{11}]_{ki}^{bc} 
+ [E_{11}]_{ki}^{bc} ) t_{cj}^{ak}
\nonumber \\ &
+ \sum_{cd} ( \frac{1}{2} [D_2]_{ij}^{cd} 
+ \frac{1}{2} v_{ij}^{cd} + \frac{1}{2} (
[E_1]_{ij}^{cd} + [E_1]_{ji}^{dc}))
\tau_{cd}^{ab}
\nonumber \\ &
+ \sum_{kc} \{
([D_{2C}]_{ci}^{kb} - v_{ci}^{bk}) t_j^c
- ([D_{2A}]_{cj}^{kb} 
- [D_{2B}]_{cj}^{kb} ) t_i^c
- ([D_1]_{ji}^{bk} + [F_2]_{ji}^{bk}) \} t_k^a .
\nonumber
\end{align}
The definition of g depends on which index is where. Remembering $\textbf{a}$ is an index indicating an occupied orbital and $\textbf{i}$ is an index indicating a Fermi hole.

\begin{align}
g_i^a = & 
2 \sum_b [F_{11}]_{ib}^{ab} 
- \sum_b [F_{12}]_{ib}^{ba}
- \sum_b [D_{2A}]_{ib}^{ba}
+ \sum_{bc} (
[D_1]_{bc}^{ic} - 2[D_1]_{cb}^{ib} ) t_c^a
\\
g_a^i = &
\sum_c \{2([E_1]_{ic}^{ac} + [D_2]_{ic}^{ac} )
- ([E_1]_{ic}^{ca} + [D_2]_{ic}^{ca}) \} .
\end{align}
The rest of the intermediates in the SSLRS algorithm is now defined in order of appearance. 

\begin{equation}
[D_1]_{ij}^{ab} = \sum_k v_{ik}^{ab} t_j^k .
\end{equation}

\begin{equation}
[D_{2A}]_{ij}^{ab} = \sum_{kc}
v_{ci}^{ka} (2 t_{jc}^{ka} - t_{jc}^{ak}) .
\end{equation}

\begin{equation}
[D_{2B}]_{ij}^{ab} = \sum_{kc}
v_{ci}^{ak} t_{jc}^{ka} .
\end{equation}

\begin{equation}
[D_{2C}]_{ij}^{ab} = \sum_{kc}
v_{ci}^{ak} t_{jc}^{ak} .
\end{equation}

\begin{equation}
[B_2]_{ij}^{ab} = \sum_{kl} v_{kl}^{ab} t_{ij}^{kl} .
\end{equation}

\begin{equation}
[E_1^*]_{ij}^{ab} = \sum_k v_{ij}^{ak} t_k^b .
\end{equation}

\begin{equation}
[F_{12}]_{ij}^{ab} = \sum_c
v_{ic}^{ab} t_j^c .
\end{equation}

\begin{equation}
[F_{11}]_{ij}^{ab} = \sum_c v_{ic}^{ba} t_j^c .
\end{equation}

\begin{equation}
[E_{11}]_{ij}^{ab} \sum_c v_{ij}^{cb} t_c^a .
\end{equation}

\begin{equation}
[D_2]_{ij}^{ab} = \sum_{cd} v_{cd}^{ab} t_{ij}^{cd} .
\end{equation}

\begin{equation}
[E_1]_{ij}^{ab} = \sum_c v_{ic}^{ab} t_j^c .
\end{equation}

\begin{equation}
[F_2]_{ij}^{ab} = \sum_{cd} v_{cd}^{ab} \tau_{ij}^{cd} .
\end{equation}

\subsection{Scaling}
Specific notice should be paid to $[D_{2A}]$, $[D_{2B}]$, $[D_{2C}]$ and $[F_{2}]$. These intermediates scale as with a factor of $n^6$, where n is the number of orbitals. However it is not necessary to loop over all orbitals as index i,j,k refer to unoccupied orbitals. Indexes a,b,c refer to occupied orbitals. The scaling then reduces to $n_v^3 n_o^3$. Overall the Eq. \eqref{SSRS1}, \eqref{SSRS2} and \eqref{SSRS3} scales as 

\begin{equation}
\frac{1}{2} n_v^4 n_o^2 + 7 n_v^3 n_o^3 + \left( \frac{1}{2} + \frac{1}{2} \right) n_v^2 n_o^4 . 
\end{equation} 
It should be noted that SSLRS does propose this algorithm with the purpose of not only low scaling, but also avoid storage a large number of variables. In their papers they do mention another algorithm with slightly better scaling, but they do argue the extra needed variable storage this requires is a worrying aspect. For this reason the SSLRS algorithm is presented like this in this thesis.\\

Another positive aspect of this algorithm is the emphasis on matrix use. All the $n^6$ and $n^5$ scaling parts of the algorithm is designed so that matrix multiplication libraries can be used. These libraries are often specially designed to be efficient. \\

SSLRS algorithm also scales as $n^6$, but the number of intermediates are much larger. For this reason we chose the prior algorithm.

\section{TCE}
There are more than one way to factorize the CCSD equations. The best factorization actually depends on the system of interest. This has prompted the interest in the Tensor Contracted Engine, TCE. \\

TCE is an automated code generator for computational chemistry methods. Once the system of interest is known, the TCE aims to construct the optimal code. We will not go in detail on TCE, but additional information can be found in Ref.\cite{tce_citation_numbah_10}. 





\chapter{Comments Prior to Implementation}
In this chapter we discuss a few external libraries used in the implementation and how they work. Also we will discuss a few guiding principles we will apply to our implementation later. We will mainly discuss armadillo, MPI and general parallel programming. We will also mention OpenMP and external math libraries. The external math library we will use is Intel MKL.

\section{Armadillo}
Armadillo is a C++ linear algebra library. The library is designed to be similar to matlab in syntax. It provides good speed relative to other libraries and makes it easy to utilize matrix or vector multiplications in an efficient way. The armadillo documentation is available in Ref.\cite{armadillo-ref1}. Armadillo is also available for other programming languages, but we will strictly focus on the C++ version.

\subsection{Armadillo Types}
Armadillo has its own objects. We will use four objects in armadillo. The first three are vector, matrix and cube. These are simply put one, two and three dimensional arrays defined by standard to contain numerical values of double precision. The last is field. A field in armadillo is a two dimensional array that can contain other things than numerical values. A field can contain things like strings, vectors, matrices or cubes. Anything that can be used in combination with the "=" operator and a copy, like memcpy(). A field can be defined like this:

\begin{equation}
field<mat> A \nonumber
\end{equation}
This defines a two dimensional field of matrices, meaning a four dimensional array. The field is called "A". This can be accessed by first two indexes for the field and next two indexes for the matrix. A(0,1) for example is the matrix with indexes 0 and 1 in the field. A(0,1)(2,3) is a double precision number with indexes 2,3 in the matrix located in indexes 0,1 in the field A.

\subsection{Matrix Operations}
Matrix multiplication can be utilized very easy in armadillo. If there are three matrices defined, A, B and C, we can easily call on matrix multiplication by stating:

\begin{lstlisting}
C = A * B 
\end{lstlisting}
Other operations available are additions, subtractions, element wise multiplications and element wise divisions. These are accessed in order like this:

\begin{lstlisting}
C = A + B;
C = A - B;
C = A % B;
C = A / B;
\end{lstlisting}
Also there is a function called accu(C). This is an accumulation function that accumulates the values of C, where C can be a vector, matrix or a cube. If used in combination we can define

\begin{lstlisting}
D = accu(A % B)
\end{lstlisting}
This leaves D as a double precision number. Here A and B are first element wise multiplied and the resulting matrix accumulated. A and B must be same size. This will be used in our implementation. Take for example the term

\begin{equation}
D_{ij}^{ab} t_{ij}^{ab} \leftarrow \sum_{cd} I_{ab}^{cd} \tau_{ij}^{cd} . \label{armadilloterm}
\end{equation}

If we store I as a field with indexing I(a,b)(c,d) and $\tau_{ij}^{cd}$ is stored as a field with indexing $\tau$(i,j)(c,d) then we can use element wise multiplication and accumulation to calculate Eq. \eqref{armadilloterm}.

\begin{equation}
\sum_{cd} I_{ab}^{cd} \tau_{ij}^{cd}
= accu(I(a,b) \% \tau(i,j)) .
\end{equation}
Here I(a,b) is a matrix and $\tau$(i,j) is a matrix of same size. A major positive of armadillo is that it is possible to link other effective external math libraries to perform the actual matrix operations. We can link BLAS, LAPACK, OpenBLAS and many others. Armadillo initiates calls to these libraries automatic and effectively if installed properly. We then get the effectiveness of the best external math libraries, and the simplicity of the armadillo syntax.

\subsection{Element access}
An interesting feature when using armadillo is the way we access elements. In C syntax one usually allocates an array using malloc(). This gives great control over memory accessing, as we can have even multidimensional arrays sequential in memory. \\

In armadillo we usually allocate a matrix with just mat A. We can have a field of matrices with field<mat>. However each element in the field must be allocated on its own. Also armadillo has a few checks in place to ensure a bug free working code. Based on performance and experience, this is not efficient. \\

Other types in armadillo such at mat or vector is quite efficient. However when using this library it is important to be aware that not all types in armadillo are as efficient when it comes to memory access. Usually it is best to stick with one or two dimensional arrays as much as possible, even make temporary vectors or matrices to avoid accessing a field to much. Trial and error is thus a good tool if we want to use armadillo for high performance computing. This comment applies to armadillo at the time of this thesis. The armadillo developers are continuously working to improve the performance. \\

Another problem we encountered in our implementation is that OpenMPI does not take armadillo types in its communication functions. Armadillo does have functions that can help modestly in this regard, like the function .memptr(). However, we found it was not an optimal combination.

\section{Parallel Computing and OpenMPI}
The Open Source Message Passing Interface, OpenMPI, will be used in our implementation, Ref.\cite{openmpi_cite}. OpenMPI is a library that makes parallel computing much easier. It removed the need for low level parallel programming. In this section we will discuss briefly why we need parallel computing and what it is.

\subsection{The CPU \label{the_cpu_section}}
The CPU, or the Central Processing Unit, is the brain of the computer. This unit processes instructions, many of which requires transfer from or to the memory on a computer. \\

The CPU integrates many components, such as registres, FPUs and caches. The CPU has a "clock" that synchronizes the logic units within the CPU each clock cycle to precess instructions. Among other things this clock allows us to accurately measure the time used from one section of the program to another. \\

Instructions are put in a pipeline for the CPU to execute. While the CPU is processing instructions, it also looks down the pipeline, to see what instructions it will need to perform soon and what values it will need. These values can then pre-emptively placed in the cache. In the cache they are faster to access when they are needed. \\

If the CPU makes a wrong guess on for example an if test, the pipeline is filled with instructions that should not be processed and must be flushed. This slows down performance. \\

A supercomputer consists of nodes. Each node has a number of CPUs. On the abel super computing cluster, each node has 16 CPUs. 

\subsection{The Compiler}
The compiler allows the CPU to understand easy syntax such as C++. The compiler takes the code as input and produce the .o file. In the .o file there are instructions for the CPU to process. The CPU only understands the .o file, as such we must always compile our code. The compiler also creates the pipeline. We want the pipeline to be as optimal as possible, for this reason we need a good compiler. \\

A normal compiler performs a three step procedure. Step one is to check the code for syntax errors, include problems and other basics. \\

Step two is to translate the code into an intermediate language. Here optimizations are performed. If we wrote a code segment like this

\begin{lstlisting}
double A, B, C;
A = 50;
B = 20;
C = B * B * B;

// More calculations

B *= A;
\end{lstlisting}

The compiler will take note that the variable A is defined early on, and used much later. A is defined, stored into memory, then read from memory and finally it takes part in calculations. The compiler can rearrange our code to optimize this segment. 

\begin{lstlisting}
double B, C;
B = 20;
C = B * B * B;

// More calculations

double A = 50;
B *= A;
\end{lstlisting}

Here the variable A is defined and used directly. It is defined were we need it and ready in the pipeline for calculations. If the pipeline for some reason is filled with wrong instructions, we will not take advantage of optimizations such as this. \\

Step three is to output the .o file. Compilers are extremely complex, we should mention this was a brief and simplified description. 

\subsection{Data}
Data is stored in memory as a sequence of 0s and 1s. One 0 or 1 occupies one bit. 8 bits is one byte. The memory is read as bytes. Even a bool which can either be true or false is one byte. A bool of value true is stored in memory as 00000001. \\

Other types of data have different sizes in memory. An int is 4 bytes, a float 4 bytes and a double 8 bytes. Data is usually stored in memory, or rapid access memory, RAM. Here we can access it faster than from disk. \\

On a supercomputer, each node has a fixed number of memory available. The CPUs on the node can share this memory, or we can distribute it into smaller chunks were each CPU has its own unshared memory. 

\subsection{Bandwidth}
The bandwidth is a measure of number of bytes transferred per second. The bandwidth is a feature of the hardware, we will look at it as a constant value. We will be dealing with software. \\

If we want to send an array of 100 doubles from one computer to another, this will be 800 bytes. If we sent it as floats, it would be 400 bytes. If we assume the bandwidth is same, it would be twice as fast to transfer floats than doubles. \\

However, we will always use double precision values. But also in situations where we can reduce the size of the array, if it for example is symmetric. If we reduce the size by half, to 50 doubles or 400 bytes, we have saved much time in communication. \\

The communication inside a node is quite fast on a supercomputer. However when we need to use multiple nodes at once there are challenges. The nodes are not at the same physical distance to each other, this means we can not achieve the same bandwidth between different nodes. Abel supercomputer has nodes stacked in a rack. The nodes inside the same rack are closer. The bandwidth is usually higher in communication inside a rack, relative to communication between nodes in different racks. 

\subsection{Designing Parallel Algorithms}
When we design a parallel algorithm we look for hotspots. These are computation intense areas, and a parallel implementation should be designed to work good around this area. \\

However we must be careful, as communication can sometimes overtake computation. This can happen even in computation intense areas. A measure known as Granularity is known as the ratio of computation versus communication.

\subsection{Performance}
A serial algorithm is evaluated by its runtime. The runtime of a parallel program depends on input size , number of processors and the communication. This is a multidimensional problem, and not so easy to measure. \\

Sometimes we can use two different algorithms to solve the same problem. One algorithm may be more efficient in serial, while the other is more efficient in parallel. \\

To measure how good performance our parallel algorithm gives, it must be measured against the best serial algorithm for the given problem. This is true even if the best serial algorithm is in no way close to the same as the parallel algorithm. \\

One could imagine a serial program that solves a problem in 10 seconds, but is impossible to run in parallel. And we can imagine another algorithm that solves it in 100 seconds, but runs easily in parallel. If we run the second algorithm with 5 CPUs, and say this takes 20 seconds. It would still be better to use the first serial algorithm. The parallel performance can only be described as not good. This is true until you can run the second algorithm in less time than 10 seconds. \\

A good model of performance we will use is the Speedup, S.

\begin{equation}
S(p) = \frac{T_0}{T_p} .
\end{equation}
With ideal performance $S(p)$ is linear, preferably $S(p) = p$. As we noted in section \ref{the_cpu_section} values needed for calculation are pre-emptively placed in the catche. If a CPU cannot fit all values needed for calculations in the catche, the CPU must get these values from main memory. This slows down performance considerably. \\

We consider a large array we want to use in calculations. It is twice as large as the catche. If we introduce two CPUs, we can split the array in two and fit it in the catche. We will then avoid the performance loss from memory accessing. This creates the possibility of super linear scaling. This is a situation where we double the number of CPUs, and get more than a doubling in performance. Figure \ref{super_linear_scaling} is an illustration of different types of scaling.

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{scaling_plots_examples_1000.eps}
\caption{Illustration of possible scaling plots. Linear, Super linear and non linear is plotted.}
\label{super_linear_scaling}
\end{figure}

\subsection{Overhead}
If we try to solve a problem using two processor, it will normally not be twice as fast as it would be on one processor. This is because of overhead. Overhead are things like wasted computations, communication and latency. \\

Wasted computation would be additional computations required for running the algorithm in parallel. Latency is the time interval required to initiate a communication, and also to tell the processors that the communication is completed. \\

Runtime in a serial program is often denoted as $T_S$. The time from the first processor to start, until the last processor exits, is often noted a parallel runtime, $T_P$. The overhead, $T_O$, can then be described as

\begin{equation}
T_O = p T_P - T_S ,
\end{equation}
where p is the number of processors. Overhead is commonly increased as we increase the number of processors.

\subsection{General Parallel Guidelines}
For this implementation we will use a few simple guidelines with MPI. First we want to minimize the number of initiated communications. This is to reduce latency. When a communication is initiated, processors are syncronized. This means all processors enter into an MPI function at the same time. If one CPU is faster than another, this CPU will have to be idle and wait for the others to reach the communication function. This is undesireable. Also when exiting a communication, CPUs does not exit at the same time. This is another reason for minimizing the number of synchronizations. \\

Second we want to minimize the number of bytes to be communicated, mainly through symmetries. We want to design our algorithm specifically for this purpose. Third we want to use OpenMPI, which has optimized functions for communication implemented. In Appendix A we list many of these functions, with a short description of what they do.

\subsection{Optimizing Communication}

We will not go in detail on how the MPI functions are optimized. A good book on the subject is Ref. \cite{mpi_boka_cite_referanse}. We will only entertain a small example. \\

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{mpi_communication.jpg}
\caption{Illustration of two scenarios. Scenario one is a naive implementation of a broadcast. Scenario two is one example on how performance can be improved.}
\label{mpi_communication_illustration_thingy}
\end{figure}

This example is illustrated in figure \ref{mpi_communication_illustration_thingy}. Say we have four processors. We want to broadcast a message from processor 1 to all the others. We distinguish the first processor by its rank, it is rank 1. If rank 1 sends its message to rank 2, then rank 3 and then rank 4, there must be three communications performed by rank 1. One communication must wait for the other to finish in this example. \\

However, if rank 1 sends its message to rank 2. And then rank 1 sends to rank 3 at the same time as rank 2 sends to rank 4, there has only been two individual communication procedures by rank 1. This gives a better performance. \\

Each vertical line in figure \ref{mpi_communication_illustration_thingy} represents one send and recieve with MPI. A MPI\_Send and MPI\_Recieve scales as

\begin{equation}
t = t_s + m t_b .
\end{equation}
Here $t_s$ is the startup time, $t_b$ is the bandwidth and m is the number of bytes. In scenario one we would be performing (P-1) such send/recieves, where P is the number of MPI procs.

\begin{equation}
T_1 = (P-1) \times (t_s + m t_b) .
\end{equation}
In scenario two we still perform send/recieves, but since they can now be done simontaniously the number of send/recieves scales as $\lceil log_2(P) \rceil$.

\begin{equation}
T_2 = \lceil log_2(P) \rceil \times (t_s + m t_b) .
\end{equation}

Ideally we do not want the number of sends/recieves performed by one CPU to increase at all when we increase the number of processors. \\

\newpage

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{timeconsumption_scenario_one_vs_two.eps}
\caption{Illustration of communication in the two scenarios. Plotted are the number of send/recieves that must be performed.}
\label{mpi_communication_illustration_thingy2}
\end{figure}

We also perform performance tests of the actual performance on abel of the OpenMPI broadcast function. Results are presented in figure \ref{mpi_communication_real}. We notice there are indeed optimizations present from the non linear scaling. 

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{mpi_bcast_scaling.eps}
\caption{Illustration of actual communication with different number of CPUs. Time is measured for 100 Broadcasts with $8 \times 70^4$ bytes.}
\label{mpi_communication_real}
\end{figure}

\subsection{Optimizing Work Distribution \label{work_dist_section_1341}}
Also in parallel programming it is important that all processors get assigned the same workload. We think of the workload as a series of jobs that can be executed in parallel. \\

First we must define what is one job and second we must distribute these jobs among processors. Imagine running the following calculation in parallel.

\begin{equation}
K = \sum_{ijkl}^N X_{ij} Y_{kl} Z_{lk} .
\end{equation}
We first factorize it.

\begin{equation}
Z = \sum_{ij}^N X_{ij} \times \sum_{kl}^N Y_{kl} Z_{lk} .
\end{equation}
We will look at two possible definitions of one job in this scenario. First, we define a job by its job ID. This job ID can for example be expressed as a function of i and j. For example

\begin{equation}
job\_ID = i + j . \label{example_job_distribution}
\end{equation}
If we choose this definition we have $N \times N$ jobs to distribute. Alternatively we can define a job ID as a function of i, j, k and l. For example

\begin{equation}
job\_ID = i + j + k + l .
\end{equation}
We would then have $N^4$ jobs to distribute. If we have $N^4$ jobs, we can  use $N^4$ CPUs at max. If we however chose to prior job definition, we could only use $N^2$ CPUs. In general we want to have as many jobs as possible to distribute, but sometimes this can lead to additional communication. \\

Another important feature is to optimize the job distribution. If we chose one job\_ID to be expressed by i and j, we can visualize the job\_ID in a matrix. Each column is a different index i, and each row is an index j. The matrix elements are the job\_IDs. If we use N = 4 and Eq. \eqref{example_job_distribution} we would get

\begin{center}
\begin{tikzpicture}

        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            0 & 1 & 2 & 3 \\
            1 & 2 & 3 & 4 \\
            2 & 3 & 4 & 5 \\
            3 & 4 & 5 & 6 \\
        };  
    \end{tikzpicture}
\end{center}
We want all jobs to have a different ID. We redefine the job\_ID to be

\begin{equation}
job\_ID = i \times N + j .
\end{equation}
Using this definition our matrix of job\_IDs becomes

\begin{center}
\begin{tikzpicture}

        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            0 & 1 & 2 & 3 \\
            4 & 5 & 6 & 7 \\
            8 & 9 & 10 & 11 \\
            12 & 13 & 14 & 15 \\
        };  
    \end{tikzpicture}
\end{center}
Each matrix element represent a job\_ID. Here each job has got its unique ID, and it is easier to distribute. When we distribute work we must use the MPI rank and total number of MPI procs, p. For example we can define a condition for each processor that must be true if the processor are to perform the job.

\begin{lstlisting}
if (job_ID % p = rank){
   // Perform job
}
\end{lstlisting}
From the perspective of our CPUs we can use this relation to identify our job distribution. Noted now in the matrix is what processor performs which job. We assume we have p = 8.

\begin{center}
\begin{tikzpicture}

        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            0 & 1 & 2 & 3 \\
            4 & 5 & 6 & 7 \\
            0 & 1 & 2 & 3 \\
            4 & 5 & 6 & 7 \\
        };  
    \end{tikzpicture}
\end{center}
We see the job distribution is optimal, because the amount of work for all CPUs are identical. If we used the job\_ID defined in Eq. \eqref{example_job_distribution} the amount of work for each processor would not be the same. This is a sub-optimal work distribution. 

\subsection{Why Parallel}
Constructing a parallel program seems like quite the challenging feature. Every year there are new processors released with improved performance. Why do we not just wait for a great CPU that can solve all our problems? The reason we do not wait for this, is that it will never happen. The problem with great performance CPUs is that their power consumption is generally very high. A CPU with twice the performance generally needs 3 or 4 times the power, according to a lecture from Intel on parallel programming, Ref.\cite{intelduden_citeation}. It is therefore much more feasable to have several CPUs with less performance, than one high performance CPU. \\

So not only does parallel programming enable us to perform calculations faster and on larger systems, it also requires less power. Power consumption is the limiting factor in CPU performance today. Parallel programming is thus very important, and likely to become even more important in the future. On a sidenote, this is a reason why GPUs have become so popular in scientific programming, GPUs are optimized for performance per watt. Christoffer Hirth wrote extensively about this in his thesis, Ref.\cite{non_refer_numba1}. His principles has not been incorporated in our implementation, but is a likely source of further performance gains.

\section{OpenMP}
OpenMP is another library for parallel programming. It is developed by Intel and can be activated in most compilers. OpenMP use shared memory model. Here the main memory is available on all processors. The key word here is main memory, as each processor has its own cache. OpenMP is very easy to get started with. We will not be using it, but more information is available in Ref.\cite{openmp_citation_po_g}.

\section{External Math Libraries}
External Math Libraries are optimized for performance. They have built-in functions to handle matrix-matrix multiplications, vector-matrix multiplications, and similar problems. The best libraries are the likes of OpenBLAS, Ref.\cite{openblas_citation}, and Intel MKL, Ref.\cite{mkl_citation}. For our implementation we will make use of MKL on the Abel computing cluster. These libraries often give a huge performance gain in matrix operations, relative to a naive for-loop implementation. \\

We also mention that MKL comes with a parallel version, in where it makes use of OpenMP. Both these libraries are developed by Intel. For our purposes we only made use of the serial version. 











\chapter{Implementation}
In this chapter we will present the implementation of all the methods discussed in the previous chapters. We will implement them all in one program. We want to give our program a few positive features. First, we want people who know nothing about programming to be able to make use of our program. A background in chemistry is ideal for making use of quantum chemistry methods. However, a background in chemistry does not always include programming. For this reason we need a user friendly program. Second, we want it to be effective, both in serial and parallel. \\

The chapter is divided into six sections. First we look at how a program user can give input and run the program. Next we examine the programs general structure. The following four sections discuss the details of HF, AOtoMO, serial CCSD and parallel implementation of CCSD. All ideas, code and wise remarks in this chapter and on github is written from scratch by the author. The implementation is of course not the only one possible, for this reason we also include some citations to alternative implementations were appropriate. 

\section{Input File}
This section will deal with the user friendly part of our program. This means easy input. We must be able to define what method to use, what atoms and where they are placed and other input variables. We want these to be defined in a separate textfile, to ensure the user  never needs to recompile or edit any code. The input file must be named "INCAR". \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{inputfile100.eps}}
\caption{Example of input file for our program}
\label{fig:inputfile100}
\end{center}
\end{figure}

An example of input file is given in figure \ref{fig:inputfile100}. This is the only file we need to change the system or method in use. Our program uses the standard fstream library to read the textfile. We then go through it searching for keywords. The keywords are defined to be the leftmost word in each line. \\

Basis\_Set is the first keyword. Here we can choose from a variety of basis sets and the program will make use of this. The current options are STO-3G, 3-21G, 4-31G, 6-31G, 6-311ss, 6-311-2d2p and 6-311-3d3p. Most of basis sets are implemented for all atoms for which they are available. \\

The next keyword is Method. Here the choices are HF, CCSD, CCSDT-1a, CCSDT-1b, CCSDT-2, CCSDT-3, CCSDT-4 and CCSDT. The CCSDT part will be discussed in the next chapter. \\

convergence\_criteria is defined to be $10^{n}$, where n is given in the textfile. -8.0 gives a convergence criteria of $10^{-8}$. The same convergence criteria is used for all methods. \\

Relax\_Pos is meant to call a relaxation procedure, but this is not jet implemented in the program. \\

use\_angstrom gives the user the option to give atomic coordinates in angstrom, instead of atomic units. The options here are true or false. If it is set to true the coordinates are transformed to atomic units inside the program. \\

print\_stuffies is a variable that gives the user the option if he wants extra values printed. If this is set to true there are several interesting numbers printed during calculations. If this is set to false we only print the final energy. This option is added for a situation where we want to perform several hundred smaller calculation. A situation where we most likely are only interested in a final number.. \\

Freeze\_Core is an option available for CCSD. This is not jet implemented. \\

The next few lines give the atoms and its positions in x, y and z. The first letter is used to determine the number of electrons and nuclei charge. The program does not deal with ions. The simplicity of atom positions and charge is an advantage. Usually in computational chemistry packages the number of atom types and number of atoms of each type must be defined. Here we keep it simple and user friendly. \\

The input file stops searching for keywords once they are all found. Hence the user is free to put comments for self in the input file, as long as they are not placed next to keywords or inside the ATOMS section.

\section{General Code Overview}
In this section we describe the general overview of the code. We will present this as figures, and fill inn the blanks throughout the remaining sections. Each class will be described by its input, output and internal workings. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{structure.eps}}
\caption{Code structure}
\label{fig:structure}
\end{center}
\end{figure}

The first class in use is the initializer. This class takes the input from main and makes sure we use it correctly. If angstroms is used as units, the coordinates are transformed to atomic units. If we want extra print options, this is ensured here. We also define a Hartree Fock object in this class, since all methods in computational chemistry generally start with a HF calculation. \\

We then make sure the correct method is called, and pass the HF object. For this reason we drew an arrow from HF to initializer only in figure \ref{fig:structure}, since it is now passed as an object to the other methods. 


\section{Hartree Fock}
In this section we discuss the HF implementation in detail. Our Hartree Fock implementation is grounded in the class hartree\_fock\_solver. The main function is called Get\_Energy. In this function we will calculate the HF energy. The main outlay can be seen in figure \ref{fig:hfimp}. \\

\newpage

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{hf_imp.jpg}}
\caption{Basic Outlay of HF Implementation. First column is what action is done, second column is in what class this action takes place}
\label{fig:hfimp}
\end{center}
\end{figure}

The code is described in the text. We have also included key lines from the code itself to better illustrate the implementation. \\

\subsubsection{Filling numbers from EMSL}
\begin{lstlisting}
   // Set Matrix Sizes
   matrix_size_setter matset(Z, Basis_Set, n_Nuclei);
   Matrix_Size = matset.Set_Matrix_Size();

   // Fill numbers from EMSL
   Fill_Alpha Fyll(n_Nuclei, Z, Basis_Set,
Matrix_Size, matset.Return_Max_Bas_Func());
   alpha = Fyll.Fyll_Opp_Alpha();
   c = Fyll.Fyll_Opp_c();
   n_Basis = Fyll.Fyll_Opp_Nr_Basis_Functions();
   Number_Of_Orbitals = Fyll.Fyll_Opp_Antall_Orbitaler();
   Potenser = Fyll.Fyll_Opp_Potenser();
\end{lstlisting}

The first procedure performed in this function is to call the matrix\_size\_setter class. We make an object of this class and send the basis set in use and which atoms are in play. This class then returns how large our arrays must be. We then allocate these arrays. \\

The next step is going to the fill\_alpha class. This class contains data from EMSL, and fills up this data in arrays. The array alpha is filled with values for $\alpha_i$, the array c is filled with values of $c_i$ and Potenser is filled with the angular momentum. We also make a one dimensional array, Number\_Of\_Orbitals, which holds information on how many basis functions are in use for a specific atom. The array n\_Basis holds how many primitives each of these basis functions consist of. \\

\subsubsection{Normalizing GTOs}
\begin{lstlisting}
   // Normalize coefficients from EMSL
   Normalize_small_c();
\end{lstlisting}

The next step is to multiply in the normalization constant. This is multiplied in with the array c, through the function Normalize\_small\_c. In this function we have implemented the equations from section \ref{normalization_section}. \\

\subsubsection{Overlap Integrals}
The next step is to make an object of the class hartree\_integrals. Inside this class we will eventually calculate all the integrals we need. However the first step is to fill up an array of $E_t^{ij}$. These values are present in all our integrals. For this we use Eqs. \eqref{important_hf1}, \eqref{important_hf2} and \eqref{important_hf3}. The values for $E_t^{ij}$ will be calculated for all combinations of two primitive GTOS. This enables us to reuse the values in all our integrals, even the electron-electron repulsion. \\

\begin{lstlisting}
   // Precalculations
   HartInt.Fill_E_ij();

   // Overlap
   O = HartInt.Overlap_Matrix();
\end{lstlisting}

We then calculate the integrals. The overlap is stored in a matrix S and is calculated using Eq. \eqref{overlap_integral}. \\

\subsubsection{Kinetic Energy}
The two index integrals are stored in a matrix EK. EK consists of our kinetic energy and the nuclei-electron interaction. \\ 

\begin{lstlisting}
   // One electron operator
   EK = -0.5*HartInt.Kinetic_Energy()+
   - HartInt.Nuclei_Electron_Interaction();
\end{lstlisting}

The kinetic energy is calculated using Eq. \eqref{EKintegralsss}. \\

\subsubsection{Hermite Integrals}
For the nuclei-electron interaction we need to calculate the Hermite Integrals, $R_tuv^n$. We make a new function to calculate these called Set\_R\_ijk. This function implements the equations given in Eqs. \eqref{nucelec_0_int}, \eqref{nucelec_1_int}, \eqref{nucelec_2_int} and \eqref{nucelec_3_int}. We include the implementation of Eqs. \eqref{nucelec_0_int} and  \eqref{nucelec_1_int}. We put the values in a global four dimensional array R\_ijk.

\begin{lstlisting}
void Hartree_Integrals::Set_R_ijk(double p, int t, int u, int v, rowvec R1, rowvec R2)
{
   int t_max,nn,i,j,k,tt = t,uu = u,vv = v;
   t_max = t+u+v;
   double Boys_arg;
   rowvec Rcp(3);
   Rcp = R1-R2;
   Boys_arg = p*dot(Rcp, Rcp);
   Boys_arg = Boys(Boys_arg, 0);
   
   // Initialize R^n_0,0,0
   for (nn=0; nn<(t_max+1);nn++){
      R_ijk.at(nn)(0,0,0) = pow(-2*p, nn) *    F_Boys(nn);
   }

   // Fill up R^n_i,0,0
   for (i=0; i<tt; i++){
      for (nn=0; nn<(t_max-i); nn++){
         R_ijk.at(nn)(i+1,0,0) = Rcp(0) * R_ijk.at(nn+1)(i,0,0);
         if (i > 0){
            R_ijk.at(nn)(i+1,0,0) += i * R_ijk.at(nn+1)(i-1,0,0);
         }
      }
   }
   
   // Rest of Set_R_ijk function   
\end{lstlisting}
We here make use of the Boys function.

\subsubsection{Boys Function}
The function that calculates a value for the Boys function is called Boys. We have two equations we can use, Eq. \eqref{boys_int_1} and Eq. \eqref{boys_int_2}. One works for small x, the other for large x. We define everything less than x = 50 to be small, and everything greater or equal to 50 to be large. We include the implementation of large x. \\

\begin{lstlisting}
if (x > 50){
   Set_Boys_Start(N);
   F = Boys_Start / pow(2.0, N+1) * sqrt(M_PI/pow(x, 2*N+1));
}
\end{lstlisting}
For small x we Taylor expand around zero. We choose M = 100 in Eq. \eqref{boys_int_1} for the Taylor expansion. 

\begin{lstlisting}
else{
   double F=0, sum=0;
   int M;
   for (int j=0; j<100; j++){
      sum = pow(2*x, j);
      M = 2*N+1;
      while (M < (2*N+2+2*j)){
         sum /= M;
         M += 2;
      }
      F += sum;
   }
   F *= exp(-x);
}
\end{lstlisting}
We then use the recursive relation in Eq. \eqref{boys_int_3}. 

\begin{lstlisting}
F_Boys(N) = F;
for (int i = N; i > 0; i--){
   F_Boys(i-1) = (2*x*F_Boys(i) + exp(-x))/(2*i-1);
}
\end{lstlisting}
We are left with designing a value to N. N denotes the starting $F_n$ value, from which we will iterate down to the approximate solution. Popular here is putting N as some function of angular momentum, like $N = 6 \times l$. However we just put it to 30. In this value we were able to recreate all the benchmark values in Refs. \cite{boys_referanse_1} and \cite{boys_referanse_2}. These articles discuss the numerical calculation of the Boys function. Using N = 30 our results were also in agreement with the rest of the Computational Physics Group.

\subsubsection{Nuclei-Electron Interaction}
With these functions we can implement nuclei-electron interaction as given in Eq. \eqref{final_nuclei_electron_thang}. We add these into the array EK. 

\subsubsection{Electron-Electron Interaction}
The electron-electron repulsion integrals are stored in a four dimensional field, field\_Q. They are calculated through a function called Calc\_Integrals\_On\_The\_Fly. This function takes the input of four orbitals, i, j, k, l, and returns its value for $\langle i j | k l \rangle$. The function is an implementation of Eq. \eqref{electron_electron_int_1_1}, also using Eq. \eqref{electron_electron_int_1_2}. \\

\begin{lstlisting}
double Calc_Integrals_On_The_Fly(int orb1, int orb2, int orb3, int orb4)
{    
    int i,j,k,m;

    // Figure out what atom the AO belongs to, need atomic position
    i = Calc_Which_Atom_We_Are_Dealing_With(orb1);
    j = Calc_Which_Atom_We_Are_Dealing_With(orb3);
    k = Calc_Which_Atom_We_Are_Dealing_With(orb2);
    m = Calc_Which_Atom_We_Are_Dealing_With(orb4);

    // Here we calculate the two electron integrals
    // We have already stored E_ij^t so we reuse these
    // Symmetry considerations are applied elsewhere.

    int E_counter1, E_counter2; // These ensures we get the right E_ij^t
    int n,p,o,q; // Index for primitive GTO
    double temp = 0;
    E_counter1 = E_index(orb1,orb2);
    for (n=0; n<n_Basis(orb1); n++)
    {
        for (p=0; p<n_Basis(orb2); p++)
        {
            E_counter2 = E_index(orb3, orb4);
            for (o=0; o<n_Basis(orb3); o++)
            {
                for (q=0; q<n_Basis(orb4); q++)
                {
                    temp += c(orb1,n)*c(orb2,p)*c(orb3,o)*c(orb4,q)*
                            HartInt.Electron_Electron_Interaction_Single
                (orb1, orb3, orb2, orb4,
                  i, j, k, m, n, o, p,
                q, E_counter1, E_counter2);

                // E_t^ij is stored for x,y,z direction
            // Hence +3 on the counter
                    E_counter2 += 3;
                }
            }
            E_counter1 += 3;
        }
    }
    return temp; // temp is the value of <ij|kl>
}
\end{lstlisting}



We also take advantage of the eighfold symmetries, written our in Eqs. \eqref{interchangesym} and \eqref{interchangesym2}. We constructed the code like this originally to have the option to not store these integrals at all, and instead calculate them as needed. This would be a game changer in terms of what calculations are possible, since memory would now be scaling as $n^2$ instead of $n^4$. However we later decided on a memory distribution model was sufficient for our purposes, since we want to use coupled cluster. Coupled Cluster use more memory than HF, so the system size is restricted as is. 

\subsubsection{Parallel Implementation and Memory Distribution}
The hotspot in HF is the two electron integrals. We are not looking to make an optimized HF solver, but we must run this part of the calculation in parallel. \\

We want the workload of the integrals $\langle i j | k l \rangle$ distributed for a given index i and j. We only calculate one version of each symmetric term. \\

\begin{lstlisting}
    field_Q.set_size(Matrix_Size, Matrix_Size);
    for (int i = 0; i < Matrix_Size; i++)
    {
        for (int j = 0; j < Matrix_Size; j++)
        {
        // Leave parts of the field un-initialized
        // size = number of MPI procs
        // rank = my MPI rank
            if ((i+j)%size == rank)
            {
                field_Q(i,j) = zeros(Matrix_Size, Matrix_Size);
            }
        }
    }
\end{lstlisting}

The two electron integrals are a part of the Fock matrix calculation. We want to run this also in parallel, so ensure we can keep the integrals distributed in memory. We remember the Fock matrix was dependant upon 

\begin{equation}
\sum_{kl} \langle i j | k l \rangle ,
\end{equation}
and

\begin{equation}
\sum_{kl} \langle i l | k j \rangle .
\end{equation}
Because of this we define field\_Q to store the integrals as such

\begin{equation}
field\_Q(i,k)(j,l) = \langle i j | k l \rangle .
\end{equation}
We place the two indexes to be swapped in the matrix part of our armadillo field. We then store a $N^3$ sized array of temporary values, F\_temp(i,j,k). We then add the terms together in the correct order to produce $F_{ij}$. Here we can use functions like MPI\_Reduce, or make our own implementation of this function to produce the same result. \\

The important feature is that each processor only calculates terms based on the index i and k. This enables us to leave the indexes not in use in the field undefined, thus distributing the $N^4$ memory over all our P MPI processors in use (MPI procs). Each processor then only stores $\frac{N^4}{P}$ doubles. The amount of bytes for communication scales as $N^3$ doubles. \\

However we earlier calculated the integrals with a work distribution of indexes i and j. This work distribution makes it easier to use symmetries to avoid recalculation of symmetric terms. We therefore also introduce a communication procedure where we reshuffle the terms in field\_Q among the MPI procs. The amount of bytes for communication here is $\frac{1}{8} N^4$, and must be done using MPI\_Alltoallw or a similar implementation producing an identical result. Our HF implementation is not particularly optimized. Comments on we could optimize this implementation is available in the Future Prospects chapter. 

\subsubsection{Pre Iterative Steps}
The equation to solve in HF is the eigenvalue equation from Eq. \eqref{FOCK_EQUATION_STUFF}. To do this on a computer we must rewrite it slightly. The equation stands as

\begin{equation}
F C = S C \epsilon . \label{fdsaghbxcxd}
\end{equation}
We define a matrix V that satisfies

\begin{equation}
V^{\dag} S V = I , \label{fdsafafdsafdsafa}
\end{equation}
where I is the identity matrix. We insert $V^{\dag}$ to the left on both sides. Also $V V^{-1} $ is inserted into the equations. This leaves

\begin{equation}
V^{\dag} F V V^{-1} C = V^{\dag} S V V^{-1} C \epsilon .
\end{equation}
We also define 

\begin{equation}
F' = V^{\dag} F V ,
\end{equation}
and

\begin{equation}
C' = V^{-1} C .
\end{equation} 
We insert Eq. \eqref{fdsafafdsafdsafa}, F' and C' into Eq. \eqref{fdsaghbxcxd}.

\begin{equation}
F' C' = C' \epsilon .
\end{equation}
This is a true eigenvalue problem, where $\epsilon$ will be the eigenvalues of F' and C' will be the eigenfunctions. \\

We also define an intermediate P, which will be the electron density. 

\begin{equation}
P_{ij} = \sum_k^N C_i^k C_j^k ,
\end{equation}
where N is the number of electrons. We are now ready to begin an iterative procedure. This procedure will be different for RHF and UHF. 

\subsubsection{RHF Iterative Procedure}
For RHF we initially put the density P to be filled with zeroes. In RHF we will have an equal number of electrons with spin up and spin down. This simplifies our density matrix to

\begin{equation}
P_{ij} = \sum_k^{N/2} C_i^k C_j^k .
\end{equation}
We use Eq. \eqref{Fock_Restricted_1} to find the Fock matrix. We first insert P into the equation.

\begin{equation}
F_{ij} = (EK)_{ij} + \sum_{kl} P_{kl} (2 \langle i j | k l \rangle - \langle i l | k j \rangle) .
\end{equation}
We then perform the iterations until we reach self consistency. \\

\begin{algorithm}[H]
 \While{RHF\_continue = true}{
  Calculate $F$ \\
  $F' = V^{\dag} F V$ \\
  Solve $F' C' = C' \epsilon$ \\
  Compute $C = V C'$ \\
  Compute P \\
  \If{RHF = converged}{
    RHF\_continue = false
  }
 }
 \caption{Psudocode for RHF iterations}
 \label{RHF_ITERATIVE_PROCEDURE}
\end{algorithm}
After we have reached self consistency we calculate the energy.

\begin{lstlisting}
double Hartree_Fock_Solver::Calc_Energy()
{
    // Optimized RHF energy calculations
    Single_E_Energy = accu(EK % P);
    Two_E_Energy = 0.5*accu(Energy_Fock_Matrix % P) - 0.5*Single_E_Energy;
    return Single_E_Energy+Two_E_Energy;
}
\end{lstlisting}
Using armadillo the energy calculation simplifies to only two lines of code. 

\subsubsection{UHF Iterative Procedure}
For UHF we define two densities, $P^{\alpha}$ and $P^{\beta}$, which are the densities for spin up and down. 

\begin{equation}
P^{\alpha}_{ij} = \sum_k^{N_{\alpha}} C^{\alpha}_{ik} C^{\alpha}_{jk} .
\end{equation}

\begin{equation}
P^{\beta}_{ij} = \sum_k^{N_{\beta}} C^{\beta}_{ik} C^{\beta}_{jk} .
\end{equation}
Here $N_{\alpha}$ is the number of spin up particles, while $N_{\beta}$ is the number of spin down particles. These must be defined as input as must be equal to the total number of electrons in the system. We define the starting density to be random uniform numbers. We ensure the two matrices are not equal to each other for the first iteration. We use Eqs. \eqref{Fock_Restricted_2} and \eqref{Fock_Restricted_3} to find the Fock matrices. \\

\begin{algorithm}[H]
 \While{UHF\_continue = true}{
  Calculate $F_{\alpha}$ \\
  Calculate $F_{\beta}$ \\
  $F_{\alpha}' = V^{\dag} F_{\alpha} V$ \\
  $F_{\beta}' = V^{\dag} F_{\beta} V$ \\
  Solve $F_{\alpha}' C_{\alpha}' = C_{\alpha}' \epsilon_{\alpha}$ \\
  Solve $F_{\beta}' C_{\beta}' = C_{\beta}' \epsilon_{\beta}$ \\
  Compute $C_{\alpha} = V C_{\alpha}'$ \\
  Compute $C_{\beta} = V C_{\beta}'$ \\
  Compute $P_{\alpha}$ \\
  Compute $P_{\beta}$ \\
  \If{UHF = converged}{
    UHF\_continue = false
  }
 }
 \caption{Psudocode for UHF iterations}
 \label{UHF_ITERATIVE_PROCEDURE}
\end{algorithm}
After iterations we again calculate the energy. With armadillo the energy calculation simplifies to just two lines of code.

\begin{lstlisting}
double Hartree_Fock_Solver::Unrestricted_Energy()
{
    // Oprimized energy for UHF
    Single_E_Energy = accu((P_up + P_down) % EK);
    Two_E_Energy = 0.5 * accu(EnF_up % P_up) + 0.5 * accu(EnF_down % P_down) - 0.5 * Single_E_Energy;
    return Single_E_Energy + Two_E_Energy;
}
\end{lstlisting}


\subsubsection{Helping Convergence}
Sometimes our solution has problems converging. This is a numerical problem and we can introduce a few features to help the convergence along. \\

Damping is one option. This means updating the density only slightly, by inserting

\begin{equation}
P_{new}' = \gamma P_{old} + \left( 1 - \gamma \right) P_{new} .
\end{equation}
This reduce the change in density between iterations. We only used this in UHF. \\

A better alternative is the DIIS method, discussed in section \ref{diis_section_po_g}. We implemented this method for RHF. The first part of our DIIS implementation is calculating the error, $\Delta p$. 

\begin{lstlisting}
delta_p = F*P*O - O*P*F;
\end{lstlisting}

We then store the error and Fock matrices for the last M iterations. M is defined to M = 3 in our implementation. After this we construct the matrix B. 

\begin{lstlisting}
for (int i = 0; i < number_elements_DIIS; i++){
   for (int j = 0; j < number_elements_DIIS; j++){
      mat1 = Stored_Error.at(i);
      mat2 = Stored_Error.at(j);
      DIIS_B(i,j) = trace(mat1.t() * mat2);
   }
}
\end{lstlisting}

We then find the coefficients c.

\begin{lstlisting}
DIIS_c = solve(DIIS_B, DIIS_Z);
\end{lstlisting}

And finally we construct the new Fock matrix, as a linear combination of the previous Fock matrices. 

\begin{lstlisting}
F = DIIS_c.at(0) * Stored_F.at(0);
for (int i = 1; i < number_elements_DIIS; i++){
   F += DIIS_c.at(i) * Stored_F.at(i);
}
\end{lstlisting}

\newpage

\section{Atomic Orbital to Molecular Orbital}
Atomic Orbital (AO) to Molecular Orbital (MO) is required before we can do any CCSD calculations. In this section we describe how to implement this transformation. We are here looking for a highly optimized implementation. Some background is available in Ref.\cite{aotomo_1_cite}. However the author found the algorithms in the literature unsatisfactory. For this reason we will present a new algorithm. First, the simplest transformation is:

\begin{equation}
\langle ab | cd \rangle = \sum_{ijkl} C_i^a C_j^b C_k^c C_l^d \langle ij|kl \rangle .
\end{equation}
This scales as $n^8$ and can be factorized. 

\begin{equation}
\langle ab | cd \rangle = \sum_i C_i^a \sum_j C_j^b \sum_k C_k^c \sum_l C_l^d  \langle ij|kl \rangle .
\end{equation}
This is usually split into four quarter transformations. 

\begin{equation}
\langle aj|kl \rangle = \sum_{i} C_i^a \langle ij|kl \rangle .
\end{equation}

\begin{equation}
\langle ab|kl \rangle = \sum_{j} C_j^b \langle aj|kl \rangle .
\end{equation}

\begin{equation}
\langle ab|cl \rangle = \sum_{k} C_k^c \langle ab|kl \rangle .
\end{equation}

\begin{equation}
\langle ab|cd \rangle = \sum_{l} C_l^d \langle ab|cl \rangle .
\end{equation}
Each of these quarter transformations scale as $n^5$. The implementation of this must be done in an effective way in terms of speed and memory. The latter is the most important as the memory here scales as $N^4$, where N is the number of contracted GTOs, for both $\langle ij | kl\rangle$, $\langle ab | cd \rangle$ and also the intermediates in between each quarter transformation. \\

\begin{algorithm}[H]
 \KwData{Psudo Code}
 \KwResult{Algorithm for parallel AOtoMO transformation }
 \For{a=0; a<N}{
  \For{k=0; k<N}{
   \For{l=0; l<N}{
    \If{Grid k and l over threads}{
     \For{j=0; j<N}{
      \For{i=0; i<N}{
       $QT1(k,l,j) += 
       C_i^a \times \langle ij|kl \rangle$
      }
     }
     \For{j=0; j<N}{
      \For{b=0; b<N}{
       $QT2(k,l,b) += C_j^b \times QT1(k,l,j)$
       }
      }
    }
   }
  }
  Communicate $QT2(k,l,b)$
  
  \For{b=0; b<N}{
   \For{c=0; c<N}{
    \If{Grid b and c over threads}{
     \For{k=0; k<N}{
      \For{l=0; l<N}{  
       $QT3(b,c,l) = C_k^c \times QT2(k,l,b)$
       }
      }
      \For{l=0; l<N}{
        \For{d=0; d<N}{
       $QT4(b,c,d) = C_l^d \times QT3(b,c,l)$
        }
      }
     }
    }
   }
   
   Communicate $QT4(b,c,d)$\\
   
   \If{Store distributed MOs to given thread}{
    $\langle ab|cd \rangle = QT4(b,c,d)$
   }
 }
  
 \caption{Simple Psudocode for parallel AOtoMO transformation. QT1, QT2, QT3 and QT4 are intermediates}
 \label{aotomotrans}
\end{algorithm}

Algorithm \ref{aotomotrans} is a description of how we optimize this implementation. Further optimizations will come later, but first an illustration of the general idea. We first hold index $a$ constant throughout the transformation. This enables us to use $N^3$ size intermediates. \\

Second the grid over k and l is chosen because neither of these are involved as an index in $C$ for the first two quarter transformations. This makes sure that the terms of $QT2(k,l,b)$ calculated by each thread is the fully two quarter transformed term. This avoids the use of MPI\_Reduce or similar operations and means only one thread needs to communicate these two quarter transformed terms with specific $k$ and $l$, minimizing the communication. The total amount of double precision values communicated in the first communication for now is $N^3$ for each $a$, making it $N^4$ in total for all $a$. \\

After the first communication each thread has all terms in $QT2(k,l,b)$ available. We then make a new grid over $b$ and $c$ and continue calculations in parallel. The grid could be made over $a$ and $b$, but the prior makes in general a better work distribution. This is because index $a$ is held fixed. After the fourth quarter transformation each thread has the fully transformed MOs available for certain $b$ and $c$ indexes. \\

At this point we can distribute the MOs in the same grid as for $b$ and $c$, and start CCSD calculations. However because we want to have the distribution optimized for CCSD we implement another communication. This communication is $N^3$ for each $a$, making it $N^4$ in total for all $a$. \\

After the second communication we simply store the MOs in a memory distributed manner. It is also possible to write to disk. \\

$QT2$ and $QT4$ must be stored as one dimensional arrays, to minimize the number of communication procedures initiated, hence minimize latency. Also all multiplications are written using external math libraries through armadillo. We should also introduce symmetries to optimize our calculations further. The starting AOs had eight-fold symmetries. So does the resulting MOs. However these symmetries does not hold at all the quarter transformed intermediates. This complicates things slightly. \\

The second quarter transformed four dimensional array, QT2, will have symmetries in the two untouched indexes, as well as in the two transformed indexes. We were able to make use of this to reduce communication by 75\%, since symmetric terms need not be communicated twice. This also holds true at the QT4 level obviously. The algorithm using symmetries and external math libraries is presented in algorithm \ref{aotomo2}. \\

\begin{algorithm}
 \KwData{Psudo Code}
 \KwResult{Effective Algorithm for parallel AOtoMO transformation using external math libraries}
 \For{a=0; a<N}{
  \For{k=0; k<N}{
   \For{l=0; l<=k}{
    \If{Calculate on local thread}{
     A1(*) = C(a,*) $\times$ $\langle kl | ** \rangle$ \\
     A2($0 \rightarrow a$) = C($0 \rightarrow a$, *) $\times$ A1 \\
     \For{b=0; b<=a}{
       QT2(b,k,l) = A2(b) 
      }
    }
   }
  }
  
  MPI\_Allgatherv(QT2) \\
  
  \For{b=0; b<=a}{
   \For{c=0; c<N}{
    \If{Calculate on local thread}{
 	 A1(*) = C(c,*) $\times$
 	  QT2(b,*,*) \\
     A2($0 \rightarrow c$) = C($0 \rightarrow c$, *) $\times$ A1 \\
     \For{d=0; d<=c}{
       QT4(b,c,d) = A2(d) 
      }
     }
    }
   }
   MPI\_Allgatherv(QT4) \\
   
   \If{Store distributed MOs to given thread}{
    $\langle ab|cd \rangle = QT4(b,c,d)$ \\
    or write to disk
   }
 }
  
 \caption{Psudocode for parallel AO to MO transformation using armadillo. A1 and A2 are one dimensional intermediates}
 \label{aotomo2}
\end{algorithm}

The communication is somewhat tricky in this algorithm. Since we have inserted symmetries, the size of the message to be transmitted changes dependant upon the index $a$. This also applies to the displacement. We therefore store both in two dimensional arrays where $a$ is the outer index, the inner is the MPI rank. \\

We run through the algorithm one time in advance to calculate these variables. We also calculate and store where each processor will start calculations. This is done to remove any pipeline flushes, which can be caused by the CPU wrongly guessing the answer of an if test. \\

For this reason we define another two dimensional array, this one of size N times the number of MPI procs. In the first two quarter transformation, each rank here stored at what index $l$ will calculations start for a given index $k$. The next $l$ the same rank will perform calculations on will then be 

\begin{equation}
l \rightarrow l + p ,
\end{equation}
where p is the number of MPI procs. The exact same procedure is repeated for quarter transformation 3 and 4. \\

We have also in the more advanced algorithm inserted one dimensional arrays A1 and A2. Using these provide more optimize ways of accessing memory. It may at first sight seem like an additional complication to first calculate A2 as a one dimensional array and later store it in QT2, but this is a more efficient way when using armadillo. \\

The algorithm is implemented in the function 

\begin{lstlisting}
void Prepear_AOs(int nr_freeze);
\end{lstlisting}
The argument is how many core orbitals to freeze. The argument is somewhat wasted, since frozen core approximation is not implemented yet.

\section{CCSD Serial Implementation \label{optimize_serial_version_bii}}
Our CCSD implementation is quite large, actually close to 10 000 lines. However this is small compared to other optimized implementations, which are usually around 40 000 lines of code. Implementation is important in CCSD, since it scales quickly for larger systems. Additional information on on the advancement of CCSD is available in a series of books, Ref.\cite{book_om_advancements_ccsd}. The most effective implementation to the authors knowledge is the Cyclic Tensor Framework, see Refs.\cite{most_effective_ccsd_dude}, \cite{most_effective_ccsd_dude2} and \cite{most_effective_ccsd_dude3}. \\

In this thesis we will present a simple and effective implementation of CCSD in parallel. First we look at a simple serial implementation. This section discusses the serial implementation. There are two specific goals for this implementation. First getting the energy in the smallest amount of time, second being able to run larger systems. \\

Even more precise we can state that our goals are: \\
a) Never get zero in a multiplication\\
b) All multiplication should be done by external math libraries\\
c) Do not store anything more than needed\\

We first present the general structure of the code. Later we will discuss a few details about different optimizations we have implemented. These will be contrasted to what kind of optimizations is commonly implemented in CCSD. Then, there will be a pros and cons list for our implementation. The chapter will be quite technical as there are several considerations behind each optimization, and it all works in combination. 

\newpage

\subsection{Structure}

For our serial program we first define arrays to store all intermediates, MOs and amplitudes. Two arrays are defined for each amplitude, one for the old amplitudes and one for the new amplitudes. We define a convergence criteria, which stops iterations once the difference of energy from one iteration to the next is bellow this criteria. \\

\begin{algorithm}[H]
 \KwData{Psudo Code}
 \KwResult{Structure of CCSD serial program}
 \While{CCSD continue = true}{
  Set Eold = Enew \\
  Calc F1 \\
  Calc F2 \\
  Calc F3 \\
  Calc W1 \\
  Calc W2 \\
  Calc W3 \\
  Calc W4 \\
  Calc New t1 amplitudes \\
  Calc New t2 amplitudes \\
  Set t1 = t1new \\
  Set t2 = t2new \\
  Calc $\tau_{ij}^{ab}$ \\
  Calc New Energy \\
  \If{Enew - Eold < Convergence criteria}{
  	CCSD continue = false
  }
 }
 \caption{Psudocode for our serial CCSD program}
 \label{CCSD_STRUCTURE_SERIAL}
\end{algorithm}

Algorithm \ref{CCSD_STRUCTURE_SERIAL} illustrates the algorithm as psudocode. Each of the terms behind "Calc" is taken as a separate function to make the code easily readable. 

\subsection{Removing redundant zeroes \label{compact_storage}}
We now briefly reconsider the molecular integrals, which were calculated as such

\begin{equation}
\langle pq|rs \rangle = 
\sum_{\alpha \beta \xi \nu} C_{\alpha}^p C_{\beta}^q C_{\xi}^r C_{\nu}^s \langle \alpha \beta | \xi \nu \rangle .
\end{equation}
Here $\langle \alpha \beta | \xi \nu \rangle$ are our atomic orbitals (AOs). These come from our RHF calculations. $\langle pq|rs \rangle$ are the molecular orbitals (MOs). MOs here are presented as a linear combination of AOs. The MOs appear in CCSD as a double bar integral. This is defined as such

\begin{equation}
\langle pq||rs \rangle = \langle pq | rs \rangle
- \langle pq | sr \rangle  .
\end{equation}
Due to spin considerations, if we fill a matrix with $\langle pq||rs \rangle$ it will be filled with mostly zeroes. However when using an RHF based CCSD it is common that all even numbered spin orbitals have the same spin orientation. This means all odd numbered orbitals will also have the same spin orientation. This results in the zeroes forming pattern that we have identified and utilized. \\

$\langle pq || rs \rangle$ are diagonal in total spin projection. In RHF the total spin is also equal to zero. When we have all odd numbered orbitals with the same spin orientation, and same with even numbered orbitals, this has a practical implication. The implication is that the only terms that will not be equal to zero are those where the sum of the orbital indexes are equal to an even number. \\

We will now visualize this. We construct a program that performs the AO to MO transformation and print $\langle pq||rs \rangle$ for a fixed $p=1$ and $r=1$. In the span of $q$ and $s$ there is formed a matrix, we have noted the terms that will be zero and also the terms that will be non-zero with the indexes (q, s).

\[ \left( \begin{array}{ccccccc}
(0,0) & 0 & (0,2) & 0 & (0,4) & 0 & \dots \\
0 & (1,1) & 0 & (1,3) & 0 & (1,5) & \dots \\
(2,0) & 0 & (2,2) & 0 & (2,4) & 0 & \dots \\
0 & (3,1) & 0 & (3,3) & 0 & (3,5) & \dots \\
(4,0) & 0 & (4,2) & 0 & (4,4) & 0 & \dots \\
0 & (5,1) & 0 & (5,3) & 0 & (5,5) & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots & \dots \end{array} \right)\]

This array is now stored in our computer as a four dimensional array that we call $I[a][b][c][d]$. We on purpose use a different index in the array than we do for the orbital, even though index a in this example is a referance to orbital p. We can note which orbitals our array-indexes refers to as such\\
a = p \\
b = r \\
c = q \\
d = s \\

This will be an array of size $(2N)^4$, where $N$ is the number of contraction Gaussian Type Orbitals (GTOs). We now perform a trick. We want our indexes of I to refer to a different orbital, in practise we want:\\
a = p\\
b = r\\
c = q/2 + (q\% 2) N\\
d = s/2 + (s\% 2) N\\

Where \% is the binary operator and we use integer division by 2. The number 2 comes from two spin orbitals per spacial orbital. Now index c is no longer a referance to orbital q, but a referance to orbital [q/2 + (q \% 2) N]. If we now visualize the same double bar integral with fixed $p=1$ and $r=1$ it looks like this

\[ \left( \begin{array}{cccccccc}
(0,0) & (0,2) & (0,4) & \dots & 0 & 0 & 0 & \dots \\
(2,0) & (2,2) & (2,4) & \dots & 0 & 0 & 0 & \dots\\
(4,0) & (4,2) & (4,4) & \dots & 0 & 0 & 0 & \dots\\
\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
(1,1) & (1,3) & (1,5) & \dots & 0 & 0 & 0 & \dots\\
(3,1) & (3,3) & (3,5) & \dots & 0 & 0 & 0 & \dots\\
(5,1) & (5,3) & (5,5) & \dots & 0 & 0 & 0 & \dots\\
\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots \end{array} \right)\]

Performing this trick will always result in a matrix that looks something like this. We can split this matrix into four sub-matrices, one top left, one top right, one bottom left and one bottom right. Regardless of $a$ and $b$, we will always have either the two left sub-matrices, or the two right sub-matrices always filled with zeroes. These do not need to be stored. If we ensure we \emph{only perform calculations on orbitals with a non-zero contribution} we can change our array-indexing to:\\
a = p\\
b = r\\
c = q/2 + (q\% 2) N\\
d = s/2 \\

This means for two orbital where s = 2 and s = 3 we will have the same d value. However one of these orbitals will always be zero, so if we avoid doing calculations on this there will be no problems. And we also reduce the size of the array to half. Visualizing now the same array it looks like this.
\[ \left( \begin{array}{cccc}
(0,0) & (0,2) & (0,4) & \dots \\
(2,0) & (2,2) & (2,4) & \dots \\
(4,0) & (4,2) & (4,4) & \dots \\
\dots & \dots & \dots & \dots \\
(1,1) & (1,3) & (1,5) & \dots \\
(3,1) & (3,3) & (3,5) & \dots \\
(5,1) & (5,3) & (5,5) & \dots \\
\dots & \dots & \dots & \dots  \end{array} \right)\]

And its size will be $\frac{1}{2} (2N)^2$. This kind of indexing can and should be performed on $\textbf{all}$ stored integrals, amplitudes and intermediates. This ensures all memory is reduced by at least 50 \%. Also if a,b,c,d is referencing orbitals in the same manner in all stored arrays we can still use external math libraries as before. However now we will not be passing any zeroes into these external math libraries, so calculations can be faster. This change in indexing keeps all symmetries and also allow easy row and column access. The row is accessed as usual.

\begin{tikzpicture}
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            (0,0) &(0,2) &(0,4) &\dots \\
            (2,0) & (2,2) & (2,4) & \dots \\
            (4,0) & (4,2) & (4,4) & \dots \\
            \dots & \dots & \dots & \dots \\
            (1,1) & (1,3) & (1,5) & \dots \\
			(3,1) & (3,3) & (3,5) & \dots \\
			(5,1) & (5,3) & (5,5) & \dots \\
			\dots & \dots & \dots & \dots \\
        };  
        \draw[color=red] (m-1-1.north west) -- (m-1-3.north east) -- (m-1-4.north east) -- (m-1-4.south east) -- (m-1-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}

A column is slightly different, since we only require either the top half or the bottom half of the matrix.

\begin{tikzpicture}
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            (0,0) &(0,2) &(0,4) &\dots \\
            (2,0) & (2,2) & (2,4) & \dots \\
            (4,0) & (4,2) & (4,4) & \dots \\
            \dots & \dots & \dots & \dots \\
            (1,1) & (1,3) & (1,5) & \dots \\
			(3,1) & (3,3) & (3,5) & \dots \\
			(5,1) & (5,3) & (5,5) & \dots \\
			\dots & \dots & \dots & \dots \\
        };  
        \draw[color=red] (m-1-2.north west) -- (m-3-2.south west) -- (m-4-2.south west) -- (m-4-2.south east) -- (m-3-2.north east) -- (m-1-2.north east) -- (m-1-2.north west);
\end{tikzpicture}

This can be specified using the submatrix command in armadillo. The attributes of the double bar integrals in RHF are such that there will be some remaining zeroes after removing these. This is because total spin must be zero. However there will be another pattern formed where all remaining zeroes are placed in one of two remaining sub-matrices. This can also be accounted for, reducing memory needs by an additional $\frac{1}{8}$. These calculations can also easily be avoided using submatrix calls in armadillo.\\

Since we are using an RHF basis some matrix elements become independent of spin. This means spin up will have the same value as spin down. This happens mostly for the two dimensional intermediates, and when we store in this manner the practical implications of this becomes identical upper and lower submatrices. In this situation we do not calculate the same term twice. In the future we will refer to this method as compact storage. This method removes all zeroes without defining additional arrays, as is usually done today. 

\subsection{Pre Iterative Calculations}
Before calculations can start we perform a few tricks. We do not store our MOs in one gigantic array, instead we split it up into several smaller ones. This is done at the end of the AOtoMO transforamtion. These are variables such as MO3 and MO4, decleared in the header file ccsd\_memory\_optimized.h.

\begin{lstlisting}
field<mat> MO3, MO4 ...
\end{lstlisting}
This is a very common procedure in CCSD implementation. It is performed to enable more effectively use of external math libraries. The reason it is more effective is because of memory accessing. If we want to send parts of an array into an external math library, we need first to extract which parts to send. Instead, we can define several smaller arrays like MO3. MO3 is then designed specifically to be sent directly into the external math library. \\

Ref.\cite{ccsd_fac3} also ponders this. In fact, this is such an optimization that even redundant storage of double bar integrals is often used. This means storing a value twice, just to have it easily available for passing to external math library. \\

Originally, we took advantage of this straight forward optimization. However, in the current implementation we will not be storing redundant values. Instead, we will be storing the single bar integrals, and have functions to map these into two dimensional arrays of double bar integrals ready for external math library use. We have surgically designed each and every for loop such that this mapping is redundant in terms of program efficiency. It does reduce memory requirements drastically. \\

The splitting of the integrals for us then becomes somewhat redundant in this regard, but as we will see later it is of clinical importance when we implement memory distribution. \\

Before iterations can start we also allocate memory for our intermediates and amplitudes. We use the principles of section \ref{compact_storage} here. In fact every array to come in contact with an external math library must be stored on this form.

\subsection{F1, F2 and F3}
We now start the iterative procedure. The first three intermediates are two dimensional and
very straight forward to calculate. We use Eqs. \eqref{intermedF1}, \eqref{intermedF2} and \eqref{intermedF3}. Our implementation has some additional complications which will be discussed shortly, but is still equivalent to our initial naive implementation.

\begin{lstlisting}
for (int a = 0; a < unocc_orb; a++){
  for (int m = 0; m < n_Electrons; m++){
     F1(a/2, m/2) = accu(integ2(a,m) % T_1);
  }
}
\end{lstlisting}

In this initial naive implementation integ2 is one part of the double bar integrals we pulled out. Since we want to store only the single bar integrals, we replace this with a function Fill\_integ\_2\_2D(int a, int m) to fill up a global mat integ2\_2D. This is then used in external math libraries.

\begin{lstlisting}
for (int a = 0; a < unocc_orb; a++){
  for (int m = 0; m < n_Electrons; m++){
     Fill_integ_2_2D(a, m);
     F1(a/2, m/2) = accu(integ2_2D % T_1);
  }
}
\end{lstlisting}

$[F_2]$ and $[F_3]$ are calculated in a similar procedure.

\subsection{W1, W2, W3 and W4}
These intermediates are calculated using Eqs. \eqref{intermedw1}, \eqref{intermedW2}, \eqref{intermedW3} and \eqref{intermedW4}. We include the initial naive implementation of $[W_1]$.

\begin{lstlisting}
for (int i = 0; i < n_Electrons; i++){
   for (int j = i+1; j < n_Electrons; j++){
      Fill_integ8_2D;
      W_1(i,j)(k/2,l/2) = integ8_2D;
   }
}

for (int k = 0; k < n_Electrons; k++) {
   for (int l = 0; l < n_Electrons; l++){      
      Fill_integ6_2D(k,l);
      Fill_integ4_2D(k,l);
      for (int i = 0; i < n_Electrons; i++){
         for (int j = i+1; j < n_Electrons; j++){
            W_1(i,j)(k/2,l/2) += accu(integ6_2D.col(j)
                % T_1.col(i));
            W_1(i,j)(k/2,l/2) -= accu(integ6_2D.col(i)
                % T_1.col(j));
            W_1(i,j)(k/2,l/2) += 0.5*accu(integ4_2D
                % tau3.at(i,j));
         }
      }
   }
}
\end{lstlisting}

Here the mapping into two dimensional double bar integrals is still done as a $N^4$ procedure, whereas the calculation is now an $N^6$ procedure. For easy external math library use later we store these variables as W1(i,j)(k,l), W2(i,j)(a,m), W3(i,m)(e,n) and W4(a,i)(c,k). We also use symmetries where they can be applied.

\subsection{New amplitudes}
The T1 amplitudes are calculated using Eq. \eqref{LINK_THIS_SHIT_1_T1}. For the T2 amplitudes we use Eq. \eqref{LINK_THIS_SHIT_1_T2}. To make this amplitude most optimal for external math libraries we store it as

\begin{equation}
T2(a,i)(b,j) . \label{howtostoret2}
\end{equation}

\subsection{$\tau_{ij}^{ab}$ and Energy}
$\tau_{ij}^{ab}$ is calculated in Eq. \eqref{intermedtau}. It is stored in a variable tau3(a,b)(i,j) for optimal use in external math libraries. The energy can be calculated using Eq. \eqref{CCSD_TOTAL_ENERGY}. However we can simplify this further by introducing $\tau_{ij}^{ab}$.

\begin{equation}
E_{CCSD} = E_0 + \sum_{ai} f_{ai} t_i^a + \frac{1}{4} \sum_{abij} \langle ij || ab \rangle \tau_{ij}^{ab} .
\end{equation}
Here we have inserted $t_i^a t_j^b = \frac{1}{2} \left( t_i^a t_j^b - t_j^a t_i^b \right)$. Also the term $f_{ai}$ will always be equal to zero when the basis for our CCSD calculations are a diagonalized Fock matrix.

\begin{equation}
E_{CCSD} = E_0 + \frac{1}{4} \sum_{abij} \langle ij || ab \rangle \tau_{ij}^{ab} .
\end{equation}

\subsection{Dodging Additional Unnecessary Calculations}
In section \ref{compact_storage} we discussed how to avoid multiplication where both terms are zero. However, for CCSD we originally had several terms to be multiplied, and we factorized them. This causes another potential optimization, that we wish to introduce with a simplified example. Consider four terms, A, B, C and D, that want to multiply together.

\begin{equation}
F = A \times B \times C \times D .
\end{equation}
Imagine factorizing this would speed up our calculations.

\begin{equation}
F = A \times (B \times (C \times D ) ) .
\end{equation}
Let us define intermediate E.

\begin{equation}
E = B \times (C \times D ) .
\end{equation}
After we calculated this we are left with

\begin{equation}
F = A \times E .
\end{equation}
At the end of the calculation, it turns out A was equal to zero. This means the entire calculation was wasted, as F would have been zero anyway. Spin consideration causes this situation to occur in CCSD. Luckily because this comes from spin considerations, it is deterministic. We can identify all these situations and avoid calculations. \\

This is implemented in our code, and we want to present an example from the contributions to $t_{ij}^{ab}$ from $[W4]$.

\begin{equation}
D_{ij}^{ab} t_{ij}^{ab} \leftarrow \sum_{kc} t_{jk}^{bc} \times [W_4]_{ic}^{ak} .
\end{equation}
Index $b$ and $j$ only appear in the T2 amplitudes, while indexes $a$ and $i$ appear in the intermediate. Imagine now that index $b$ is an odd number, while index $j$ is an even number. \\

In the sum over $k$ and $c$, $k$ and $c$ can themselves be odd or even. We remember we arranged our MOs so that all odd numbers had same spin orientation. Inside the sum, whenever now $c$ is an odd number we will be exciting two electron into spin up orbitals. If $j$ was an even number we also remove an electron from a spin down orbital. \\

Regardless of index $k$ this will not result in zero spin in total and the amplitude must be equal to zero with our spin restriction. In section \ref{compact_storage} we noted a situation where one of the two sub-matrices would be zero. This is the situation. \\

Also, the indexes $a$, $b$, $i$ and $j$ must themselves result in zero spin in total, or the amplitude will be zero. This limits the number of possible combinations of $t_{jk}^{bc}$ and $[W_4]_{ic}^{ak}$ to where we can actually avoid calculating some terms of $[W_4]_{ic}^{ak}$ that are not equal to zero. \\

The easiest and most human-time effective way of implementing this is to simply go through the factorization backwards, to identify which multiplications we did not need. This has been done.

\section{CCSD Parallel Implementation}
In parallel implementation we will make extensive use of memory distribution. In CCSD it is quite normal to read some of the MOs from disk. We will not be doing this, but we will place memory distribution as our number one priority. The code was however originally designed to read from disk, so this option is left easily available.

\subsection{Memory Distribution \label{kriseseksjon}}
In a serial implementation of CCSD the leading memory consumer is an $\frac{1}{16 \times 2} n_v^4$ sized array, $\langle ab||cd \rangle$. The $\frac{1}{16}$ comes from only storing spacial single bar MOs, $\langle ab|cd \rangle$, with $n_v$ being the number of virtual spin orbitals. The factor $\frac{1}{2}$ comes from symmetry. This array is called MO9 in our implementation. \\

However, the array only appears in the calculation of $t_{ij}^{ab}$, as this is the only place where the double bar integrals has three or more virtual indexes (which are $a$, $b$, $c$ etc). This means we can ensure one processor only requires parts of the array MO9 if we distribute work here correctly. It is also possible to distribute the double bar integrals themselves, \cite{ccsd_minne_distribuert_double_bar_artikkel} presents such an algorithm. \\

\begin{lstlisting}
for (int a = 0; a < unocc_orb; a++){
   for (int b = a+1; b < unocc_orb; b++){
      // Distribute work with Work_ID variable
      if (Work_ID % size == rank){
         // Perform calculation
     // Only the processor who passes this if test
     // will need <ab||cd> with specific a and b
      }
   }
}
\end{lstlisting}

All the largest parts of the single bar integrals will be distributed in memory. This leaves the largest un-distributed arrays as our T2 amplitudes and some intermediates. Specifically the old and new $t_{ij}^{ab}$, $[W_4]$ and $\tau_{ij}^{ab}$. \\

We will be able to distribute $[W_4]$, through some quite complex operations that actually also provides quite good parallel performance. \\

Because we want to store the old T2 amplitudes as specified in Eq. \eqref{howtostoret2} we are unable to take advantage of symmetries. We are however able take advantage of one symmetry for $\tau_{ij}^{ab}$ and the new T2 amplitudes. Also we have the storage of section \ref{compact_storage}. This means non distributed memory is scaling as

\begin{equation}
M(n_v, n_o) = \left(\frac{1}{2} + \frac{1}{4} + \frac{1}{16} + \frac{1}{16} \right) n_v^2 n_o^2 \approx n_v^2 n_o^2 .
\end{equation}
The factor $\frac{1}{2}$ is the old T2 amplitudes. The $\frac{1}{4}$ is the intermediate $\tau_{ij}^{ab}$. This intermediate improves performance only modestly in our factorization, but is very helpful when optimizing the use of external math libraries. We therefore keep it. The final factors $\frac{1}{16}$ are parts of the MOs we where unable to distribute in memory and also the new T2 amplitudes. The new T2 amplitudes require less memory because we only store one version of each symmetric term. How to distribute $[W_4]$ will be discussed shortly. Contributions from $n_v n_o^3$ is ignored in the non distributed memory scaling.

\subsection{Three Part Parallel}
Our parallel implementation will be quite straight forward. We will split the iterative procedure in three. Part two is the calculation of $[W_4]$. Part three is the amplitudes. Part one is everything else. The split is performed to keep in line with our guiding parallel principles of minimizing communication initiations. In each part we will also discuss what type of performance we can expect with increased number of CPUs in use. This is known as scaling. \\

\subsubsection{Part 3}
We first look at the third parallel part, the amplitudes. Each processor allocates memory for the new T2 amplitudes. At first each processor only stores the terms it performs calculations on itself. That is, the T2 amplitudes are distributed in memory. \\

We want $I_{ab}^{cd}$ distributed in memory. The numbers for this variable is stored in MO9. To make the memory distribution easy, we distribute work for $t_{ij}^{ab}$ based on indexes $a$ and $b$. These indexes are symmetric in $t_{ij}^{ab}$. We thus only need calculations on $b > a$. Since we stored the single bar spacial MOs, we must distribute work very delicately if we are to not get to much overhead. \\

Work is distributed with in a block cyclic manner, with the block always being of size 2. This block size is identical to the number of spin MOs per spacial MO. The optimal work distribution with these limitations has the mathematical formula, with all divisions being integer divisions.

\begin{equation}
Work\_ID = \frac{a}{2} \times \frac{n_v}{2} + \frac{b}{2} - \sum_{n=0}^a \frac{n}{2} .
\end{equation}
The number two is the block size, $\frac{n_v}{2}$ is the size of a column in MO9 and the sum ensures we get the optimal distribution for $b > a$. This forumla distributes work over indexes a/2 and b/2 optimally. The work ID is used by the processors to figure out if the calculation is to be performed.

\begin{lstlisting}
// Find new T2 amplitudes, function
for (int a = 0; a < unocc_orb; a++)
{
   sum_a_n += a/2;
   A = a/2;
   AA = A* Speed_Occ - sum_a_n;

   // Potential to read from file here.
   // Read in a 3 dimensional array of single bar integrals for a
   // specific index a. Same array is used for a and a+1
   // Can use for example MPI_File_read(...)

   // a is an even number
   for (int b = a+2; b < unocc_orb; b++){ // b is even number
      B = b/2;
      Work_ID = AA+B;
      if (Work_ID % size == rank){
     // Load up 2D arrays for external math libraries
         Fill_integ3_2D(a, b);
         Fill_integ9_2D(a, b);

     // Reindexing of tau for external math library use
         Fill_2D_tau(a, b);

         for (int i = 0; i < n_Electrons; i++){ // i is even number
            for (int j = i+2; j < n_Electrons; j++){ // j is even number
               MY_OWN_MPI[index_counter] =
        (-MOLeftovers(a/2, b/2)(j/2, i/2)
        + MOLeftovers(a/2, b/2)(i/2, j/2)
        + W_5(a,b)(i/2,j/2)
                - W_5(a,b)(j/2,i/2)
        - accu(W_2(i,j)(a/2, span()) % T_1.row(b/2))
                + accu(W_2(i,j)(b/2, span()) % T_1.row(a/2))
        + 0.5*accu(W_1(i,j)(span(0, Speed_Elec-1), span()) % tau1(span(0, Speed_Elec-1), span())) // Half matrix = 0, skip this
        - accu(t2.at(b,i)(span(0, Speed_Occ-1), j/2) % D3.row(a).t())
                + accu(t2.at(a,i)(span(0, Speed_Occ-1), j/2) % D3.row(b).t())
        + accu(t2.at(a,j)(b/2, span()) % D2.row(i/2))
                - accu(t2.at(a,i)(b/2, span()) % D2.row(j/2))
        - accu(integ9_2D(span(0, Speed_Occ-1), i/2) % T_1(span(0, Speed_Occ-1), j/2))
                + accu(integ9_2D(span(0, Speed_Occ-1), j/2) % T_1(span(0, Speed_Occ-1), i/2))
        + 0.5*accu(integ3_2D(span(0, Speed_Occ-1), span()) % tau3(i,j)(span(0, Speed_Occ-1), span()))) // Half matrix = 0, skip this
        
        / (DEN_AI(a/2,i/2)+DEN_AI(b/2, j/2));

        // This is one new T2 amplitude.
        // Plus one on index counter, and calculate the next
                index_counter++;
                j++;
             }
             i++;
          }
       }
       b++;
    }
}
\end{lstlisting}

The code segment above is a part of the function for the new T2 amplitudes. This is how our actual code looks. It is designed for performance. The outer loop is index a. If we wanted to read from file, we would be reading in the single bar integrals for a specific index a, into an $N^3$ sized array. \\

The next loop is index b. Here we figure out if a local processor is to perform these calculations. If the processor shall perform calculations, we fill up the largest arrays needed for external math library use. The smaller arrays used in external math libraries are already filled. \\

The next loops are i and j. Since spin must be zero, an even number a and b only allows for even number i and j. The other combinations of odd b, even a etc are also implemented in our code but not included here. \\

Inside the four loops we calculate the amplitude $t_{ij}^{ab}$. We notice every term is written using external math libraries, with the accumulation function. We also skip calculations using the span() function where appropriate, as noted in the previous section. Finally we divide by the denominator and store the new amplitude in a one dimensional array for easier MPI function use. \\

Once calculations are completed we must gather the results and update the old T2 amplitudes. The new T2 amplitudes are all stored in a one dimensional array on each local processor, so we only need to initiate one communication procedure. The most effective would be a collective all-to-all communication, where we send in the new amplitudes and gather them/write over the old ones. For this we could use a function like MPI\_Allgatherv. However this does not work with armadillo, since we cannot map values into an armadillo field directly with MPI. \\

This complicates things slightly. We see two solutions to the problem, but neither is as efficient as the before mentioned one. \\

We can either allocate a new one-dimensional array and perform the prior solution with a mapping of the new amplitudes into the armadillo field afterwards. Or we can perform P one-to-all broadcasts, where P is the number of MPI procs. Then each processors sends its information to others, and this is mapped into the armadillo type array. We chose the prior, but it is slightly less effective. There will be a mapping procedure required. The scaling of the communication will be identical to the scaling of an MPI\_Allgatherv.

\begin{lstlisting}
MPI_Allgatherv(MY_OWN_MPI, WORK_EACH_NODE(rank), MPI_DOUBLE,
                   SHARED_INFO_MPI, Work_Each_Node_T2_Parallel, Displacement_Each_Node_T2_Parallel,
                   MPI_DOUBLE, MPI_COMM_WORLD);
\end{lstlisting}

Here my own MY\_OWN\_MPI holds the information calculated on the processor. SHARED\_INFO\_MPI will contain the full new non distributed T2 amplitudes. The new amplitudes are symmetric, and we only store one version of each symmetric term. SHARED\_INFO\_MPI is the array we counted as non distributed new T2 amplitudes in section \ref{kriseseksjon}. But our algorithm is really designed to distribute the the new T2 amplitudes. However because we use armadillo with MPI we need this extra variable.\\

Without the complication arising from armadillo and MPI we could also skip this mapping of symmetries into the old T2 amplitudes.

\begin{lstlisting}
for (int K = 0; K < size; K++){
   sum_a_n = 0;
   for (int a = 0; a < unocc_orb; a++){
      sum_a_n += a/2;
      A = a/2;
      AA = A * Speed_Occ - sum_a_n;

      for (int b = a+2; b < unocc_orb; b++){
         B = b/2;
         INDEX_CHECK = AA+B;
         if (INDEX_CHECK % size == K){
            for (int i = 0; i < n_Electrons; i++){
               for (int j = i+2; j < n_Electrons; j++){
                  temp = SHARED_INFO_MPI[index_counter];

          // Map out symmetries after communication
                  t2(a,i)(b/2, j/2) = temp;
                  t2(b,i)(a/2, j/2) = -temp;
                  t2(a,j)(b/2, i/2) = -temp;
                  t2(b,j)(a/2, i/2) = temp;

                  index_counter++;
                  j++;
               }
               i++;
            }
         }
         b++;
      }
   }
}
\end{lstlisting}

\subsubsection{Part 2}
Next is part two of the parallel implementation, which is the $[W_4]$ calculation. Here we also want to distribute this variable in memory. We want to store $[W_4]$ as described previous to make use of external math libraries most effectively.

\begin{equation}
W4(a,i)(c,k) .
\end{equation}
Most of the contributions to $[W_4]$ are themselves distributed in memory on the indexes a and k. We therefore perform calculations on a local processor in a cyclic grid over these two indexes. A local processor thus holds

\begin{equation}
W4(a,*)(*,k) .
\end{equation}
Here star means all terms in this index. If we temporarily swaps the indexes i and k we can store W4(a,k)(*,*), and calculate these terms.

\begin{lstlisting}
for (int a = 0; a < unocc_orb; a++){
   for (int m = Where_To_Start_Part2(rank,a); m < n_Electrons; m+=jump){
      // Fill 2D arrays ready for external math libraries
      // These are distributed in memory
      Fill_integ7_2D(a,m);
      Fill_integ5_2D(a,m);

      for (int e = 0; e < unocc_orb; e++){
         Fill_integ2_2D_even_even(e, m);
         for(int i = 0; i < n_Electrons; i++){
            W4(a,m)(e,i) = -integ7_2D(e/2,i/2)
        - accu(W_3.at(i,m)(e/2,span()) % T_1.row(a/2))
                + accu(integ5_2D(span(0, Speed_Occ-1),e/2) % T_1(span(0, Speed_Occ-1),i/2))
                + 0.5*accu(integ2_2D % t2.at(a,i));
            i++;
         }
         e++;
      }
   }
   a++;
}
\end{lstlisting}

The Where\_To\_Start\_Part2(rank,a) variable will be explained later. However, when this intermediate contributes to the T2 amplitudes we need the full matrix that is stored in $W4(a,i)$. \\

Therefore we perform a communication. To pick the correct MPI function we also need to know what to do with the array after the communication. Afterwards we want to multiply

\begin{equation}
\sum_{ck} W4(a,i)(c,k) \times t2(b,j)(c,k)   .\label{gasghashkashfbdbhcxxcnxcruu}
\end{equation}
This multiplication will run in parallel, with work distributed cyclically over $a$ and $i$. The optimal work distribution forumla is

\begin{equation}
Work\_ID = a \times n_o + i .
\end{equation}
The communication needed to get the correct array needed prior and after is thus an all-to-all personalized communication. We will use MPI\_Alltoallw. Each processor here sends its own personalized message to the other MPI procs. The message is reduced to a one dimensional array in MY\_OWN\_MPI before communication. \\

\begin{lstlisting}
MPI_Alltoallw(MY_OWN_MPI, Global_Worksize_2[rank], Global_Displacement_2[rank], mpi_types_array, SHARED_INFO_MPI, Global_Worksize_2_1[rank], Global_Displacement_2_1[rank], mpi_types_array, MPI_COMM_WORLD);
\end{lstlisting}

We then perform the multiplication in Eq. \eqref{gasghashkashfbdbhcxxcnxcruu}, with work distributed over a and i. We want this contribution to be added to the new T2 amplitudes, which themselves are work distributed over $a$ and $b$. This means we add another MPI\_Alltoallw communication to get the correct data to the correct processor. \\

We have introduced a temporary memory distributed variable $W5(a,b)(i,j)$ to store this contribution to $t_{ij}^{ab}$. The positive features of this algorithm is that we indeed get all the variables distributed in memory, and communication procedures initiated are two All-to-All communications. All-to-All is generally the most effective kind of communication in MPI. We note that the number of initiated communications is independent of number of processors. This is exactly in line with our parallel implementation guidelines states earlier. The scaling of this communication will be identical to the scaling of two MPI\_Alltoallw functions. This function is highly optimized. Also the work distribution is optimal.

\subsubsection{Part 1 \label{problem_part_ccsd_parallel}}
The final part of our parallel implementation consist of everything else. Here we construct $[W_1]$, $[W_2]$, $W_3]$, $[F_1]$, $[F_2]$ and $[F_3]$. The contribution from $[F_1]$ to $[F_2]$ and $[F_3]$ is a $n^3$ contribution. Thus we do not need to run this part in parallel. The energy and $\tau_{ij}^{ab}$ is also calculated in serial in the current implementation. These are $n^4$ terms, and will be the leading non parallel calculations. \\

To perform this part of the parallel implementation we make cyclical grids of different kinds. We can reuse the array of new T2 amplitudes, since it is not needed at this step in the calculations. This enables less memory usage. We fill the array with all numbers calculated on the processor. Then perform communication just as in Part 3, and map the correct numbers into the correct armadillo fields. \\

\begin{lstlisting}
MPI_Allgatherv(MY_OWN_MPI, Work_Each_Node_part1_Parallel[rank], MPI_DOUBLE, SHARED_INFO_MPI, Work_Each_Node_part1_Parallel, Displacement_Each_Node_part1_Parallel, MPI_DOUBLE, MPI_COMM_WORLD);
\end{lstlisting}

We combine all these variables into one communication to maximise the number of jobs to distribute in accordance to the principles stated in section \ref{work_dist_section_1341}. Also to minimize the latency by initializing less communication procedures. \\

However because we combine several different variables we combine jobs that are not of the same size. This causes problems in our job distribution. The job distribution of part one in our parallel implementation is sub-optimal. In larger calculations some CPUs can get twice the workload of other CPUs. We have used a few tricks to lessen this performance problem. These are things like shifting the job distribution.

\begin{lstlisting}
if ((Work_ID + Shift) % size == rank){
   // Perform job
}
\end{lstlisting}

The results however are not optimal and as such we cannot expect an optimal performance of this part of the CCSD implementation. Even with this concern combining all remaining variables into one MPI communication was still the better solution, compared to performing communications after each intermediate calculation.   

\newpage

\subsection{Extra Pre Iterative Procedures}
Before we start iterating we must map out a few new variables. These are extra calculations not needed in serial and includes variables such as displacement and size of messages in the communications. They are calculated in the class \\ ccsd\_non\_iterative\_part. \\

\begin{lstlisting}
if (Work_ID % size == rank)
{
    // Do calculation
}
\end{lstlisting}

We also map out which Work\_ID each processor are to perform calculations on. This is for example the Where\_To\_Start\_Part2(rank,a) variable. This enables us to remove all if tests like the one above from our iterative procedure. If tests inside a for loop can be very time consuming if the value true or false changes often from one index to the next. This is especially true if we have two processors. In this case, the value would change every time an index is changed. This means the number of pipeline flushes could potentially be large, dependant upon the compiler. \\

For P processors, the value of the if test changes after (P-1) index changes. Not having any if tests helps performance somewhat, in particular for a small number of MPI procs.

\chapter{CCSDT implementation guide \label{ccsdt_chapter}}
In this chapter will discuss the implementation of CCSDT. CCSDT includes the triple electron excitation contribution. We will not derive the equations, these are available in Ref.\cite{CCSDT-ref4}. The chapter is based on this book and a series of papers, such as Ref.\cite{CCSDT-ref3} and referances therein, by Jozef Noga and Rodney Bartlett. Also Refs. \cite{CCSDT-ref5} and \cite{CCSDT-ref1} with erratium \cite{CCSDT-ref2}, and references therein. \\

This chapter is written as a guide for implementing CCSDT with simplicity and benchmarks. CCSDT requires much computation, and contains complex equations. There are however approximations made to the CCSDT equations and these have been given their own name, CCSDT-n methods. Here n = 1a,1b,2,3,4. The chapter starts with the ready CCSD equations and expands through the CCSDT-n methods one by one. \\

The CCSDT equations can be quite difficult to extract from the literature in an implementation ready form. Each section in this chapter will start with a description of which new terms are included and supply them in a factorized form ready for implementation. We will continuously use equations from Ref.\cite{CCSDT-ref4}. \\

Throughout the chapter there are provided benchmarks for each contribution added. We hope this chapter will be useful for anyone who wish to create a working CCSDT code. The additional code needed for systems not based on Hartree Fock will be provided in the final section.

\section{System for Benchmarks}
We first define our system and must ensure the input is correct if we wish to recreate the benchmarked energy. The geometries are taken from Ref.\cite{CCSDT-ref10}. We will study $H_2O$ with coordinates \\

\begin{center}
  \begin{tabular}{ c c c c }
  \hline
     Atom & x & y & z \\ \hline
     O & 0 & 0 & -0.009 \\
     H & 1.515263 & 0 & -1.058898 \\
     H & -1.515263 & 0 & -1.058898 \\
     \hline \\
  \end{tabular} 
\end{center} 

Coordinates are given in atomic units. We use a convergence criteria $10^{-7}$. The basis set is available on EMSL as DZ (Dunning). In DALTON it is available as DZ(Dunning) without the space between. A RHF calculation using this input gives energy in atomic units:

\begin{equation}
E_{RHF} = -76.0098 .
\end{equation}

The CCSD correction to the system should be:

\begin{equation}
E_{CCSD} = -0.146238 .
\end{equation}

The benchmarks can be verified in Ref.\cite{CCSDT-ref3}. We will also supply an additional benchmark of the same system outside of equilibrium. The same input is used except the geometry is now: \\

\begin{center}
  \begin{tabular}{ c c c c }
  \hline
     Atom & x & y & z \\ \hline
     O & 0 & 0 & 0 \\
     H & -2.27289   & 0 & 1.574847 \\
     H & 2.27289 & 0 & 1.574847 \\
     \hline \\
  \end{tabular} 
\end{center} 

All calculations starts with an initial guess of $t_{ijk}^{abc} = 0$.

\section{Theory}
CCSDT, as opposed to CCSD, includes $\textbf{T}_3$. 

\begin{equation}
\textbf{T} = \textbf{T}_1 + \textbf{T}_2 + \textbf{T}_3 .
\end{equation}
All the terms from CCSD will also be included in CCSDT. Indexes a,b,c,d,e,f are understood to go over virtual orbitals. Indexes i,j,k,l,m,n go over orbitals occupied in the RHF basis. The amplitude equations are defined as:

\begin{align}
\langle \Psi_i^a | \textbf{H}_N (1 + \textbf{T}_2 + \textbf{T}_1 \textbf{T}_2 + \frac{1}{2} \textbf{T}_1^2 + \frac{1}{6} \textbf{T}_1^3 + \textbf{T}_3) | \Psi_0 \rangle_C = 0.
\end{align}
Here $\textbf{T}_3$ is the new contribution, compared to the CCSD equations.

\begin{align}
\langle \Psi_{ij}^{ab} | \textbf{H}_N (
1 + \textbf{T}_2 + \frac{1}{2} \textbf{T}_2^2
+ \textbf{T}_1 + \textbf{T}_1 \textbf{T}_2 + \frac{1}{2} \textbf{T}_1^2 
+ \frac{1}{2} \textbf{T}_1^2 \textbf{T}_2 \nonumber \\
+ \frac{1}{6} \textbf{T}_1^3 + \frac{1}{24} \textbf{T}_1^4 + \textbf{T}_3 + \textbf{T}_1 \textbf{T}_3) | \Psi_0 \rangle_C = 0.
\end{align}
The new contributions are $\textbf{T}_3$ and $\textbf{T}_1 \textbf{T}_3$. 

\begin{align}
\langle \Psi_{ijk}^{abc} | \textbf{H}_N (
\textbf{T}_2 + \textbf{T}_3 + \textbf{T}_2 \textbf{T}_3 + \frac{1}{2} \textbf{T}_2^2
+ \textbf{T}_1 \textbf{T}_2 + \textbf{T}_1 \textbf{T}_3 + \frac{1}{2} \textbf{T}_1^2 \textbf{T}_2 
\nonumber \\
+ \frac{1}{2} \textbf{T}_1 \textbf{T}_2^2 + \frac{1}{2} \textbf{T}_1^2 \textbf{T}_3 + \frac{1}{6} \textbf{T}_1 \textbf{T}_2) | \Psi_0 \rangle_c = 0.
\end{align}
All of these are new contributions. The energy expression remains unchanged from CCSD. CCSDT-n methods include more and more of these new contributions. \\

For the interested reader who wish to verify the equations we will soon present with those given in Ref.\cite{CCSDT-ref4}, we add a few permutation operator tricks. One form of the permutation operator in the CCSDT equations are $\textbf{P}(a/bc)$. $\textbf{P}(a/bc)$ is defined as

\begin{equation}
\textbf{P}(a/bc) f(a,b,c) = f(a,b,c) - f(b,a,c) - f(c,b,a) .
\end{equation}
or as the permutation where a is exchanged by b and a is exchanged by c. Two of these,  $\textbf{P}(a/bc) \textbf{P}(k/ij)$, gives in total nine permutations. \\

Another form of the permutation operator is $\textbf{P}(abc)$. This is defined as the 6 permutations

\begin{align}
\textbf{P}(abc) f(a,b,c) = & f(a,b,c) - f(b,a,c) - f(a,c,b)  \nonumber \\ & - f(c,b,a) + f(b,c,a) + f(c,a,b) . \label{temporary_equation_label_for_rfeffsf}
\end{align}

These two permutation operators can be interchanged if one rewrite Eq. \eqref{temporary_equation_label_for_rfeffsf}. 

\begin{align}
\textbf{P}(abc) f(a,b,c) = & f(a,b,c) - f(b,a,c) - f(a,c,b)  \nonumber \\ & - f(c,b,a) + f(b,c,a) + f(c,a,b) \nonumber \\ &
= \left[ f(cab) - f(acb) - f(bac) \right] \nonumber \\ &
- \left[ f(cba) - f(bca) - f(abc) \right]
\nonumber \\ &
= \textbf{P}(c/ab) \left[ f(cab) - f(cba) \right] .
\end{align}

\section{CCSDT-1a}
The simplest inclusion of triples is CCSDT-1a. This method includes the contribution from $\textbf{T}_3$ in $t_i^a$, $\textbf{T}_3$ in $t_{ij}^{ab}$ and $\textbf{T}_2$ in $t_{ijk}^{abc}$. This can be expressed as

\begin{equation}
t_{ijk}^{abc} D_{ijk}^{abc} = 
\textbf{P}(a/bc) \textbf{P}(k/ij) \sum_e I_{bc}^{ek} t_{ij}^{ae}
- \textbf{P}(c/ab) \textbf{P}(i/jk) \sum_m I_{mc}^{jk} t_{im}^{ab} .
\end{equation}
Here the denominator is defined as the Moller-Plesset denominator. 

\begin{equation}
D_{ijk}^{abc} = f_{ii} + f_{jj} + f_{kk} - f_{aa} - f_{bb} - f_{cc} .
\end{equation}

To make life simpler in more advanced CCSDT-n algorithms we already insert intermediates. We define two intermediates 

\begin{equation}
[X1]_{ab}^{ei} = I_{e,i}^{ab} ,
\end{equation}
and
\begin{equation}
[X2]_{ij}^{am} = I_{am}^{ij} .
\end{equation}

This means our equation for $t_{ijk}^{abc}$ is now

\begin{align}
D_{ijk}^{abc} t_{ijk}^{abc} = & \textbf{P}(a/bc) \textbf{P}(k/ij) \sum_e [X1]^{ek}_{bc} t_{ij}^{ae} \\ \nonumber &
- \textbf{P}(c/ab) \textbf{P}(i/jk) \sum_m [X2]_{jk}^{mc} t_{im}^{ab} .
\end{align}

CCSDT-1a makes no changes in the calculation of the energy, however the $\textbf{T}_3$ contributions are added to $t_i^a$ and parts of the $\textbf{T}_3$ contribution are added to $t_{ij}^{ab}$. To indicate the terms from the original CCSD method should also be included we use $\leftarrow$. 

\begin{equation}
D_i^a t_i^a \leftarrow \frac{1}{4} \sum_{bcjk}  I_{j,k}^{b,c} t_{ijk}^{abc} m
\end{equation}
and
\begin{align}
D_{ij}^{ab} t_{ij}^{ab} \leftarrow & 
\frac{1}{2} \sum_{kcd} I_{bk}^{cd} t_{ijk}^{acd}
- \frac{1}{2} \sum_{kcd} I_{ak}^{cd} t_{ijk}^{bcd} - \frac{1}{2} \sum_{mkc} I_{mk}^{jc} t_{imk}^{abc} + \frac{1}{2} \sum_{mkc} I_{mk}^{ic} t_{jmk}^{abc} .
\end{align}

CCSDT-1a actually gives a surprisingly good approximation to the full CCSDT energy, since the terms between the two usually adds and subtracts a similar amount to the energy. \\

The equations for $t_i^a$ is the same for CCSDT as for CCSDT-1a. Implementing the new equations should give the following result in atomic units for the system in equilibrium:

\begin{equation}
E_{CCSDT-1a} = -0.147577 .
\end{equation}
For the system outside of equilibrium we should have energy

\begin{equation}
E_{CCSDT-1a} = -0.209537 .
\end{equation}

\section{CCSDT-1b}
CCSDT-1b adds the remaining contribution to $t_{ij}^{ab}$. 

\begin{align}
D_{ij}^{ab} t_{ij}^{ab} \leftarrow
\sum_{klcd} I_{kl}^{cd} t_{ijk}^{abc} t_l^d  \frac{1}{2} \sum_{klcd} I_{kl}^{cd} \left(
 t_{ikj}^{bcd} t_l^a - t_{ikj}^{acd} t_l^b 
 + t_{kli}^{adb} t_j^c - t_{klj}^{adb} t_i^c \right) .
\end{align}

The energy correction at equilibrium is now:

\begin{equation} 
E_{CCSDT-1b} = -0.147580 .
\end{equation}

The same system outside of equilibrium should have a correction energy of

\begin{equation}
E_{CCSDT-1b} = -0.209517  .
\end{equation}

\section{CCSDT-2}
For CCSDT-2 we add all contributions from T2 that does not include T1 to $t_{ijk}^{abc}$. This explicitly includes the terms

\begin{align}
D_{ijk}^{abc} t_{ijk}^{abc} \leftarrow & 
\textbf{P}(i/jk) \textbf{P}(abc) \sum_{lde} I_{lb}^{de} t_{il}^{ad} t_{jk}^{ec}
+ \textbf{P}(ijk) \textbf{P}(a/bc) \sum_{lmd} I_{lm}^{dj} t_{il}^{ad} t_{mk}^{bc} \nonumber \\ &
- \frac{1}{2} \textbf{P}(i/jk) \textbf{P}(c/ab) \sum_{lde} I_{lc}^{de} t_{il}^{ab} t_{jk}^{de} \nonumber \\ & 
 + \frac{1}{2} \textbf{P}(k/ij) \textbf{P}(a/bc) \sum_{lmd} I_{lm}^{dk} t_{ij}^{ad} t_{lm}^{bc}  .
\end{align}

In our implementation this means changing X1 and X2, since we introduced these intermediates earlier. It should be noted that since there are currently no $\textbf{T}_3$ contributions to $t_{ijk}^{abc}$ these amplitudes does not need to be stored for each iteration. This feature applies to CCSDT-n methods up but not including CCSDT-4. The new intermediates for CCSDT-2 will be:

\begin{align}
[X1]_{ab}^{ie} = I_{ab}^{ie} +
\frac{1}{2} \sum_{mn} t_{mn}^{ab} I_{mn}^{ei} ,
\end{align}
and
\begin{align}
[X2]_{ij}^{am} = I_{ma}^{ij} + \frac{1}{2} \sum_{ef} t_{ij}^{ef} I_{ma}^{ef}  .
\end{align}
We also introduce two new intermediates.

\begin{equation}
[X12]_{ab}^{id} = \sum_{ld} I_{lb}^{ed} t_{il}^{ae} ,
\end{equation}
and
\begin{equation}
[X13]_{ij}^{al} = \sum_{md} I_{ml}^{dj} t^{ad}_{im} .
\end{equation}
The expression for $t_{ijk}^{abc}$ should be changed accordingly, leaving the contribution from CCSDT-1b untouched. This is indicated by the $\leftarrow$. 

\begin{align}
t_{ijk}^{abc} \leftarrow &
\sum_e \textbf{P}(i/jk) \textbf{P}(abc) [X12]_{ab}^{ie} t_{jk}^{ec}
+ \sum_m \textbf{P} (ijk) \textbf{P}(a/bc) [X13]_{ij}^{am} t_{mk}^{bc} .
\end{align}
The energy correction should now be in equilibrium:

\begin{equation}
E_{CCSDT-2} = -0.147459 .
\end{equation}
While outside of equilibrium:

\begin{equation}
E_{CCSDT-2} = -0.208938 .
\end{equation}
The latter was achieved in our program in 30 iterations.

\section{CCSDT-3}
CCSDT-3 adds all remaining contributions to $t_{ijk}^{abc}$ that does not themselves contain $T_3$ amplitudes. These  all contain T1. The equations are available in \cite{CCSDT-ref4}. For our purposes we continue with the implementation ready equations. For CCSDT-3 we must make a tweak in our intermediates, X12 and X13. The terms from CCSDT-2 remain, but we add some new ones. We also introduce two new intermediates.

\begin{align}
[X12]_{ab}^{ie} \leftarrow  & - \sum_l I_{al}^{id} t_l^b - \sum_{le} I_{lb}^{ed} t_i^e t_l^a - \sum_{lme} I_{lm}^{ed} t_m^b t_{il}^{ae} ,
\end{align}

\begin{align}
[X13]_{ij}^{am} \leftarrow & 
\sum_{md} I_{ml}^{dj} t_i^d t_m^a
- \sum_d I_{al}^{id} t_j^d
- \sum_{mde} I_{ml}^{de} t_{im}^{ad} t_j^e ,
\end{align}

\begin{align}
[X14]_{ab}^{id} = & 
\sum_{elm} I_{lm}^{ed} \left[
+ t_i^e t_l^a t_m^b
- t_l^e t_{im}^{ab} 
+ \frac{1}{2} I_{lm}^{ed} t_i^e t_{lm}^{ab}
\right]
\nonumber \\ &
+ \sum_{lm} I_{lm}^{id} t_l^a t_m^b
+ \sum_e I_{ab}^{ed} t_i^e ,
\end{align}
and
\begin{align}
[X15]_{ij}^{am} = &
\sum_m I_{ml}^{ij} t_m^a
- \sum_{ed} I_{al}^{de} t_i^d t_j^e
+ \frac{1}{2} \sum_{med} I_{ml}^{de} \tau_{ij}^{de} t_m^a  .
\end{align}
These two intermediates must be included in our $t_{ijk}^{abc}$ equation.

\begin{equation}
t_{ijk}^{abc} \leftarrow
\sum_e \textbf{P}(a/bc) \textbf{P}(k/ij)
[X14]_{ab}^{ie} t_{jk}^{ec}
+ \sum_m \textbf{P}(c/ab) \textbf{P}(i/jk)
[X15]_{ij}^{am} t_{mk}^{bc} .
\end{equation}
Inserting these equations we should now have the following energy correction in equilibrium:

\begin{equation}
E_{CCSDT-3} = -0.147450 .
\end{equation}
and outside equilibrium:

\begin{equation}
E_{CCSDT-3} = -0.208876 .
\end{equation}
For the reader who wish to optimize a CCSDT program, the four intermediates [X12], [X13], [X14] and [X15] can actually be placed inside [X1] and [X2], using the permutation operator tricks defined in Eq. \eqref{temporary_equation_label_for_rfeffsf}. \\

\section{CCSDT-4}
For CCSDT-4 we will add the terms that are linear in T3. This corresponds to the terms

\begin{align}
D_{ijk}^{abc} t_{ijk}^{abc} \leftarrow &
\sum _{ld}
\textbf{P}(i/jk) \textbf{P}(a/bc) 
I_{al}^{id} t^{dbc}_{ljk}
+
\frac{1}{2} \sum_{mk}
\textbf{P}(k/ij) I_{lm}^{ij} t_{lmk}^{abc}
\nonumber \\ &
+
\frac{1}{2} \sum_{de} 
\textbf{P}(c/ab) I_{ab}^{de} t^{dec}_{ijk} .
\end{align}
We will introduce these terms as three new intermediates, because there are more terms in full CCSDT that can use this factorization. These are defined as

\begin{equation}
[X3]_{ij}^{lm} = \frac{1}{2} I_{lm}^{ij} ,
\end{equation}

\begin{equation}
[X4]_{ab}^{de} = \frac{1}{2} I_{ab}^{de} ,
\end{equation}
and
\begin{equation}
[X6]_{al}^{id} = I_{al}^{id} .
\end{equation}
Inserting this in the amplitude equation leaves

\begin{align}
D_{ijk}^{abc} t_{ijk}^{abc} \leftarrow &
\sum _{ld}
\textbf{P}(i/jk) \textbf{P}(a/bc) 
[X6]_{al}^{id} t^{dbc}_{ljk}
+
 \sum_{mk}
\textbf{P}(k/ij) [X3]_{ij}^{lm} t_{lmk}^{abc}
\nonumber \\ &
+
\sum_{de}
\textbf{P}(c/ab)[X4]_{ab}^{de}  t^{dec}_{ijk} .
\end{align}
We again perform energy calculations on the same system as before. At equilibrium the energy correction should now be

\begin{equation}
E_{CCSDT-4} = -0.147613 .
\end{equation}
And off-equilibrium we should have

\begin{equation}
E_{CCSDT-4} = -0.209668 .
\end{equation}

\section{Full CCSDT}
For full CCSDT we introduce all the remaining terms. We will add these into our existing intermediates, and define a few new ones.

\begin{equation}
[X1]_{ab}^{ic} \leftarrow 
\sum_{lme}
\frac{1}{2}
I_{lm}^{ce} t_{lmi}^{aec} .
\end{equation}

\begin{equation}
[X2]_{ij}^{am} \leftarrow 
\sum_{lde}
\frac{1}{2}
I_{ml}^{de} t_{ilk}^{dea} .
\end{equation}

\begin{equation}
[X3]_{ij}^{lm} \leftarrow
\sum_d \left( I_{lm}^{dj} t_i^d
- I_{lm}^{di} t_j^d \right)
+ \sum_{de} \frac{1}{2} I_{lm}^{de} \tau_{ij}^{de} .
\end{equation}

\begin{equation}
[X4]_{ab}^{de} \leftarrow
\sum_l \left( 
I_{lb}^{de} t_l^a
- I_{la}^{de} t_l^b \right)
+ \sum_{ml} \frac{1}{2} I_{lm}^{de} \tau_{lm}^{ab} .
\end{equation}

\begin{equation}
[X6]_{al}^{id} \leftarrow 
\sum_{e} I_{al}^{ed} t_i^e
- \sum_m I_{ml}^{id} t_m^a
+ \sum_{em} I_{ml}^{ed} t_{im}^{ae}
- \sum_{em} I_{ml}^{ed} t_i^e t_m^a .
\end{equation}
We also introduce two new intermediates, [X7] and [X8].

\begin{equation}
[X7]_i^m = - \sum_{ld} I_{lm}^{di} t_l^d
- \frac{1}{2} \sum_{lde} I_{lm}^{de} \tau_{li}^{de} .
\end{equation}

\begin{equation}
[X8]_a^e = \sum_{ld} I_{la}^{de} t_l^d
- \frac{1}{2} \sum_{dlm} I_{lm}^{de} \tau_{lm}^{da} .
\end{equation}
These are added to $t_{ijk}^{abc}$ with the following permutation operators in front

\begin{equation}
D_{ijk}^{abc} t_{ijk}^{abc} \leftarrow \sum_e \textbf{P}(a/bc) [X8]_a^e t_{ijk}^{ebc}
+ \sum_m \textbf{P}(i/jk) [X7]_i^m  
t_{mjk}^{abc} .
\end{equation}
Implementing these terms, we get the full CCSDT energy correction in equilibrium

\begin{equation}
E_{CCSDT} = -0.147594 .
\end{equation}
This is identical to benchmark. Outside of equilibrium we get

\begin{equation}
E_{CCSDT} = -0.2095(20) .
\end{equation}
Here we marked a parenthesis around (20) due to the fact that Bartlett in his letters gives this energy as

\begin{equation}
E_{Bartlett} = -0.209519 .
\end{equation}
Meaning we have a difference of -0.000001 to Bartletts results. We will assume this is caused by round off errors.  

\section{Excluded Terms}
Some terms are zero when using a HF basis, because the Fock eigenvalues are diagonalized. If we want to perform CCSDT calculations for anything other than a HF basis, we must add these terms

\begin{equation}
[X1]_{ab}^{ic} \leftarrow - \sum_{ld} \langle l | F | d \rangle t_{li}^{ab} .
\end{equation}

\begin{equation}
[X15]_{ij}^{al} \leftarrow - \sum_{md} \langle m | F | d \rangle t_{ij}^{ad} .
\end{equation}

\begin{equation}
[X8]_a^d \leftarrow - \sum_l \langle l | F | d \rangle t_l^a .
\end{equation}

\begin{align}
t_{ijk}^{abc} \leftarrow &
\sum_d \textbf{P}(c/ab)
(1 - \delta_{cd}) \langle c | F | d \rangle t_{ijk}^{abd}
- \sum_l \textbf{P}(k/ij) 
(1 - \delta_{kl}) \langle k | F | l \rangle  t_{ijl}^{abc}
\nonumber \\ &
-
\sum_{ld}
\textbf{P}(i/jk)
\langle l | F | d \rangle t_i^d  t3^{abc}_{ljk} .
\end{align}


\chapter{Benchmarks}
In this chapter we will benchmark our code. We have already performed calculations using CCSDT and verified them with benchmark values. The full CCSDT method took advantage of all our implementation except for the UHF part. In this chapter we will look at the codes performance and find out for sure does it works for more systems. Also what are the strengths and what are the weaknesses of our implementations. We will look at all our methods, RHF, UHF, CCSD, and CCSDT. Also we will test our memory distributed AOtoMO transformation algorithm to its limits. In general we will benchmark our code against LSDALTON, Ref.\cite{LSDALTON_CITATION}, but we will also provide additional benchmarks in each section. CCSDT is neither available in DALTON nor LSDALTON. \\

We also mention no special flags are used to compile. We will supply plenty of performance results. The flags used were 

\begin{lstlisting}
CFLAGS = -pipe -O2 -Wall -W 
\end{lstlisting}

\section{Small systems}
We first perform some initial testing on small systems of water and $H_2$. These will be compared with the LSDALTON package, aswell as Ref.\cite{ccsd_benchmark_url_stuff} and Ref.\cite{CCSDT-ref1}. \\

We use these coordinates for all tests except with the DZ basis set, where we use the coordinates we used throughout chapter \ref{ccsdt_chapter}.  

\begin{center}
  \begin{tabular}{ c c c c }
  \hline
     Atom & x & y & z \\ \hline
     O & 0 & 0 & 0 \\
     H & 0 & 1.079252144093028 & 1.474611055780858 \\
     H & 0 & 1.079252144093028 & -1.474611055780858 \\
     \hline \\
  \end{tabular} 
\end{center} 

These are in atomic units. The coordinates are taken from Ref.\cite{ccsd_benchmark_url_stuff}.

\begin{center}
\begin{tabular}{ l c c r }
	\hline
  	Basis Set & RHF & CCSD Correction & Benchmark \\ \hline
  	STO-3G & -74.9627 & -0.0501273 & \cite{ccsd_benchmark_url_stuff} \\ 
  	4-31G & -75.9081 & -0.13668 & LSDALTON \\ 
  	6-31G & -75.9845 & -0.13603 & LSDALTON \\ 
  	DZ & -76.0098 & -0.146238 & \cite{CCSDT-ref1} \\ \hline
  	\\
	\end{tabular}
\end{center}

Our results are in agreement with the references down to the final decimal. We examine the RHF calculations with the STO-3G basis set more closely, in its components.  \\

\begin{center}
\begin{tabular}{ l r }
  	One-electron energy & = -122.219 \\ 
  	Two-electron energy & = 38.1615 \\
  	Repulsion energy & = 9.09485 \\
  	\\
	\end{tabular}
\end{center}

This is in perfect agreement with reference. With DIIS turned on this was achieved in 13 iterations with RHF. With DIIS turned off we needed 21 iterations.

\section{Hydrogen molecule}
Our next calculations will be on the diatomic hydrogen molecule. These results will be benchmarked against an FCI study from 1968, Ref.\cite{fci_h2_molecule_stuff}. Pople and others pioneered the effective use of Gaussian Type Orbitals throughout the 1970s. The FCI calculation used Slater Type Orbitals. We will plot the energy as a function of R, where R is the distance between the two nuclei in a.u. The results are available in figure \ref{fig:h2poten}. We will use the 6-311++G(2d,2p) basis set for both HF and CCSD calculations, and a convergence criteria of $10^{-5}$. Calculations are performed from R = 0.6 to R = 4.5 with 0.1 a.u. intervals. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{H2_CCSD_HF_plot.eps}}
\caption{Energypotential for $H_2$ Molecule. 
$^a$FCI results from Ref.\cite{fci_h2_molecule_stuff}}
\label{fig:h2poten}
\end{center}
\end{figure}

The energy minimum is located at R = 1.4 au with an energy of -1.17086. This was benchmarked against LSDALTON. We could also perform CCSDT calculations on this molecule, but we only have two electrons. This means the answer will be the same as a CCSD calculation, because we do not have three electrons to excite making all the $t_{ijk}^{abc}$ amplitudes 0. This was also confirmed by our program. \\

Also, this is true for all higher versions of coupled cluster, meaning the only difference between these results and the  reference value should be a truncated basis set. We therefore repeat our calculation with the largest Pople basis set the author is aware of, 6-311++G(3df,3pd). This calculation is marked with (a). We use R = 1.4011, which is the equilibrium distance according to our FCI benchmark. We also try the aug-cc-pVQZ basis set. This calculations is marked with (b). Our calculations results in an energy in atomic units:

\begin{equation}
E_{CCSD,(a)} = -1.17264 .
\end{equation}

\begin{equation}
E_{CCSD,(b)} = -1.17394.
\end{equation}
According to the benchmark FCI energy with these nuclei positions is -1.17447. \\

We did benchmark our results against LSDALTON and the two programs were in agreement for the same basis set. On our energy plot FCI results are plotted for $R \in [1.0, 2.0]$, however, results for R = 1.1 was lacking.

\section{First row Diatomic molecules}
The natural next step is to introduce heavier atoms in our diatomic molecule. In these systems CCSD no longer include the full correlation, and we will have more sources of error than just the basis set truncation. We use a decent sized basis set, 6-311++G(2d,2p) and a convergence criteria of $10^{-6}$. Our results are benchmarked against a paper with DMC calculation and marked with $^a$, see Ref.\cite{first_row_diatomic_referance_stuff}. \\

\begin{center}
\begin{tabular}{ l c  c c c r }
	\hline
  	Molecule & R [au] & $E_{HF}$ & $E_{CCSD}$ & 	$E_0^a$ & $E_R^b$ \\ \hline
  	$Li_2$ & 5.051 & -14.8701 & -14.9322 & -14.995 &  0.50 \\\hline
  	$Be_2$ & 4.63 & -29.1321 & -29.2646 & -29.338 &  0.64 \\ \hline
  	$B_2$ & 3.005 & -48.8656 & -49.1738 & -49.415 &  0.56 \\ \hline
  	$C_2$ & 2.3481 & -75.3973 & -75.7703 & -75.923 &  0.71 \\ \hline
  	$N_2$ & 2.068 & -108.979 & -109.367 & -109.542 & 0.70 \\ \hline
  	$O_2$ & 2.282 & -149.58 & -150.058 & -150.326 & 0.64  \\ \hline
  	$F_2$ & 2.68 & -198.741 & -199.272 & -199.529 & 0.67 \\ \hline
  	\\
	\end{tabular}
\end{center}

In this table the molecule is listed to the left. R is the distance between the nuclei. $E_{HF}$ and $E_{CCSD}$ is the HF and CCSD energies for the system. $E_0$ is our benchmark value, from Ref.\cite{first_row_diatomic_referance_stuff}. This is the exact, non-relativistic, infinite nuclei mass energy. We define $E_R$ to be the percentage of correlation recovered using CCSD.

\begin{equation}
E_R = \frac{E_{CCSD} - E_{HF}}{E_0 - E_{HF}} .
\end{equation}
We also benchmark our results against Henrik Mathias Eidings RHF - MP2 and MP3 results for $O_2$, as seen in Ref.\cite{hmeiding}. His results are marked as $^a$. We use the larger 6-311++G(3df,3pd) basis set, same as is used by Henrik. 

\begin{center}
\begin{tabular}{ l c c c r }
	\hline
  	Molecule & HF & MP2$^a$ & MP3$^a$ & CCSD \\ \hline
  	$O_2$ & -149.588 & -150.142 & -150.130 & -150.138 \\\hline
  	\\
	\end{tabular}
\end{center}

$O_2$ is commonly known as an open shell molecule, meaning our spin restriction is likely to cause problems. We therefore repeat this calculation using our unrestricted Hartree Fock implementation. \\

The first calculation for $O_2$ is performed with singlet spin orientation, meaning total spin is 0. All other input remains the same. 

\begin{equation}
E_{UHF,0} = -149.647 .
\end{equation}
The triplet state, where total spin is 1, gives energy

\begin{equation}
E_{UHF,1} = -149.674  .
\end{equation}
These results are in good agreement with Henriks calculations. The singlet calculation differ from his calculations with 0.001 in energy. In our iterations we had Henriks energy exact, but we used a stronger convergence criteria and continued iterations. The number of iterations needed was about 80. 

\section{$C_{20}$ Ground State} 
$C_{20}$ is a particularly interesting molecule. Calculations using different computational methods seem to provide very different answers to the geometry of the ground state, as is noted in Ref. \cite{c20article_cite_this}. There are three structures in considerations. They are ring, bowl and cage. All these structures are present in experiment, but ring seems to be the most likely orientation. This is followed by bowl as the second most likely, and cage as third most likely.  \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{c20_ring.eps}}
\caption{Orientation for $C_{20}$ in ring formation.}
\label{fig:c20ring}
\end{center}
\end{figure}

However methods considered highly accurate, such as DMC, does not agree with experiment. Different methods do not even agree with each other, as is seen in Ref.\cite{c20coordinatesarticlezz}. One theory for the disagreement is that experiments are not done at 0 Kelvin, while quantum chemistry ground state calculations are. We will perform RHF and CCSD calculations on the system. \\

We will use the 6-31G basis set, with a convergence criteria of $10^{-4}$. For a reference we calculate the ground state energy of a single carbon atom.

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{c20_bowl.eps}}
\caption{Orientation for $C_{20}$ in bowl formation.}
\label{fig:c20bowl}
\end{center}
\end{figure}

\begin{equation}
E_{HF} = -37.58 .
\end{equation}

\begin{equation}
20 \times E_{HF} = -751.6 .
\end{equation}

\begin{equation}
E_{CCSD} = -37.65 .
\end{equation}

\begin{equation}
20 \times E_{CCSD} = -753.0 .
\end{equation}

We would also like to visualize the three different orientations. All coordinates are taken from Refs.\cite{c20coordinatesarticlezz} and \cite{c20coordinatesarticlezz10}. Both these references use the same coordinates. The geometries for bowl, cage and ring are illustrated in figures \ref{fig:c20bowl}, \ref{fig:c20cage} and \ref{fig:c20ring}.\\




\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{c20_cage.eps}}
\caption{Orientation for $C_{20}$ in cage formation.}
\label{fig:c20cage}
\end{center}
\end{figure}

\begin{center}
\begin{tabular}{ l c r }
	\hline
  	Orientation & RHF & CCSD \\ \hline
  	ring & -756.454 & -758.261  \\ \hline
  	cage & -756.122 & -758.004  \\ \hline
  	bowl & -756.312 & -758.195  \\ \hline
  	\\
	\end{tabular}
\end{center}

Calculations on ring was performed without DIIS in HF, whereas cage and bowl had DIIS enabled. This was required to achieve convergence. We see the energy is lower than 20 times the energy of a single carbon for all three orientations. Our results are in good agreement with Ref.\cite{c20coordinatesarticlezz10} and experimental results. We have indeed found the ring to be the lowest energy orientation. However, we did not use a particularly large basis set. Thus our energies are not as low as we would want. According to Ref.\cite{c20article_cite_this} the energies for this system should be around -759. \\

We may not have settled the debate about the ground state orientation for $C_{20}$ at 0 Kelvin temperature, but we did manage to perform calculations on a somewhat larger molecule, showcasing some of our programs memory distributed potential. 

\section{Energy as function of number of AOs}
The size of the basis set has a great impact on our calculations. However at some point, the basis set is so large that making it even larger will not provide any noticeable improvement in accuracy. Any increase in basis set size will however always increase the runtime of our program. For this reason there is great interest in knowing what size basis set gives what level of accuracy. \\

To study this we have performed calculations on a single water molecule, $H_2O$. We have performed calculations using the STO-3G, 6-31G, 6-311G**, 6-311++G(2d,2p) and 6-311++G(3df,3pd). These basis sets are listed from smallest to largest. Results are provided in fig. \ref{fig:convplot}. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{convergenceplot.eps}}
\caption{Energy of $H_2O$ as a function of number of AOs}
\label{fig:convplot}
\end{center}
\end{figure} 

We check the convergence with respect to the number of AOs for both HF and CCSD. We see the HF energy has better convergence than CCSD. CCSD is not particularly well converged at all, this was to be expected however based on our $H_2$ results from earlier. \\

The largest calculation used approximate 80 AOs. With 80 AOs this would be $n_o = 10$ and $n_v = 150$, meaning $\frac{n_v}{n_o} = 15$. When performing CCSD calculations this fraction is normally expected to be between 5 - 10.

\section{Hartree Fock Performance Testing}
In this section we will test the performance of our Hartree Fock program. We will first look at memory usage, and afterwards discuss the performance of both RHF and UHF for different systems. We have not spent much time on optimizing this part of the program, except for the parallel implementation and memory distribution. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{available_memory.eps}}
\caption{Memory Requirements for HF as a function of N}
\label{fig:memory_needs}
\end{center}
\end{figure}

In figure \ref{fig:memory_needs} we have plotted the memory needs for our HF program as a function N for the serial version. N is the number of AOs. We will perform our calculation on the Abel computing cluster, Ref.\cite{abel_po_g_citation1234567}. This cluster consist of nodes. Each nodes has 16 CPUs and 64 GB of memory. This means each CPU has 4 GB of memory, if we discount that a small portion of the memory is already occupied by processes already running on the node. \\

On our figure we have marked the total available memory for a different number of CPUs. We are able to distribute the absolute dominant memory consuming array. This means the crossing point between available memory and needed memory is a good indication of the number of AOs we can run with this number of CPUs. For example 512 CPUs crosses the blue line at N = 700. This means we can do approximately 700 AOs with 512 CPUs. \\

It is possible on abel to ask for more memory for each CPU. There are also high-memory nodes available. In theory we could ask for all 64 GB of memory and only use one CPU on the node. This would however leave the other 15 CPUs unusable for other users, since there is no memory left. 

\subsection{HF performance}
To test the performance of our HF implementation we perform calculations on $O_2$ with the 6-311++G(3df,3pd) basis set. We use convergence criteria of $10^{-8}$. Only the four index integrals are run in parallel, so there are some serial calculation involved. The UHF implementation has more communication than RHF. We plot both performances in the same plot for comparison. The raw data is included in the table, and the timings involve all calculations in HF. Results are plotted in figure \ref{fig:hf_performance_stuff_jesus1234}.

\begin{center}
\begin{tabular}{ l c c }
	\hline
  	P & RHF time [s] & UHF time [s] \\ \hline
  	1 & 675.78 & 854.31 \\ 
  	2 & 322.16 & 411.68 \\ 
  	4 & 182.36 & 324.26 \\
  	8 & 129.94 & 123.69 \\ 
  	16 & 60.79 & 73.52 \\ 
  	32 & 36.58 & 49.87 \\ 
  	64 & 27.55 & 55.32 \\ 
  	128 & 25.02 & 44.59 \\
  	256 & 34.29 & 49.1  \\ \hline
  	\\
	\end{tabular}
\end{center}

\newpage

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{hartree_fock_parallel_performance.eps}}
\caption{Time in seconds for a HF calculation on $O_2$ with the 6-311++G(3df,3pd) basis set using $2^p$ processors. Both RHF and UHF calculations included. Results are not averaged over multiple runs.}
\label{fig:hf_performance_stuff_jesus1234}
\end{center}
\end{figure}


\section{AOtoMO Performance Testing \label{aotomoperformancetesting}}
In this section we will test the performance of our AOtoMO transformation algorithm for the four index integrals, as a function of number of CPUs. The raw data is included in the table, and we plot the data in two plots. The quantities of interest are time used in calculation versus time used in communication. Calculations spread over more CPUs can be performed faster. However more CPUs will require more communication. Calculations on 251 and 569 AOs are done on the $C_{12} H_{22} O_{11}$ molecule, sucrose. We use the 6-31G and the 6-311++G** basis sets. The 130 AO calculation is done on an imaginary molecule, simply to measure performance. \\

 \begin{center}
  \begin{tabular}{ c  c  c  c }
  \hline
     p & AOs = 130 & AOs = 251 & AOs = 569  \\ \hline
     1 & 123.55 & 3458  & - \\
     2 & 69.64 & 1800  & - \\
	 4 & 37.73 & 923  & -  \\
     8 & 25.51 & 618  & -  \\
    16 & 17.11 & 389  & 21076 \\
    32 & 14.52 & 297  & 15556 \\
    64 & 10.35 & 266  & 14961 \\
    128 & 10.62 & 232  & 11294
\\
    256 & - & -  & 9413
\\
    512 & - & -  & 9346
    
     \\ \hline \\
  \end{tabular} 
\end{center} 

We define wall time as the time from the first CPU start until the last CPU finish. We will use this as a measurement of performance of our algorithm. We will also define the point where wall time increase with increased number of CPUs as the time communication overtakes computation. \\

We should first remind ourselves the scaling here is $N^5$, where N is the number of AOs. The communication however scales as $N^4$. We see from our benchmarks that larger number of AOs scales better with increased number of CPUs. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{scaling_1.eps}}
\caption{AOtoMO transformation scaling for small number of AOs and up to 128 processors, p. Plotted on y - axis is T/$T_0$, where $T_0$ is the wall time for the serial version. On the x - axis we have $2^p$, where p is the number of CPUs}
\label{fig:aotomo1}
\end{center}
\end{figure}

We notice especially that the wall time increase from 64 to 128 CPUs for 130 AOs. This does not happen for 251 AOs. Indicating a better scaling for higher number of AOs, as expected from the algorithm. We also note that in a less optimized AOtoMO transformation, communication would overtake computation much quicker. \\

Our memory distributed model makes us able to run larger systems using increased number of CPUs. The memory usage of the algorithm is closely related to that of our Hartree Fock implementation. This is due to the fact that in our HF program we stored all terms of the four index integrals, whereas after HF is completed we will delete the terms corresponding to one symmetry, reducing the memory requirements by $\frac{1}{2}$. The transformed integrals are also stored using this one symmetry, and need $\frac{N^4}{2}$ memory. These two combined equals the memory needed for HF. \\

For implementation reasons there will be an additional $N^3$ array needed. This is because we will need one $N^3$ intermediate for armadillo, and one for the communication in MPI. This changes memory requirements  modestly, but becomes a problem for approximate 1000 AOs. \\

For 569 AOs we are unable to perform the calculation in serial. We start calculations using 16 CPUs. From figure \ref{fig:aotomo1} we notice the best scaling is behind us at this number of CPUs. However we can make an educated guess that running this sized system in serial would require several days of computations. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{scaling_2.eps}}
\caption{AOtoMO transformation wall time scaling for 569 AOs and $2^p$ CPUs. 16 to 512 CPUs used. }
\label{fig:aotomo2}
\end{center}
\end{figure}

Our results for 569 AOs confirm that the scaling is better with increased number of AOs. Even from 256 to 512 CPUs we improve overall performance.  \\ 

We also list a few single calculations for different number of AOs and CPUs, to see the performance of the transformation for a variety of calculations. The basis set used for the calculations was 6-311++G(2d,2p). \\

  \begin{center}
  \begin{tabular}{ c c c c }
  \hline
     Molecule & CPUs & AOs & AOtoMO \\ \hline
     $CH_4$ & 16 & 69 & 0.98 s \\
     $C_2H_6$ & 64 & 118 & 9.71 s \\
     $C_2 O H_6$ & 64 & 147 & 19.9 s \\
     $\left(H_2O\right)_8$ & 256 & 392 & 23 min \\
     $C_{20}$ & 256 & 580 & 238 min \\
     $\left(H_2O\right)_{15}$ & 512 & 735 & 35.8 hour \\
     \hline \\
  \end{tabular} 
\end{center} 

We see up to 600 AOs is very doable calculations. At 735 AOs the transformation itself takes more than one day. However, these timings are very good relative to others, Ref.\cite{aotomo_2_cite}. Full AOtoMO transformations are rare in the literature, especially for high number of AOs. It is therefore difficult to make comparisons. \\ 

Alternatively, on the benchmark page for ACESIII, Ref.\cite{aces_non_ref}, they list the wall time of a few CCSD calculations. CCSD regularly use the full AOtoMO transformation, and ACESIII is a very optimized CCSD program. Thus it is interesting to see if CCSD or AOtoMO would dominate wall time in a calculation with an optimized CCSD version. \\

An $Ar_6$ calculation with 300 AOs is listed on ACESIII benchmark site with 128 CPUs as 5.9 min per CCSD iteration. Using our AOtoMO transformation, 251 AOs was performed in 3.8 min. \\

They also list calculations on sucrose, $C_{12} H_{22} O_{11}$, using the 546 AOs, and 23 orbitals frozen. This means 523 unfrozen spin orbitals. They report 29.4 minutes for one iteration of 256 CPUs. Our four index transformation spent 156 minutes on 569 AOs. \\

These two results indicate the wall time for our transformation for this number of AOs lies approximated between 2-4 CCSD iterations, which is very reasonable. CCSD usually requires 10-20 iterations for convergence in equilibrium. Most of the benchmarks on the ACESIII site for CCSD with up to 512 CPUs is between 300 - 600 AOs, all of which is doable with this algorithm. \\

It should also be noted that our full AOtoMO transformation only depends upon number of AOs, whereas CCSD depends on the combination of occupied versus virtual orbitals. In a CCSD calculation with $n_v >> n_o$ AOtoMO would be more time consuming relative to a CCSD iteration. 

\section{CCSD Serial Performance \label{ccsdserialperformance_ppp}}
In this section we will test the CCSD performance in serial. We will try a cluster of water molecules, of size $(H_2O)_N$. We will use the 6-311++G(2d,2p) basis set. We also note the serial time of the AOtoMO transformation. 

\begin{center}
\begin{tabular}{ l c c c c c }
	\hline
  	System & AOs & $n_o$ & $n_v$ & CCSD iteration [s] & AOtoMO [s] \\ \hline
  	$(H_2O)$ & 49 & 10 & 88      & 1.17 & 0.75  \\ 
  	$(H_2O)_2$ & 98 & 20 & 176   & 37 & 14.41 \\ 
  	$(H_2O)_3$ & 147 & 30 & 264  & 632 & 297  \\
  	$(H_2O)_4$ & 196 & 40 & 352  & 2255 & 1454 \\ 
  	\hline
  	\\
	\end{tabular}
\end{center}

We test the serial implementation against an unoptimized but factorized CCSD program. This implementation is available through "CCSD1" as method in the input file. The equations are fully factorized, so the theoretical scaling of the equations is still $N^6$, where N is the number of AOs. However we have not implemented the use of external math libraries, compact storage and the other optimizations discussed in section \ref{optimize_serial_version_bii}. These calculations are not performed on abel. We also use the external math library BLAS. The performance here on one CPU is generally better than on abel supercomputer, but we do not have the high number of CPUs available. 

\begin{center}
\begin{tabular}{ l c c c c c }
	\hline
  	System & $n_o$ & AOs & Unoptimized iter [s] & Optimized iter [s] & Fraction \\ \hline
  	Ne & 10 & 29 & 3.5 & 0.1 & 35 \\
  	$H_2O$ & 10 & 49 & 40 & 0.78 & 51 \\
  	$C_2H_4$ & 16 & 62 & 670 & 5.4 & 124 \\
  	$C_2H_4$ & 16 & 98 &  & 48 &  \\
  	\hline
  	\\
	\end{tabular}
\end{center}

\section{CCSD Parallel Performance}
Our CCSD serial implementation is among the fastest. We want to test its parallel performance in detail. First we run a small system of $H_2O$ with the 6-311++G(3df,3pd) basis set for a different number of processors. We use up to 64 processors on this system. The raw data is included.
   
\begin{center}
\begin{tabular}{ l c}
	\hline
  	P & CCSD iteration time [s] \\ \hline
  	1 & 4.44  \\ 
  	2 & 2.35   \\ 
  	4 & 1.30   \\
  	8 & 0.88    \\ 
  	16 & 0.48   \\ 
  	32 & 0.27   \\ 
  	64 & 0.18   \\ \hline
  	\\
	\end{tabular}
\end{center}

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.7\textwidth]{h2o_ccsd_time_per_iter.eps}}
\caption{Time per iteration of a CCSD run on $H_2O$ with the 6-311++G(3df,3pd) basis set using $2^p$ processors.}
\label{fig:ccsd_h2o_time_per_iter}
\end{center}
\end{figure}

\newpage

The iteration time for 64 processors oscillated between 0.17 and 0.18. 0.18 was more frequent. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{h2o_speedup_ccsd.eps}}
\caption{The speedup, S, as a function of number of processors for a CCSD iteration on $H_2O$ with the 6-311++G(3df,3pd) basis set.}
\label{fig:ccsd_h2o_time_speedup}
\end{center}
\end{figure}

We also test a system of two water molecules. We use the same basis set, to achieve a system of exactly twice the size. We are only interested in the performance of our program, so we can place the two molecules in any location. The raw performance data is included in the table.

\begin{center}
\begin{tabular}{ l c}
	\hline
  	P & CCSD iteration time [s] \\ \hline
  	1 & 300   \\ 
  	2 & 225   \\ 
  	4 & 120   \\
  	8 &  59.0  \\ 
  	16 & 30.2   \\ 
  	32 & 15.6   \\ 
  	64 & 8.3  \\
  	128 & 4.8 \\ \hline
  	\\
	\end{tabular}
\end{center}


\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{ccsd_iter_2_time.eps}}
\caption{Time per iteration of a CCSD run on $(H_2O)_2$ with the 6-311++G(3df,3pd) basis set using $2^p$ processors.}
\label{fig:ccsd_2h2o_time_per_iter}
\end{center}
\end{figure}

In figure \ref{fig:ccsd_2h2o_time_per_iter} we see some weird behaviour when going from one to two CPUs. This is due to the sub-optimal work distribution noted in section \ref{problem_part_ccsd_parallel}. However from four to eight CPUs, and higher, we do not have this problem. This is because the sub-optimal distribution does not get worse with an increased number of CPUs, it stays at the same sub-optimal level. We still have a good performance with time per iteration approaching a value close to zero, but with an optimal work distribution in part 1 of the implementation we would approach zero even faster. How sub-optimal the distribution is depends on the system. However distribution of part 2 and 3 is always optimal. 

\newpage


\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{ccsd_scaling_2.eps}}
\caption{The speedup, S, as a function of number of processors for a CCSD iteration on $(H_2O)_2$ with the 6-311++G(3df,3pd) basis set.}
\label{fig:ccsd_2h2o_time_speedup}
\end{center}
\end{figure}

\newpage

\section{Potential Energy Plots}
Here we present two potential energy plots for the HF and BH molecules. We are in agreement with the benchmarked values in  Ref.\cite{potential_energy_citation_plots}. Plots are presented in figures \ref{fig:bhpoten} and \ref{fig:hfpoten}. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{BH_CCSD_HF_plot.eps}}
\caption{Energypotential for BH Molecule, RHF and RHF-CCSD}
\label{fig:bhpoten}
\end{center}
\end{figure}

CCSD generally improve our results. However there are some features with our CCSD calculations we would like to investigate further. We here presents plots of the number of iterations as a function of R for the HF molecule. We also plot the correction to the HF energy, $E_{CCSD}$, as a function of R. \\ 


\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=\textwidth]{hf_pot_en_plot_kek.eps}}
\caption{Potential energy of HF Molecule. RHF and RHF-CCSD}
\label{fig:hfpoten}
\end{center}
\end{figure}

\newpage

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{hf_iterationplot.eps}}
\caption{HF Molecule}
\label{fig:hfiter}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{hf_correctionplot.eps}}
\caption{HF Molecule}
\label{fig:hfcorr}
\end{center}
\end{figure}

We notice the number of iterations required for self consistency increase when moving away from the equilibrium geometry. If we move to far away the CCSD correction begin either diverging or oscillating between solutions. We have plotted the number of iterations required for self consistency in figure \ref{fig:hfiter}. The CCSD correction energy is plotted in figure \ref{fig:hfcorr}. From figure \ref{fig:hfiter} we see that CCSD is an equilibrium geometry method. It does not always work outside of equilibrium. If the molecule is to far away from equilibrium we can expect a diverging or oscillating CCSD energy. 

\chapter{Results}
In this chapter we present our new results. We will experiment with our code, test its performance against existing software and look into features we found interesting during development. We will provide a contribution to the advancement of computational chemistry and specifically the Computational Physics Group at UiO. 

\section{Single Atoms For Future Reference}
In this section we do calculations on single atoms. Our hope is that these results may be used as a benchmark for future calculations, using different methods. Since Pople basis sets are not available for all atoms, we can only perform reliable calculations where they are available. \\

For even number electrons we will use RHF as a referance for CCSD, or CCSDT. For odd number electrons we will use UHF. In UHF we will assume we have one extra electron with spin up, relative to the number of spin down. \\

We will where available provide a reference energy from Jorgen Hoberget, comparing our results to his DMC calculations, Ref.\cite{dmc_jorgens_resultater_master}. 

\begin{center}
\begin{tabular}{ c c c c c c }
	\hline
  	Z & Atom & Basis Set & Method & Energy & DMC \\ \hline
  	1 & H & 6-311++G(3df,3pd) & UHF & -0.499818
  	&  \\
  	2 & He & 6-311G** & RHF-CCSD & -2.89057 
  	& -2.9036(2) \\ \hline
  	3 & Li & 6-311++G(2d,2p) & UHF & -7.4321 
  	& \\
  	4 & Be & 6-311++G(2d,2p) & RHF-CCSDT & -14.6341 & -14.657(2) \\
  	5 & B & 6-311++G(2d,2p) & UHF & -24.5313& \\
  	6 & C & 6-311++G(2d,2p) & RHF-CCSDT & -37.7383 &\\
  	7 & N & 6-311++G(2d,2p) & UHF & -54.3402 &\\
  	8 & O & 6-311++G(2d,2p) & RHF-CCSD & -74.884& \\
  	9 & F & 6-311++G(2d,2p) & UHF & -99.4014& \\
  	10 & Ne & 6-311++G(2d,2p) & RHF-CCSDT & -128.798 & 128.765(4) \\  \hline
  	11 & Na & 6-311++G(2d,2p) & UHF & -161.847& \\
  	12 & Mg & 6-311++G(2d,2p) & RHF-CCSDT & -199.774 & -199.904(8)\\
  	13 & Al & 6-311++G(2d,2p) & UHF & -241.874& \\
  	14 & Si & 6-311++G(2d,2p) & RHF-CCSD & -289.014 &\\
  	15 & P & 6-311++G(2d,2p) & UHF & -340.68& \\ 
  	16 & S & 6-311++G(2d,2p) & RHF-CCSD & -397.692 &\\
  	17 & Cl & 6-311++G(2d,2p) & UHF & -459.476& \\
  	18 & Ar & 6-311++G(2d,2p) & RHF-CCSD & -527.056& -527.30(4) \\
  	\hline
  	19 & K & 6-311++G(2d,2p) & UHF & -559.15& \\
  	20 & Ca & 6-311++G(2d,2p) & RHF-CCSD & -677.096& \\ \hline
  	32 & Ge & 6-311G** & RHF-CCSD & -2075.66& \\
  	33 & As & 6-311G** & UHF & -2234.12& \\
  	34 & Se & 6-311G** & RHF-CCSD & -2400.17& \\
  	35 & Br & 6-311G** & UHF & -2572.36 &\\
  	36 & Kr & 6-311G** & RHF-CCSD & -2752.44& -2749.9(2) \\
  	\hline
  	\\
	\end{tabular}
\end{center}


\section{Methods}
We will test all our methods. In coupled cluster there are two approximations, a truncated basis set and the limitation of how many excitations are included. In the benchmark chapter we looked into convergence with respect to the basis set. Now we also want to check convergence when we include higher order excitations. \\

We start with the LiH molecule. We will use the largest Pople type basis set, 6-311++G(3dp,3df). We will use Ref.\cite{very_accurate_lih_poten} as a reference. The equilibrium distance given in this paper is R = 3.015. \\

The LiH molecule contains four electrons. Here two can be considered core electrons and two valence electrons. 
 
\begin{center}
\begin{tabular}{ l c c}
	\hline
  	Method & Correction & Energy \\ \hline
  	HF & - &  -7.986 376 7\\
  	CCSD     &  -0.053 444 1  & -8.039 820 8 \\ 
    CCSDT-1a &  -0.053 536 7  & -8.039 913 4 \\ 
  	CCSDT-1b &  -0.053 536 9  & -8.039 913 6 \\
  	CCSDT-2  &  -0.053 537 0  & -8.039 913 7        \\ 
  	CCSDT-3  &  -0.053 537 4  & -8.039 914 1        \\ 
  	CCSDT-4  &  -0.053 557 3  & -8.039 934 0    \\ 
  	CCSDT    &  -0.053 555 5  &  -8.039 932 2    \\ 
  	\cite{very_accurate_lih_poten}, est $E_0$ & - & -8.070 548 0 \\ \hline
  	\\
	\end{tabular}
\end{center}

We also perform calculations on the BH molecule using all our methods. We are interested in a small area around the equilibrium distance found previously in figure \ref{fig:bhpoten}. We will compare RHF, CCSD, CCSDT-1a and CCSDT. We also include MP2 calculations performed with LSDALTON for comparison. Results are available in figures \ref{fig:zom1} and \ref{fig:zom2}. 

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{zoomed_BH_figure.eps}}
\caption{BH Potential Energy Minimum plot. Methods in use RHF, MP2, CCSD and CCSDT. Distances in Angstrom.}
\label{fig:zom1}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{zoomed_BH_figure2.eps}}
\caption{BH Potential Energy Minimum plot. Methods in use CCSD, CCSDT-1a and CCSDT. Distances in Angstrom.}
\label{fig:zom2}
\end{center}
\end{figure}






We have performed calculations with a resolution of 0.05 Angstrom. Around the minimum the resolution is increased to 0.01 Angstrom.  For this system we see different methods provide different results. \\

The improvement from HF to MP2 is larger than from MP2 to CCSD. Also we see the improvement from MP2 to CCSD is larger than the improvement from CCSD to CCSDT. We see a convergence of the energy with the respect to the contributions included. This is illustrated in figure \ref{fig:zom3}. \\

We also notice that for HF and MP2 the energy minimum is R = 1.22 Angstrom. For CCSD and CCSDT the minimum is shifted to 1.23 Angstrom. 

\newpage

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{zoom_BH_figure3.eps}}
\caption{BH Energy minimum with respect to Method plot. Methods in use HF, MP2, CCSD, CCSDT-1a and CCSDT. MP2 results from LSDALTON. }
\label{fig:zom3}
\end{center}
\end{figure}

\section{CCSD Performance}
In this section we will test our CCSD performance against ACESIII. There are a few challenges when making this test, especially that we do not have implemented frozen core in our optimized AOtoMO transformation. We cannot perform the AOtoMO calculation with the unoptimized algorithm for the systems available on the ACESIII benchmark site. They are simply to large. We need the optimized AOtoMO algorithm, and as such we cannot perform frozen core calculations. The benchmarked values on ACESIII benchmark site are with frozen core. \\

However since CCSD calculations depend on $n_o$ and $n_v$, we will create a system of the same size and compare performance. We will benchmark against a $C_{20}$ calculation with ACESIII from 2009, \cite{aces_non_ref}. We are interested in program performance. Their calculations are performed with the following system specifics: \\

\begin{center}
\begin{tabular}{ l c}
	\hline
  	System 1 & $C_{20}$\\
  	Electrons & 120 \\
  	Basis Set & cc-pVDZ\\
  	Contracted GTOs & 280 \\
  	Frozen Orbitals & 20 core\\
  	$n_o$ for CCSD & 80\\
  	$n_v$ for CCSD & 440\\ \hline
  	\\
	\end{tabular}
\end{center}

The hardware in use here is the Kraken supercomputer, \cite{kraken_citation}. In 2009 Kraken was the most powerful supercomputer in the world managed by an academia. Each node on Kraken has the following hardware specifications

\begin{center}
\begin{tabular}{ l }
	\hline
  	Two 2.6 GHz six-core AMD Opteron processors (Istanbul)\\
    12 cores\\
    16 GB of memory\\
    Connection via Cray SeaStar2+ router\\ \hline
  	\\
	\end{tabular}
\end{center}

We will use a slightly different system, but we have ensured $n_v$ and $n_o$ is approximately the same as for the prior system.

\begin{center}
\begin{tabular}{ l c}
	\hline
  	System 2 & $C_{13} H_2$\\
  	Electrons & 80 \\
  	Basis Set & 6-311G** \\
  	Contracted GTOs & 259 \\
  	Frozen Orbitals & 0 \\
  	$n_o$ for CCSD & 80 \\
  	$n_v$ for CCSD & 438 \\ \hline
  	\\
	\end{tabular}
\end{center}

We perform our calculations on the abel computing cluster, \cite{abel_po_g_citation1234567}. Both ACESIII and our calculations are performed with 200 CPUs. We note our system has two virtual orbitals less. \\

\begin{center}
\begin{tabular}{ l c}
	\hline
  	Code & Time per Iteration [min] \\
  	ACESIII & 1.6 \\
  	Our Results & 4.17 (250 s) \\ \hline
	\end{tabular}
\end{center}

We note the AOtoMO transformation took 140 seconds. This is less than one iteration in our program, and close to one iteration in ACESIII. \\

Compiler flags can optimized our code further. Compiler flags allows the compiler to perform further optimizations, and can sometimes affect the resulting energy. We want the performance to be a realistic measure of how fast our program can produce real results. For this reason we did not use any flags in the compiler until now, but we will try this calculation once more using the flags recommended by abel. 

\begin{lstlisting}
CFLAGS = -pipe -O2 -xAVX -mavx -fomit-frame-pointer -fno-alias -Wall -W
\end{lstlisting}
These compiler settings produce a runtime per iteration of $T_P = $ 230 s (3.8 m).


\chapter{Conclusion}
In this chapter we will review our results and benchmarks. We will review each part of our code and draw conclusions of positive and negative features of our implementation. 

\section{Performance}

\subsection{HF}
Our HF code was not particularly optimized. But when we wish to run a Coupled Cluster calculation afterwards, we do not need an extremely fast HF code. \\

The parallel performance plot, figure \ref{fig:hf_performance_stuff_jesus1234}, exhibit some strange behaviour. We double the number of processors and more than double performance. This seems like super linear scaling, but it is not the case. In HF we distributed the jobs of calculating $\langle i j | k l \rangle$. These are calculated in a sequence, and if we have more than one CPU each CPU gets its own sequence of integrals to calculate. However the amount of memory needed for each integral remains constant. Our implementation excludes super linear scaling, as each individual integral should be calculated in the same amount of time. \\

We did not average our results over multiple runs, and the same CPU does not always produce the same performance even for the same exact code. \\

Even though we did not spend as much time optimizing the RHF performance, it is currently good enough for the largest CCSD calculations we have available. We did not make much use of UHF. \\

DIIS was implemented and used, but mostly to achieve convergence when needed. If we were to calculate two electron integrals on the fly, DIIS would be much more important. In this situation if we reduced the number of iterations required to half, we would also reduce the runtime of our program to half. 

\subsection{AOtoMO}
Our AOtoMO performance is good. We have been unable to find anyone beating this performance. The literature from the authors point of view is lacking high performance full AOtoMO transformation algorithms and performance results. \\

Ref.\cite{aotomo_2_cite} lists some results. They do calculations on ethylene ($C_2 H_4$) with cc-pVTZ basis set. This is 114 orbitals. We will compare their results to our results for 130 AOs, listed in section \ref{aotomoperformancetesting}. We take the best results from Ref.\cite{aotomo_2_cite}. We also list how much faster our algorithm runs, relative to theirs. \\

\begin{center}
\begin{tabular}{ l c c c }
	\hline
  	CPUs & \cite{aotomo_2_cite}, 114 AOs [s] & Our, 130 AOs [s] & Fraction \\ \hline
  	1 & 699.30 & 123.55 & 5.7   \\ 
  	2 & 382.10 & 69.64  & 5.5   \\ 
  	4 & 210.90 & 37.73  &  5.6  \\
  	8 & 132.50 &  25.51 &  5.2  \\ 
  	16 & 99.4  & 17.11  & 5.8   \\ 
  	32 & - & 14.52 & -  \\ \hline
  	\\
	\end{tabular}
\end{center}

The AOtoMO transformation comes to dominate calculations in Ref.\cite{aotomo_2_cite}. With this new algorithm this can be avoided. The article also mentions the use of point group symmetry, which will be discussed later. This makes coupled cluster calculations faster, and can potentially speed up the AOtoMO transformation also. No mention was made if they applied it in the transformation. The difference in number of AOs is also significant. We did perform calculations for 118 AOs in section \ref{aotomoperformancetesting}. With 64 CPUs the runtime was half that of 130 AOs with 64 CPUs. With this in mind the outperformance is likely much higher than the fractions listed here. \\

The key feature of our algorithm is memory distribution. The non distributed memory scales as $N^3$. However when the number of AOs become so large that $N^3$ does not fit in memory, we can introduce an additional intermediate after two quarter transformatinos. This would be of size $N^4$ and distributed in memory. Then we would have non distributed memory scaling as $N^2$, and would only need a large amount of CPUs. The amount of CPUs in use for the largest CCSD calculations make our algorithm for the full AOtoMO transformation feasible, in most cases. \\

However not all post HF methods require the full AOtoMO transformation. Much work has been done to avoid the transformation partially or entirely. If performing for example MP2 calculations only a partial AOtoMO can be performed with good results. See Refs.\cite{non_aotomo_1}, \cite{non_aotomo_2} and \cite{non_aotomo_3}. In general, CCSD without approximations is one of the faster post-HF method that do require the full AOtoMO transformation. For these reasons we conclude that our AOtoMO transformation is extremely effective. For most calculations the transformation become insignificant in terms of runtime. 

\subsection{CCSD}
The CCSD implementation is quite large, and performs very well. Our results are approximate 2.5 times slower than ACESIII for the benchmarked system. However CCSD scales as $N^6$, and 2.5 times slower for this sized system is not bad. We also have several obvious further optimizations available. This will be discussed in detail in the chapter Future Prospects. ACES has been optimized over more than 20 years. This is the first implementation of CCSD in quantum chemistry on the computational physics group. We do conclude that it is very possible to beat ACESIII and other high performance software in performance. \\

The parallel performance is sub optimal. Especially when going from 1 to 2 CPUs, as seen in figure \ref{fig:ccsd_2h2o_time_per_iter}. The scaling from 4 to 8 CPUs and so on is much better. This is because the work distribution stays at the same sub-optimal level once a given number of CPUs are in use. The sub-optimal distribution is discussed in section \ref{problem_part_ccsd_parallel}, and a solution is proposed in chapter Future Prospects. \\

The serial algorithm is extremely fast. Serial performance for CCSD is not as interesting because whatever the performance is, we will need multiple CPUs for large systems when using our memory distributed model. We wanted to make comparisons of performance with LSDALTON, but LSDALTON applies approximations to enable faster calculations on larger systems. For this reason we could only benchmark smaller systems down to the final decimal. Also we were unable to print time per iteration in LSDALTON. . \\

In section \ref{ccsdserialperformance_ppp} we also noticed a better performance on the office computer relative to abel. This may be caused by armadillo having problems working with Intel MKL. This can be a problem because armadillo does not return error if the linking is not correct, it only returns suboptimal performance. Also we did not use any special compiler settings in the makefile to optimize performance. \\

However a single CPU on abel is not expected to outperform significantly. The great thing with supercomputers is the massive number of CPUs available. As such we cannot expect any huge performance gain if this is a problem. This situation remains slightly unclear. \\

In section \ref{ccsdserialperformance_ppp} we also see how our optimized implementation compares to the unoptimized, but factorized, implementation. We see the scaling of performance is improved dramatically. Small optimizations for small systems turn into huge performance gains when the system size increase.

\section{Basis sets and Convergence}
We notice the CCSD energy was not converged in figure \ref{fig:convplot}. Similar results are noted in Refs.\cite{basis_set_convergence_important_citation} and \cite{hmeiding}. We expected HF to converge faster than CCSD based on the discussion in section \ref{imporatant_ccsd_convergence_ting}. The CCSD energy for $H_2O$ decreased with approximately 0.1 a.u. from the second largest to the largest Pople type basis set. The HF energy is much better converged. \\

We also note that the CCSD calculation scales with the number of virtual and occupied orbitals. The hotspot in HF is the two electron integrals, which are based on the number of primitive GTOs. This is a fundamental difference. Pople type basis sets use Cartesian GTOs. For $l = 2$ we then have 6 orbitals. Spherical GTOs only have 5 orbitals for $l = 2$. Since the values for $\alpha_i$ are shared for these orbitals with $l = 2$, it would not make much of a difference in an optimized HF code. It does however make a huge difference in CCSD, as more orbitals results in more time consuming calculations. \\

For these reasons we conclude that Spherical GTOs are better suited for CCSD. The basis sets that should be used are of the type cc-pVDZ, cc-pVTZ, aug-cc-pVDZ etc. The implementation of spherical GTOs is a bit more complicated according to the literature.

\section{CCSDT}
Our implementation of CCSDT is not optimized and as such we can only run smaller systems. \\

For LiH we noticed the energy did not change much from CCSD to any of the CCSDT-n methods. This system only has two valence electrons. This means any triple excitation involves excitation of a core electron. For this system the error from the limited basis set is dominating. \\

For the BH molecule we noticed a larger contribution from triple excitations. Since our CCSDT implementation is not optimized we are limited to smaller system sizes. CCSDT calculations for the BH molecule potential energy curve took one day on abel. \\

In figure \ref{fig:convplot} we see the energy as a function of basis set size. In figure \ref{fig:zom3} we see the energy as a function of method in use. These are the sources of errors in coupled cluster theory. Both of them are converging towards some value. The Coupled Cluster method is in theory exact if we do not truncate and have an infinite large basis set. Our results are in agreement. From the two figures we conclude the largest errors for the systems we have studied come from the limited basis set, since this plot shows a worse convergence. \\

We also noticed the equilibrium distance between the atoms changed with different methods. We conclude that not only the energy is converging, but also other physical properties. \\

Also interesting is how good the CCSDT-1a model recovers the triple excitation contribution for the BH molecule. This method is much faster with lower scaling. Also the new T3 amplitudes in CCSDT-1a does not depend on the old T3 amplitudes. This means we do not need to store them, since they are not reused. 

\chapter{Future Prospects}
In this chapter we discuss future prospect for all parts of our implementation. We will give our answer on how to enable larger and faster calculations. Our main focus will be on the CCSD part of our implementation, but most or all of the discussion will apply also to CCSDT. 

\section{Hartree Fock Performance}
Our implementation of HF is not optimized. There are several obvious optimizations available to enable larger molecules in calculations. The most obvious is making use of $\alpha_i$ values. \\

Several orbitals share $\alpha_i$ values. As such the two electron integrals over primitives can be reused. In our implementation we have included a few comments on which values are changed and which are constant when the only change from one orbital to another is the value of $c_i$ and the composition of angular momentum, $m$, $n$ and $o$. The comments are included throughout the code, but mainly in the function Electron\_Electron\_Interaction\_Single in the Hartree\_Integrals class. \\

The way our implementation make use of symmetries and especially our parallel implementation made it difficult to make use of this optimization for now. \\

Also a huge performance gain is available through integral screening. Integral screening is described in detail in Refs. \cite{helg2} and \cite{helg3}. These references also describe other HF optimizations.

\section{CCSD Performance}
The CCSD implementation is quite large and is constructed from scratch. We had no prior experience with coupled cluster. For this reason we are left with several potential optimizations. These will be discussed here.

\subsection{Work Distribution}
The first is the work distribution of part 1. This is suboptimal, but the only reason it is was time constraints. The work distribution should be optimized further, by using the shifts already introduced. \\

In our program we first distribute the jobs of calculating terms in $[W_1]$. When this is complete, the CPU that got the last job with this array is noted. When we start distributing jobs for $[W_2]$, the first job will go to the next CPU after the one that got the last job in $[W_1]$. This should be continued, so that any irregularities in job distribution is removed as much as possible. \\

We implemented this shift on one of the arrays in part 1 of parallel CCSD implementation. From figure \ref{fig:ccsd_2h2o_time_per_iter} we see the terrible scaling from 1 to 2 CPUs. This is normally were we expect the best scaling. Close to 100\% speed up is not uncommon in CCSD from 1 to 2 CPUs. If this work distribution is fixed we will likely see a fantastic increase in performance.

\subsection{Memory Access}
The next optimization we did not have time to implement is the problem with field in armadillo. Armadillo field is inefficient. When we removed fields and implemented one dimensional vectors as intermediates in our AOtoMO transformation we experienced a significant performance gain. Most resent armadillo patches contains some optimizations on fields. The code will likely improve in performance if armadillo is simply patched.  \\

However even matrix and vector are not as efficient as normal C malloc() syntax. If we want the fastest CCSD program in the world armadillo should be removed and replaced with normal arrays of doubles. This would also remove the additional mapping and reduce the number of lines in our implementation. This would not only make it faster, but also more readable. 

\subsection{All Out Memory Distribution \label{comgroups}}
Also the memory distribution can be improved. If we were to activate OpenMP in our implementation, and hold the largest arrays as shared in memory over 16 CPUs, we would have 64 GB of memory available. This is enough to perform almost all the benchmark calculations from ACESIII and NWChem, \cite{aces_non_ref}, \cite{nwchem_non_ref}. However we did not experience any performance gain using OpenMP. Also a shared memory model is limited to the number of CPUs on a node. We want a memory distribution algorithm where the systems that fit in memory are arbitrarily large for an infinite number of CPUs. \\

This can only be achieved if we either read everything from disk or distribute both arrays in all matrix multiplications. We will illustrate with an example on the term

\begin{equation}
t_{ij}^{ab} \leftarrow \sum_{cd} I_{ab}^{cd} \tau_{ij}^{cd} .
\end{equation}
In our current implementation we hold $\tau_{ij}^{cd}$ in memory on all CPUs, and distribute $I_{ab}^{cd}$ among P CPUs. The memory of these two terms scales as

\begin{equation}
M = \frac{n_v^2}{P} \times n_v^2 + n_v^2 n_o^2 \label{memory11} .
\end{equation}
We want to define communication groups. We will split the P CPUs in U communication groups, where each group consists of O CPUs. 

\begin{equation}
P = U \times O .
\end{equation}

We want O CPUs to distribute $\tau_{ij}^{cd}$ among themselves, and we want to distribute $I_{ab}^{cd}$ among the U communication groups. The memory in this situation would scale as 

\begin{equation}
M = \frac{n_v^2}{U} \times n_v^2 + \frac{n_o^2}{O} \times n_v^2 . \label{memory12} 
\end{equation}
If memory scales as Eq. \eqref{memory11} $n_v^2 n_o^2$ is the limiting factor, and we still have highly limited available system size. However if memory scales as Eq. \eqref{memory12} we can optimize U and O to fit any system in memory, if $P \rightarrow \infty$. The limitation will be $n_v^2$.  \\

This turns into an optimization problem with two variational parameters U and O. We also have the constraint that $P = U \times O$. We want to optimize the distribution of O and U based on performance, and ensure that the system fits in memory. \\

Using communication groups we cannot use the same communication model as is implemented now. We will still need all new amplitudes $t_{ij}^{ab}$ in all communication groups. To achieve this there must be a second communication group implemented. This group will hold all processors that will store the same value of the new T2 amplitudes. This will be U number of CPUs. We can then use a all-to-all personalized communication, like MPI\_Alltoallw. In this communication one CPU from each of the second communication groups will take part. Then each of the second communication group must perform a one-to-all broadcast of this information. \\

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.9\textwidth]{distributed_u_p_memory.jpeg}}
\caption{Illustration of parallel model. CPUs in row A share common communication rank. CPUs in column B share common calculation rank. One CPU has a unique set of the two.}
\label{fig:p_u_model}
\end{center}
\end{figure}

After the one-to-all broadcast, each of the first defined communication groups will have complete access to the new T2 amplitudes. As such they will be ready for the new iteration. This complication is necessary because we assumed we could not store the full new T2 amplitudes on one CPU. For this reason we cannot use the same communication group as defined in the calculations, because this would require first an all-to-all broadcast of the full new amplitudes followed by a personalized one-to-all scatter. This would exceed memory limits. \\

This algorithm sounds like a good idea, but it is complicated. Also we initialize more communications. However, we would like to propose a simple and straight forward way to implement this. First we should define two new variables on each processor. These will be dependant upon its rank, and will be unique to each CPU. This is illustrated in figure \ref{fig:p_u_model}.

\begin{lstlisting}
Calculation_Rank = (int) rank / P;
Communication_Rank = rank % (P + 1);
\end{lstlisting}
These integer division ensure multiple CPUs will get the same value for Calculation\_Rank. The modulus with (P+1) ensures all CPUs with the same Calculation\_Rank get unique values for Communication\_Rank. We want all CPUs with the same Communication\_Rank to store the same values for $t_{ij}^{ab}$. This can be achieved by giving the same displacement and number of bytes to recieve in MPI\_Alltoallw, which is an all-to-all scatter function used in our implementation earlier. The displacement should then depend upon the new variable Communication\_Rank, and not the normal MPI rank. \\

Also the additional work distribution can be achieved from adding additional if tests.

\begin{lstlisting}
// Psudocode for optimal work distribution
// in new proposed implementation
for (int a = 0; a < unocc_orb; a++){
   for (int b = 0; b < unocc_orb; b++){
       Work_ID1 = F(a,b); // Some function of a,b
       if (Work_ID1 % P == Calculation_Rank){
         // All CPUs with common 
         // calculation rank enter here
         
         for (int i = 0; i < n_Electrons; i++){
            for (int j = 0; j < n_Electrons; j++){
               Work_ID2 = F(i,j); // Some function of i,j
               if (Work_ID2 % U == Calculation_Rank){
                  // Perform calculation. Only one CPU will have 
                  // unique set of 
                  // calculation and communication rank.
               }
            }
         }
      }
   }
}
\end{lstlisting}
In other word the implementation does not really need communication groups to work. The proposed algorithm here will also likely improve performance, in accordance to the principles stated in section \ref{work_dist_section_1341}. If we distribute work based on four indexes we have a greater number of smaller jobs to distribute. More jobs is easier to distribute evenly, as such the scaling will likely improve. We also avoid broadcasting information, and instead scatter. \\

Since a scattering of information can be achieved ineffectively by a broadcast, it is safe to assume a scatter is more effectively implemented in MPI.

\subsection{Removing mapping of two electron integrals}
Also the additional mapping introduced to only store the single bar integrals can be removed. This was implemented early on. At this point the author did not believe a full parallel distributed CCSD algorithm was achievable in one master thesis. This will increase performance slightly. However if the points noted in section \ref{comgroups} is implemented, only storing single bar integrals is preferred from a memory perspective. 

\subsection{Removing Four Dimensional Arrays}
In CCSD it is quite normal to contract two and two indexes. The term $t_{ij}^{ab}$ is often not stored as a four dimensional array. It is stored as two dimensional. Often indexes a and b are contracted into an index e, where e can be defined as

\begin{equation}
e = a \times n_v + b .
\end{equation}
Also i and j can be contracted into an index k, where k can be defined as

\begin{equation}
k = i \times n_o + j .
\end{equation}
Using these definitions $t_{ij}^{ab}$ can be calculated directly as a matrix-matrix multiplication. This is way more efficient in terms of memory accessing. We have not jet taken advantage of this optimization, but we have not excluded it. \\

Implementing this will be slightly more complex when we use the memory distributed model. If we go back to the prior example

\begin{equation}
t_{ij}^{ab} \leftarrow I_{ab}^{cd} \tau_{ij}^{cd} .
\end{equation}
The standard implementation is to define this as a matrix-matrix multiplication where the matrices are of size $n_v^2 \times n_v^2$ and $n_o^2 \times n_v^2$. This will result in a matrix of size $n_o^2 \times n_v^2$. We distribute work based on a and b. This will mean if we contracted the indexes we would distribute work based on the index e. Each processor would perform a matrix-matrix multiplication where the matrices are of size $\frac{n_v^2}{P} \times n_v^2$ and $n_v^2 \times n_o^2$. This would result in a matrix of size $\frac{n_v^2}{P} \times n_o^2$. \\

If the communication groups in section \ref{comgroups} is used the matrices for matrix-matrix multiplication would be of size $\frac{n_v^2}{U} \times n_v^2$ and $\frac{n_o^2}{O} \times n_v^2$. The resulting matrix would be of size $\frac{n_v^2}{U} \times \frac{n_o^2}{O}$. Using matrix-matrix multiplications directly will likely cause a fantastic performance gain, and also enable better use of OpenMP. OpenMP was ineffective because the arrays are to small. In a real matrix-matrix multiplication the arrays will be larger. \\

Also in our current implementation the number of calls to external math libraries scales as $N^4$. If we implemented two dimensional arrays the number of calls to external math libraries would be constant. This would likely improve the scaling of performance with respect to system size. 

\subsection{Read From File \label{dont_do_it}}
Another solution to memory concerns is to read the largest array from file. We go back to the previous example

\begin{equation}
t_{ij}^{ab} \leftarrow I_{ab}^{cd} \tau_{ij}^{cd} .
\end{equation}
The variable to read from file is $I_{ab}^{cd}$. This was previously distributed in memory and with a large number of CPUs it did not take up much space. However, if we read it from file and have it available on all CPUs we can distribute $\tau_{ij}^{cd}$ in memory. This enables much larger calculations. 

\subsection{Summary}
Several further optimizations has been proposed for our already highly optimized CCSD code. Most of the optimizations require much work. With the exception of section \ref{dont_do_it} and maybe section \ref{comgroups}, all proposed optimizations will likely increase performance further. The author believes it is highly likely that a better performance than ACESIII is achievable with only these proposed optimizations. \\

To the authors knowledge this would be the first time a highly optimized CCSD program is constructed using only a few collective OpenMPI function calls. It is much more popular to use algorithms such as Cannons Algorithm, Refs.\cite{cannon_citation_po}, \cite{mpi_boka_cite_referanse}, or 2.5D communication which has become increasingly popular in recent years, Refs.\cite{most_effective_ccsd_dude3} and \cite{superduperartikkel}. \\

Another popular optimization in recent years is chunksize. Please see Ref.\cite{aotomo_2_cite} and references therein.

\section{Further Method Development}
We have developed most of our methods with a spin restriction. Removing this spin restriction is of course possible, and required for certain situations. For CCSDT a good article is Ref.\cite{ccsdt_unrestricted_spin}. CCSD unrestricted spin can be found in references therein. The CCSDTQ method is described in detail in Ref.\cite{nevin_ccsdtq_duden}. \\

Also possible is merging the code presented here with Henriks, Ref.\cite{hmeiding}. This would create a more complete package of quantum chemistry methods. We would have available RHF, UHF, RMP2, UMP2, RMP3, UMP3, RCCSD, RCCSDT and RCCSDT-n methods. R and U signifies restricted and unrestricted spin. In this section we will focus mainly on new approximations that can enable much larger CCSD calculations. We have not examined these approximations in detail, but we will list a few approximations we found interesting. 

\subsection{Natural Orbitals}
The largest CCSD calculations performed today take advantage of natural orbitals, Ref.\cite{uno_cas_stas}. Simply put, natural orbitals involve dividing orbitals into three segments. Occupied, active and unoccupied. The occupied orbitals are the core orbitals. We can assume these are always occupied. This limits the size of the occupied space, or in other words reduce $n_o$, since we do not need to include them in CCSD calculations. \\

Unoccupied orbitals in this context are orbitals so highly excited they are rarely occupied. This is approximated to never occupied, and the orbitals are ignored when performing CCSD. This limits the virtual space, or in other words reduce $n_v$. \\

We are left with the active orbitals. These are the valence orbitals and a few orbitals close to the same energy level. However natural orbitals are not as straight forward. Please see Ref.\cite{natural_orbitals_ccsd}.  The performance gain here is high because we limit both $n_v$ and $n_o$. CCSD scales as $n_o^2 n_v^4$, so we will have much better performance. 

\subsection{Frozen Core}
The Frozen Core approximation in quantum chemistry is not the exact same as one would assume. To freeze the core means to ignore the lowest energetic molecular orbitals. If we want to freeze six core orbitals, we ignore the first six molecular orbitals. The hope is that these will correspond to the core orbitals, and in most cases they do. 

\subsection{Local Coupled Cluster}
Natural Orbitals can be combined with Local Coupled Cluster, to make Local-Natural-Orbitals Coupled Cluster. Local Coupled Cluster takes advantage of the density as an approximation to achieve performance gains, Ref.\cite{natural_orbitals_ccsd2}. The performance gains available here is in magnitudes of $n$. 

\subsection{Point Group Symmetry}
Points Group Symmetry is especially impressive. These are specified as C1, D1, D2 etc. They reference a certain symmetry. If this symmetry exist in the molecule we can give it as input to our program and take advantage of it, Ref.\cite{point_group_1}. \\

Dependant upon the extent of the symmetry, this can provide good performance gains, Refs.\cite{ccsd_fac2}, \cite{ccsd_fac3}. The performance gain available here is not in a magnitude of $n$, but as a fraction. 

\subsection{Divide and Conquer}
Also divide and conquer is a popular approximation. This is relevant to matrix-matrix multiplications. A large matrix-matrix multiplication can be divided into smaller matrices. Each of the smaller matrices can be calculated in parallel, Ref.\cite{divide_and_conquer}. \\

Combining all four approximations would give huge performance gains, but they are approximations. Limiting $n_v$, $n_o$, reducing the scaling and also adding a fraction in front of wall time, makes large calculations possible for CCSD. The largest the author is aware of is approximately 2000 AOs. \\

\subsection{Summary}

Most of these approximations can be implemented without changing the CCSD implementation itself. Natural orbitals is an optimization on the basis from HF. Optimizing the basis from HF is practically reducing $n_v$ and $n_o$. It is not uncommon to see $n_v$ reduced by 50-80\%. This reduction is made after a HF calculation and makes the much larger CCSD calculations possible. If we reduce 2000 AOs by 80\% we only have $(n_o + n_v) = 800$ in CCSD. 2000 AOs with a decent sized basis set can be a molecule of 60-70 atoms. \\

Natural Orbitals is a way of improving the basis. Preliminary estimates are made on how likely a virtual orbital is to be occupied, and a cut off is defined where all orbitals beneath this value is ignored. This means 200 Natural Orbitals will likely give much better results than 200 Atomic Orbitals. \\

We can combine these approximations with the already fast CCSD implementation presented in this thesis. 

\section{Other Methods}
Also other methods can be of interest. 

\subsection{DFT}
Density Functional Theory (DFT) is the most commonly used method in quantum chemistry. The cost is similar to that of HF and the results are generally more accurate. DFT performs particularly well for metals. We had no Pople type basis sets available for metals. Ref.\cite{dft_citation_urio}. 

\subsection{Monte Carlo}
Monte Carlo (MC) has been applied with a HF and even CCSD probability distribution. Ref.\cite{monte_carlo_citation_urio}. 

\subsection{Perturbation}
M\"oller Plesset calculations provide a slightly less accurate energy correction than CCSD. However it can potentially be calculated much faster. These methods are named MP2, MP3 and so on, dependant on what terms are included. Ref.\cite{hmeiding} describes the details. 

\subsection{CCn}
The center for computational and theoretical chemistry (CCTC) group at the university of Oslo has developed its own coupled cluster methods. One method, CC2, is closely related to CCSD. CC3 is closely related to CCSDT and so on. Ref.\cite{cc3_metode_artikl}. These are iterative methods. 

\subsection{Approximate Contributions}
CCSD(t) is considered the gold standard in quantum chemistry, Ref.\cite{ccsdt_approxim_method}. CCSD(t) contains CCSD plus an approximation to higher order amplitudes. Also CCSD[T], formerly known as T(CCSD), is an approximation of triples excitation. Any excitation can be approximated. This is methods like CCSDT(Q), CCSD(TQ) and so on. 

\section{Getting Closer to $E_0$}
We concluded that the largest source of error is now the limited basis set. This was due to the fact that larger Pople basis sets does not exist on the EMSL website. Although it is possible to perform calculations using Cartesian GTOs with basis sets designed for spherical GTOs, it is not optimal. Future work should be implementing spherical GTOs, rather than including higher order excitations. \\

Since the largest error currently is the basis set we need a larger basis set to get more accurate results. A larger basis set does require more calculations. However a more accurate method generally require higher scaling. Both of these options are evils in terms of computational resources, but a larger basis set is the lesser of the two. This means the outlook for quickly achieving more accurate results is very bright. 

\chapter{Appendix A}

Listed here are some useful MPI functions that deals with communication. Their input can be found on google. This list can be used to figure out what functions exist, and a few basics of what they do. 


\section{MPI Functions}

MPI\_Bcast() \\
One to all operation. Broadcasts information from one node to all nodes. \\

MPI\_Reduce() \\
All to one operation. Gathers information from all nodes onto one node. \\

MPI\_Scatter() \\
One to all operation. Sends unique information of same size to each node, including self. \\

MPI\_Gather() \\
All to one operation. Gathers unique information of same size from each node, including self, exact opposite of scatter. \\

MPI\_Scatterv() \\
One to all operation. Sends unique information of optionally different size to each node, including self. \\

MPI\_Gatherv() \\
All to one operation. Gathers unique information of optionally different size from all nodes onto one node. \\

MPI\_Recv() \\
One to one operation. Receives information. Use with MPI\_Send. \\

MPI\_Send() \\
One to one operation. Sends information. Use with MPI\_Recv. \\

MPI\_Isend() \\
One to one operation. Same as MPI\_Send but this stores information in a buffer and continues with operations. Then sends the buffer once link with MPI\_Irecv has been established. \\

MPI\_Irecv() \\
One to one operation. Receive information, use with MPI\_Isend. \\

MPI\_Get\_Count() \\
Returns precise count of data items received. \\

MPI\_Sendrecv() \\
One to one operation. Sends and receives information between two nodes. This is faster than initiating two sends and two receives. \\

MPI\_Sendrecv\_replace() \\
One to one operations. Sends and receives information between two nodes, and replaces information. This is faster than initiating two sends and two receives. \\

MPI\_Barrier() \\
Synchronizes all nodes. \\

MPI\_Allgather() \\
All to all operation. Gathers information from all nodes like MPI\_Gather, but then spreads information to all nodes. Faster than MPI\_Gather followed by MPI\_Bcast, but same results. \\

MPI\_Alltoall() \\
All to all operation, broadcasts information from all nodes to all nodes. Faster than MPI\_Bcast from all nodes individually, but same result. \\

MPI\_Alltoallv() \\
All to all operation, like MPI\_Alltoall but more freedom. \\

MPI\_Alltoallw() \\
All to all operation. Each node can have different size of buffer for broadcast, very general function. \\

MPI\_Comm\_Split() \\
Creates a new communication group (communicator). The all in "All to all operation" refer to all nodes in a selected communication group, like MPI\_COMM\_WORLD. Using this function, "all" can be limited to certain nodes, reducing the number of nodes that take part and increasing speed. \\

MPI\_Comm\_size() \\
Returns how many nodes are a part of a communicator. 



\section{MPI Datatypes}

Here are a few basic types in MPI, that is used in the communication and mostly not listed in the function descriptions. \\

MPI\_CHAR , signed char \\

MPI\_SHORT , signed short int \\

MPI\_INT , signed int \\

MPI\_LONG , signed long int \\

MPI\_UNSIGNED\_CHAR , unsigned char \\

MPI\_UNSIGNED\_CHAR , unsigned short int \\

MPI\_UNSIGNED , unsigned int \\

MPI\_UNSIGNED\_LONG , unsigned long int \\

MPI\_FLOAT , float \\

MPI\_DOUBLE , double \\

MPI\_LONG\_DOUBLE , long double \\

MPI\_PACKED \\

MPI\_BYTE 







\bibliographystyle{spr-mp-nameyear-cnd}

\begin{thebibliography}{}

\bibitem[1]{ref111111}
Lecture series as part of the Georgia Tech's Summer LEctures Series in Theoretical Chemistry. Available for free online at this url: \\
\url{http://vergil.chemistry.gatech.edu/opp/summer-lectures.html}

\bibitem[2]{ref222222}
Book, Modern Quantum Chemistry. \\
Introduction to Advanced Electronic Structure Theory \\
Attila Szabo, Neil. S. Ostlund

\bibitem[3]{sadragly}
MA Thesis University of Oslo 2014.\\
Bridging Quantum Mechanics and Molecular Dynamics with Artificial Neural Networks. \\
S. A. Dragly. 

\bibitem[4]{hmeiding}
MA Thesis University of Oslo 2014. \\
Ab Initio Studies of Molecules \\
H. M. Eiding.

\bibitem[5]{mhmobarhan}
MA Thesis University of Oslo 2014. \\
From Quantum to Molecular, A Review of Gaussian Basis Sets in Ab Initio Molecular Dynamics. \\
M. H. Mobarhan.

\bibitem[6]{KoopmansTheorem}
Koopmans Theorem. Some general information from an online lecure available for free online. \\
\url{http://www.youtube.com/watch?v=5T_HyShi9W0}

\bibitem[7]{helg1}
Book. Molecular electronic-structure theory. Wiley, 2013. \\
T. Helgaer, P. Jorgensen and J. Olsen.

\bibitem[8]{helg2}
Slides. Quantum Chemistry and Molecular Properties: Molecular Integral Evaluation. 2006. \\
T. Helgaker. \\
\url{http://folk.uio.no/helgaker/talks/SostrupIntegrals_06.pdf}

\bibitem[9]{helg3}
Slides. Molecular Integral Evaluation. 2010. \\
T. Helgaker. \\
\url{http://folk.uio.no/helgaker/talks/SostrupIntegrals_10.pdf}

\bibitem[10]{helg4}
One- and two-electron integrals over cartesian gaussian functions. \\
Larry E. McMurchie and Ernest R. Davidson. \\
Journal of Computational Physics. Volume 26, Issue 2, February 1978, Pages 218-231.

\bibitem[11]{helg5}
Computation of electron repulsion integrals involving contracted Gaussian basis functions. \\
John A. Pople and Warren J. Hehre. \\
Journal of Computational Physics. Volume 27, Issue 2, May 1978, Pages 161-168.

\bibitem[12]{hermite_stuffies}
The Hermite Polynomials from wikipedia. \\
\url{http://en.wikipedia.org/wiki/Hermite_polynomials}

\bibitem[13]{emsl_stuffies}
The Role of Databases in Support of Computational Chemistry Calculations. \\
Feller, D., J. Comp. Chem., 17(13), 1571-1586, 1996. 

\bibitem[14]{emsl_stuffies2}
Basis Set Exchange: A Community Database for Computational Science. \\
Schuchardt, K.L., Didier, B.T., Elsethagen, T., Sun, L., Gurumoorthi, V., Chase, J., Li, J., and Windus, T.L.\\
J. Chem. Inf. Model., 47(3), 1045-1052, 2007, doi:10.1021/ci600510j.

\bibitem[15]{emsl_stuffies3}
The EMSL Basis Set Exchange website: \\
\url{https://bse.pnl.gov/bse/portal}

\bibitem[16]{ccsdbook11}
Book. An Introduction to Coupled Cluster Theory for Computational Chemists. \\
T. Daniel Crawford and Henry S. Schaeffer III. 

\bibitem[17]{ccsd_fac1}
Conference. \\
20-24 May 2013 \\ 
Cyclops Tensor Framework: reducing communication and eliminating load imbalance in massively parallel contractions. \\
Parallel and Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on \\
DOI 10.1109/IPDPS.2013.112 \\
Edgar Solomonik, Devin Matthews, Jeff Hammond, James Demmel. \\
URL: \\ \url{http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6569864}

\bibitem[18]{ccsd_fac2}
Use of molecular symmetry in coupled-cluster theory. \\
J. Chem. Phys. 87,411 (1987).
P. Carsky, L. J. Schaad, B. A. Hess, M. Urban, and J. Noga.

\bibitem[19]{ccsd_fac3}
A direct product decomposition approach for symmetry exploitation in many-body methods. I. Energy calculations.
Chem. Phys. 94 (6), 15 March 1991.
John F. Stanton, J\"urgen Gauss, John D. Watts, and Rodney J. Bartlett.

\bibitem[20]{ccsd_fac4}
A direct atomic orbital driven implementation of the coupled cluster singles and doubles (CCSD) model. \\
Chemical Physics Letters 228 (1994) 233-238 . \\
Henrik Koch, Ove Christiansen, Rika Kobayashi, Poul Jorgensen , Trygve Helgaker

\bibitem[21]{CCSDT-ref3}
Towards a full CCSDT model for electron correlation. CCSDT-n models. \\
Jozef Noga and Rodney J. Barlett. \\
Chemical Physics Letters. Volume 134, Issue 2, 20 February 1987, Pages 126-132.


\bibitem[22]{CCSDT-ref1}
The full CCSDT model for molecular electronic structure. \\
Jozef Noga, Rodney J. Bartlett \\
The Journal of Chemical Physics 86, 7041 (1987).

\bibitem[23]{CCSDT-ref2}
Erratium: The full CCSDT model for molecular electronic structure. \\
J. Noga and R. J. Bartlett. \\
The Journal of Chemical Physics 86, 7041 (1987).

\bibitem[24]{CCSDT-ref4}
Book. Meny Body Methods in Chemistry and Physics: MBPT and Coupled Cluster Theory (Cambridge Molecular Science) \\
Isaiah Shavitt and Rodney J. Bartlett

\bibitem[25]{CCSDT-ref5}
Comparison of coupled cluster methods which include the effects of connected triple
excitations \\
Gustavo E. Scuseria and Timothy J. Lee
Citation: \\
The Journal of Chemical Physics 93, 5851 (1990); doi: 10.1063/1.459684

\bibitem[26]{non_refer_numba1}
MA Thesis University of Oslo 2012. \\
Studies of quantum dots: Ab initio coupled-cluster analysis using opencl and gpu programming.  \\
Christoffer Hirth.

\bibitem[27]{non_refer_numba2}
New developments in molecular orbital theory. \\
Rev. Mod. Phys., 23:69, 1951. \\
C. C. J. Roothaan.

\bibitem[28]{non_refer_numba3}
Self-consistent orbitals for radicals. \\
J. Chem. Phys., 22:571, 1954. \\
J. A. Pople and R. K. Nesbet.

\bibitem[29]{CCSDT-ref10}
Exact solution (within a double-zeta basis set) of the schrodinger electronic equation for water \\
Paul Saxe, Henry F Shaefer III \\
Chemical Physics Letters \\
Volume 79 Issue 2, 15 April 1981 Pages 202-204 

\bibitem[30]{armadillo-ref1}
Armadillo source: \\
\url{http://arma.sourceforge.net/}

\bibitem[31]{fci_h2_molecule_stuff}
Improved Theoretical GroundState Energy of the Hydrogen Molecule \\
W. Kolos and L. Wolniewicz \\
The Journal of Chemical Physics 49, 404 (1968); doi: 10.1063/1.1669836

\bibitem[32]{first_row_diatomic_referance_stuff}
Multiconfiguration wave functions for quantum monte carlo calculations of first-row diatomic molecules \\
Claudia Filippi and C. J. Umrigar, \\
J. Chem. Phys, 105, 213, 1996. 

\bibitem[33]{nwchem_non_ref}
NWChem Benchmarks Website: \\
\url{http://www.nwchem-sw.org/index.php/Benchmarks}

\bibitem[34]{aces_non_ref}
ACESIII Benchmarks Website: \\
\url{http://www.qtp.ufl.edu/PCCworkshop/PCCbenchmarks.html}

\bibitem[35]{aotomo_1_cite}
A parallel algorithm for generating molecular integrals over MO basis sets. \\
Kazuto Nakata, Tadashi Murase, Toshihiro Sakuma and Toshikazu Takada. \\
Journal of Computational and Applied Mathematics 149 (2002) 351 - 357

\bibitem[36]{aotomo_2_cite}
Automatic Code Generator for Many-Body Electronic Structure Methods: The Tensor Contraction Engine. \\
Alexander A. Auer, Gerald Baumgartner, David E. Bernholdt, Alina Bibireata, Venkatesh Choppella, Daniel Cociorv, Xiao yang Gao, Robert Harrison, Sriram Krishnamoorth, Sandh ya Krishnan, Chi-Chung Lam, Qingda Lu, Marcel Nooijen, Russell Pitzer, J. Ramanujam, P. Sadayappan and Alexander Sibiryak. \\
\url{http://www.csc.lsu.edu/~gb/TCE/Publications/Bartlett-MolPhys06.pdf} \\
Submitted to Molecular Physics, R. J. Bartlett Festschrift Special Issue

\bibitem[37]{non_cite_dadada10}
Coupled cluster algorithms for networks of shared memory parallel processors \\
Jonathan L. Bentz, RyanM.Olson, Mark S. Gordon, Michael W. Schmidt,Ricky A. Kendall. \\
Computer Physics Communications 176 (2007) 589-600

\bibitem[38]{boys_referanse_1}
Evaluation of the Boys function using analytical relations \\
I. I. Guseinov, B. A. Mamedov. \\
Journal of Mathematical Chemistry, Vol. 40, No. 2, August 2006. \\
DOI: 10.1007/s10910-005-9023-3

\bibitem[39]{boys_referanse_2}
On the evaluation of Boys functions using downward recursive relation. \\
B. A. Mamedov. \\
Journal of Mathematical Chemistry Vol. 36, No 3, July 2004.

\bibitem[40]{ccsd_benchmark_url_stuff}
Testrun with NWChem for \\
a) $H_2O$ molecule and STO-3G \\
b) $CH_3$ with UHF \\
\url{http://institute.loni.org/NWChem2012/documents/tce-session.pdf}

\bibitem[41]{LSDALTON_CITATION}
LSDALTON Program. \\
K. Aidas, C. Angeli, K. L. Bak, V. Bakken, R. Bast, L. Boman, O. Christiansen, R. Cimiraglia, S. Coriani, P. Dahle, e. K. Dalskov, U. Ekstr\"om, T. Enevoldsen, J. J. Eriksen, P. Ettenhuber, B. Fern\`andez, L. Ferrighi, H. Fliegl, L. Frediani, K. Hald, A. Halkier, C. H\"attig, H. Heiberg, T. Helgaker, A. C. Hennum, H. Hettema, E. Hjertnes, S. Host, I. M. Hoyvik, M. F. Iozzi, B. Jansik, H. J. Aa. Jensen, D. Jonsson, P. Jorgensen, J. Kauczor, S. Kirpekar, T. Kjergaard, W. Klopper, S. Knecht, R. Kobayashi, H. Koch, J. Kongsted, 
A. Krapp, K. Kristensen, A. Ligabue, O. B. Lutnes, J. I. Melo, K. V. Mikkelsen, R. H. Myhre, C. Neiss, C. B. Nielsen, P. Norman, J. Olsen, J. M. H. Olsen, A. Osted, M. J. Packer, F. Pawlowski, T. B. Pedersen, P. F. Provasi, S. Reine, Z. Rinkevicius, T. A. Ruden, K. Ruud, V. Rybkin, P. Salek, C. C. M. Samson, A. Sanchez de Meras, T. Saue, S. P. A. Sauer, B. Schimmelpfennig, K. Sneskov, A. H. Steindal, K. O. Sylvester-Hvid, P. R. Taylor, A. M. Teale, E. I. Tellgren, D. P. Tew, A. J. Thorvaldsen, L. Thogersen, O. Vahtras, M. A. Watson, D. J. D. Wilson, M. Ziolkowski and H. Aagren. \\
The Dalton quantum chemistry program system. \\
WIREs Comput. Mol. Sci. (doi: 10.1002/wcms.1172). \\
Dalton, a molecular electronic structure program, Release DALTON2013 (2013), see http://daltonprogram.org. \\
lsDalton, a linear scaling molecular electronic structure program, Release DALTON2013 (2013), see http://daltonprogram.org. \\
URL: \\
\url{http://daltonprogram.org} 

\bibitem[42]{non_refer_4151111111}
Simplewick, Contractions for Latex:\\
\url{http://www.fzu.cz/~kolorenc/tex/simplewick/}

\bibitem[43]{non_refer_5145151232252}
Algorithm2e, Algorithms in Latex: \\
\url{http://mlg.ulb.ac.be/files/algorithm2e.pdf}

\bibitem[44]{non_refer_1516378378578234678623634787890}
Latex Package: tikz, Matrix Representation in Latex.

\bibitem[45]{c20coordinatesarticlezz}
Isomers of C20. Dramatic effect of gradient corrections in density functional theory. \\
Krishnan Raghavachari, D. L. Strout, G. K. Odom, G. E. Scuseria, J. A. Pople, B. G. Johnson and P. M. W. Gill. \\
Chemical Physics Letters, Volume 214, Issues 3-4, 5 November 1993, Pages 357-361.

\bibitem[46]{c20coordinatesarticlezz10}
Website. Same coordinate as in  \cite{c20coordinatesarticlezz} presented in easier way. \\
\url{http://altair.physics.ncsu.edu/projects/c20/c20.html}

\bibitem[47]{c20article_cite_this}
Ab initio calculations of bowl, cage, and ring isomers of C20 and C20~. \\
Wei An, Yi Gao, Satya Bulusu and Xiao Cheng Zeng. \\
Free article from the Published Research - Derpartment of Chemistry at DigitalCommons@University of Nabraska. \\
\url{http://digitalcommons.unl.edu/chemzeng/20/}

\bibitem[48]{openmpi_cite}
OpenMPI Website: \\
\url{http://www.open-mpi.org/}

\bibitem[49]{mpi_boka_cite_referanse}
Book. Introduction to Parallel Computing, Second Edition. \\
Ananth Grama, Anshul Gupta, George Karypis and Vipin Kumar. 

\bibitem[50]{openmp_citation_po_g}
OpenMP Website. 
\url{http://openmp.org/wp/}

\bibitem[51]{openblas_citation}
OpenBLAS Website.
\url{http://www.openblas.net/}

\bibitem[52]{mkl_citation}
Intel MKL Website.
\url{https://software.intel.com/en-us/intel-mkl}

\bibitem[53]{intelduden_citeation}
Youtube Video Lectures:
\url{http://www.youtube.com/watch?v=cMWGeJyrc9w}

\bibitem[54]{extra_if_interested_stuffff}
Accelerating the Convergence of the Coupled-Cluster approach. The use of the DIIS method. \\
G. E. Scuseria, T. J. Lee, H. F. Schaefer III. \\
Chemical Physics Letters, Volume 130, Number 3, 3 October 1986. 

\bibitem[55]{extra_if_interested_2fff}
Full configuration interaction potential energy curves for breaking bonds to hydrogen: An assessment of single-reference correlation methods. \\
Antara Dutta and C. David Sherrill. \\
Journal of Chemical Physics, Volume 118, Number 4, 22 January 2003.

\bibitem[56]{lagrange_duden}
M.L. Boas. Mathematical Methods in the Physical Sciences. Wiley, 2005

\bibitem[57]{tce_citation_numbah_10}
Tensor Contraction Engine: Abstraction and Automated Parallel Implementation of Configuration-Interaction, Coupled-Cluster, and Meny-Body Perturbation Theories. \\
So Hirata. \\
J. Phys. Chem. A. 2003, 107, 9887 - 9897.

\bibitem[58]{nevin_ccsdtq_duden}
A multi-reference coupled-cluster method using a single-reference formalism. \\
PhD Thesis University of Arizona (1991). \\
Nevin Oliphant. 

\bibitem[59]{boys_original_work_stuff}
Boys S F 1950 Proc. R. Soc. A 200 54

\bibitem[60]{ccsd_minne_distribuert_double_bar_artikkel}
Coupled cluster algorithms for networks of shared memory parallel processor.\\
Jonathan L. Bentz, Ryan M. Olson, Mark S. Gordon, Michael W. Schmidt, Ricky A. Kendall.\\
Computer Physics Communications 176 (2007) 589-600.\\
doi:10.1016/cpc.2007.03.001

\bibitem[61]{book_om_advancements_ccsd}
Book. Recent Progress in Coupled Cluster Methods volume 11.\\
Theory and Applications.\\
J. Leszczynski.\\
P. Carsky, J. Paldus, J. Pittner.

\bibitem[62]{most_effective_ccsd_dude}
2.5D Communication in CCSD, 2012 seminar free available on youtube. \\
Edgar Solomonik. \\
\url{https://www.youtube.com/watch?v=MCl5fGvVaLU}

\bibitem[63]{most_effective_ccsd_dude2}
2.5D Communication in CCSD, 2013 slides. \\
Edgar Solomonik. \\
\url{http://www.eecs.berkeley.edu/~solomon/talks/matrix-seminar-2013.pdf} 

\bibitem[63]{most_effective_ccsd_dude3}
2.5D Communication in CCSD, 2014 slides.\\
Edgar Solomonik. \\
\url{http://www.eecs.berkeley.edu/~solomon/talks/ctf-ExMatEx-mar-2014.pdf}

\bibitem[64]{sslrs_citation1}
An efficient reformulation of the closed shell coupled cluster single and double excitation (CCSD) equations. \\
Gustavo E. Scuseria, Curtis L. Janssen, Henry F. Schaefer III. \\
Journal of Chemical Physics 89, 7382 (1988).
doi: 10.1063/1.455269

\bibitem[65]{sslrs_citation2}
The closed shell coupled cluster single and double excitation (CCSD) model for the description of electron correlation. A comparison with configuration interaction (CISD) results. \\
Gustavo E. Scuseria, Andrew C. Scheiner, Timothy J. Lee, Julia E. Rice and Henry F. Schaefer III. \\
Journal of Chemical Physics 86, 2881 (1987). \\
doi: 10.1063/1.452039

\bibitem[66]{diis_citation1}
Improved SCF Convergence Acceleration. \\
P. Pulay. \\
Journal of Computational Chemistry, Vol 3. No 4, 556-560 (1982).\\
Available for free at: \\
\url{http://onlinelibrary.wiley.com/doi/10.1002/jcc.540030413/pdf}

\bibitem[67]{diis_citation2}
Convergence Acceleration of Iterative Sequences. The case of SCF Iteration. \\
P. Pulay. \\
Chemical Physics Letters. Volume 73, number 2. 15 July 1980. 

\bibitem[68]{diis_citation3}
The Mathematics of DIIS. \\
\url{http://vergil.chemistry.gatech.edu/notes/diis/node2.html}

\bibitem[69]{abel_po_g_citation1234567}
We want to acknowledge the help received from the Department for Research Computing at USIT,
the University of Oslo IT-department.

\bibitem[70]{potential_energy_citation_plots}
Hybrid correlation models based on active-space partitioning: \\
Correcting second-order M\"oller-Plesset perturbation theory for bond-breaking reactions. \\
Arteum D. Bochevarov and C. David Sherrill. \\
The Journal of Chemical Physics, 122, 234110, (2005).

\bibitem[71]{non_aotomo_1}
Second Order M\"oller-Plesset Perturbation Theory in
the Condensed Phase: An Efficient and Massively
Parallel Gaussian and Plane Waves Approach. \\
Mauro Del Ben, J\"urg Hutter, and Joost VandeVondele. \\
Journal of Chemical Theory and Computation, 8(11):4177-4188,

\bibitem[72]{non_aotomo_2}
QUICKSTEP: Fast and accurate density functional calculations
using a mixed Gaussian and plane waves approach. \\
Joost VandeVondele, Matthias Krack, Fawzi Mohamed, Michele Parrinello, Thomas Chassaing, J\"urg Hutter.\\
Computer Physics Communications 167 (2005) 103-128

\bibitem[73]{non_aotomo_3}
Linear scaling local coupled cluster theory with density fitting.
Part I: 4-external integrals. \\
Martin Schutz and Frederick R. Manby.
Phys. Chem. Chem. Phys., 2003, 5, 3349-3358

\bibitem[74]{kraken_citation}
Kraken Cray Supercomputer. \\
\url{http://www.nics.tennessee.edu/computing-resources/kraken}

\bibitem[75]{cannon_citation_po}
Cannon Algorithm Explained. \\
\url{http://www.youtube.com/watch?v=sB-Dh4DsOy0}

\bibitem[76]{ccsdt_unrestricted_spin}
The Journal of Chemical Physics 93, 6104 (1990); doi: 10.1063/1.45900  

\bibitem[77]{uno_cas_stas}
The unrestricted natural orbital complete active space (UNO-CAS) method: An inexpensive alternative to the complete active space-self-consistent-field (CAS-SCF) method. \\
Josep M. Bofill and Peter Pulay. \\
J. Chem. Phys. 90, 3637 (1989).

\bibitem[78]{natural_orbitals_ccsd}
An efficient and near linear scaling pair natural orbital based local coupled cluster method. \\
Christoph Riplinger and Frank Neese. \\
The Journal of Chemical Physics 138, 034106 (2013); doi: 10.1063/1.4473581

\bibitem[79]{natural_orbitals_ccsd2}
Journal of Chemical Physics; Sep2013, Vol. 139 Issue 9, p094105.

\bibitem[80]{point_group_1}
Point Group Symmetry Tutorial. \\
\url{http://symmetry.otterbein.edu/tutorial/pointgroups.html}

\bibitem[81]{basis_set_convergence_important_citation}
Basis set dependence of the doubly hybrid XYG3 functional. \\
Igor Ying Zhang, Yi Luo and Xin Xu. \\
The Journal of Chemical Physics, 133, 104105, (2010). 

\bibitem[82]{divide_and_conquer}
Divide and Conquer PDF From Berkeley University. \\
Lester Mackey, Ameet Talwalkar, Michael I. Jordan. \\
\url{http://www.cs.berkeley.edu/~ameet/dfc.pdf}

\bibitem[83]{dft_citation_urio}
Course on UIO in DFT. \\
\url{http://www.uio.no/studier/emner/matnat/fys/FYS-MENA4111/}

\bibitem[84]{monte_carlo_citation_urio}
Quantum Monte Carlo with Coupled-Cluster wave functions. \\
Alessandro Roggero, Abhishek Muherjee and Francesco Pederiva. \\
Phys Rev B 88, 115138 (2013). \\
Journal reference: 	Phys Rev B 88, 115138 (2013) \\
DOI: 	10.1103/PhysRevB.88.115138 \\
Cite as: 	arXiv:1304.1549 [nucl-th]

\bibitem[85]{very_accurate_lih_poten}
Very accurate potential energy curve of the LiH molecule. \\
Wei-Cheng Tung1, Michele Pavanello and Ludwik Adamowicz. \\
J. Chem. Phys 134, 06117 (2011): http://dx.doi.org/10.1063/1.3554211

\bibitem[86]{cc3_metode_artikl}
Henrik Koch et al. J. Chem. Phys. 106 (5), 1 February 1997

\bibitem[87]{ccsdt_approxim_method}
Krishnan Raghavachari, Gary W. Trucks, John A. Pople and Martin Head-Gordon
Chem. Phys. Lett. 157, 479. 1989.

\bibitem[88]{dmc_jorgens_resultater_master}
Quantum Monte-Carlo Studies of Generalized Many-Body Systems. \\
MA Thesis. \\
Jorgen Hoberget.\\
June 2013.

\bibitem[89]{superduperartikkel}
Electrical Engineering and Computer sciences. \\
University of California at Berkeley. \\
August 2, 2014. \\
Technical Report No. UCB/EECS-2014-143. \\
\url{http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.pdf}

\bibitem[90]{slater_determinant_new_citation}
\url{http://vergil.chemistry.gatech.edu/notes/hf-intro/node4.html}

\bibitem[91]{rothaan_new_citation}
\url{http://www.phys.sinica.edu.tw/TIGP-NANO/Course/2011_Spring/classnotes/CMS_20110511.pdf}

\bibitem[92]{my_own_code_shiiiit}
Code developed during master thesis:\\
\url{https://github.com/otnorli/CCSD_PARALLEL}

 \end{thebibliography}

\end{document}
