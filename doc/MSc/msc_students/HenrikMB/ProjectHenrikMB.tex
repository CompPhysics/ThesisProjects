\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.2in]{geometry}
% Math
\usepackage{amsmath}
\usepackage{physics}
\usepackage{isotope}

\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}


% Formatting
\usepackage{float}
\usepackage{graphicx}


\title{Quantum Computing and Quantum Machine Learning \\ \vspace{5px} Boltzmann Machines and AutoEncoders \large \\ \vspace{20px} \large Master of Science thesis project}
% \author{Henrik Modahl Berenstein}
\date{December 1,  2022}

\begin{document}
\maketitle

\section*{Introduction and overview}

\textbf{Quantum Computing and Machine Learning} are two of the most promising
approaches for studying complex physical systems where several length
and energy scales are involved.  Traditional many-particle methods,
either quantum mechanical or classical ones, face huge dimensionality
problems when applied to studies of systems with many interacting
particles. To be able to define properly effective potentials for
realistic molecular dynamics simulations of billions or more
particles, requires both precise quantum mechanical studies as well as
algorithms that allow for parametrizations and simplifications of
quantum mechanical results. Quantum Computing offers now an
interesting avenue, together with traditional algorithms, for studying
complex quantum mechanical systems. Machine Learning on the other hand
allows us to parametrize these results in terms of classical
interactions. These interactions are in turn suitable for large scale
molecular dynamics simulations of complicated systems spanning from
subatomic physics to materials science and life science.

\section*{Specific tasks and milestones}

\subsection*{Boltzmann machines, from classical ones to quantum Boltzmann machines (Classical and Quantum Machine Learning).}

Boltzmann Machines (BMs) offer a powerful framework for modeling
probability distributions.  These types of neural networks use an
undirected graph-structure to encode relevant information.  More
precisely, the respective information is stored in bias coefficients
and connection weights of network nodes, which are typically related
to binary spin-systems and grouped into those that determine the
output, the visible nodes, and those that act as latent variables, the
hidden nodes.

Furthermore, the network structure is linked to an energy function
which facilitates the definition of a probability distribution over
the possible node configurations by using a concept from statistical
mechanics, i.e., Gibbs states.  The aim of BM training is to learn a
set of weights such that the resulting model approximates a target
probability distribution which is implicitly given by training data.
This setting can be formulated as discriminative as well as generative
learning task.  Applications have been studied in a large variety of
domains such as the analysis of quantum many-body systems, statistics,
biochemistry, social networks, signal processing and finance

BMs are complicated to train in practice because the loss
function's derivative requires the evaluation of a normalization
factor, the partition function, that is generally difficult to
compute.  Usually, it is approximated using Markov Chain Monte Carlo
methods which may require long runtimes until convergence

Quantum Boltzmann Machines (QBMs) are a natural adaption of BMs to the
quantum computing framework. Instead of an energy function with nodes
being represented by binary spin values, QBMs define the underlying
network using a Hermitian operator, normally a parameterized
Hamiltonian, see references [1,2] below.

Here we will focus on classification problems such as the famous MNIST
data set which contains handwritten numbers from $0$ to $9$.  This can
serve as a starting point.  More data sets can be included at a later
stage.  The next project parallels this but replaces Boltzmann
machines with Autoencoders. Alternatively, one can study autoencoders only. It is possible to collaborate with other students on the code developments.


\subsection*{From Classical Autoenconders to Quantum Autoenconders and classification problems (Classical and Quantum Machine Learning).}

As an alternative to Boltzmann machines, one can implement and study quantum Autoencoders.
Classical autoencoders are neural networks that can learn efficient
low dimensional representations of data in higher dimensional
space. The task of an autoencoder is, given an input $x$, is to map
$x$ to a lower dimensional point $y$ such that $x$ can likely be
recovered from $y$. The structure of the underlying autoencoder
network can be chosen to represent the data on a smaller dimension,
effectively compressing the input.

Inspired by this idea, we can alternatively, following references [4-6]
below, introduce Quantum Autoencoders to compress a particular
dataset like the famous MNIST data set which contains handwritten
numbers from $0$ to $9$.


The specific task here 

\begin{enumerate}
    \item Spring 2023: Start writing a code for classical Boltzmann machines (alternatively autoencoders) and apply these to a classification problem like the MNIST data set. Finalize courses
    \item Fall 2023: Extend the program from spring 2023 to quantum Boltzmann machines and/or Quantum variational autoencoders.
    \item Spring 2024: Perform studies of various datasets and start wirting and finalizing thesis.
\end{enumerate}
The thesis is expected to be handed in May/June 2024

\textbf{Literature:}

\begin{enumerate}
\item Amin et al., \textbf{Quantum Boltzmann Machines}, Physical Review X \textbf{8}, 021050 (2018).

\item Zoufal et al., \textbf{Variational Quantum Boltzmann Machines}, ArXiv:2006.06004.

\item Maria Schuld and Francesco Petruccione, \textbf{Supervised Learning with Quantum Computers}, Springer, 2018.

\item Carlos Bravo-Prieto, \textbf{Quantum autoencoders with enhanced data encoding}, see \href{{https://arxiv.org/pdf/2010.06599.pdf}}{\nolinkurl{https://arxiv.org/pdf/2010.06599.pdf}} 

\item Khoshaman et al., \textbf{Quantum Variational autoencoders}, ArXiv:1802.05779.
  
\item Jonathan Romero et al, \textbf{Quantum autoencoders for efficient compression of quantum data}, see \href{{https://arxiv.org/pdf/1612.02806.pdf}}{\nolinkurl{https://arxiv.org/pdf/1612.02806.pdf}}


\end{enumerate}


\end{document}






