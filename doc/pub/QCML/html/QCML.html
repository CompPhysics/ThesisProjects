<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html QCML.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=QCML
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Quantum Machine Learning">
<title>Quantum Machine Learning</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Quantum Computing and Machine Learning',
               2,
               None,
               'quantum-computing-and-machine-learning'),
              ('Support vector machines', 2, None, 'support-vector-machines'),
              ('Quantum Neural Networks and Variational Circuits',
               2,
               None,
               'quantum-neural-networks-and-variational-circuits'),
              ('Boltzmann machines', 2, None, 'boltzmann-machines'),
              ('Specific tasks and milestones',
               3,
               None,
               'specific-tasks-and-milestones'),
              ('Literature', 3, None, 'literature')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Quantum Machine Learning</h1>
</center>  <!-- document title -->

<!-- author(s): Master of Science thesis project -->
<center>
<b>Master of Science thesis project</b> 
</center>
<!-- institution(s) -->
<br>
<center>
<h4>May 4, 2025</h4>
</center> <!-- date -->
<br>
<h2 id="quantum-computing-and-machine-learning">Quantum Computing and Machine Learning </h2>

<p><b>Quantum Computing and Machine Learning</b> are two of the most promising
approaches for studying complex physical systems where several length
and energy scales are involved.  
</p>

<p>Quantum computing is an emerging area of computer science that
leverages the principles of quantum mechanics to perform computations
beyond the capabilities of classical computers. Unlike classical
computers, which use bits to represent data as 0s or 1s, quantum
computers use quantum bits, or qubits. Qubits can exist in multiple
states simultaneously (superposition) and can be entangled with one
another, allowing quantum computers to process vast amounts of
information in parallel.
</p>

<p>These unique properties enable quantum computers to tackle problems
that are currently intractable for classical systems, such as complex
simulations in chemistry and physics, optimization problems, and
large-scale data analysis.
</p>

<p>Quantum machine learning (QML) is an interdisciplinary field that
combines quantum computing with machine learning techniques. The goal
is to enhance the performance of machine learning algorithms by
utilizing quantum computing&#8217;s capabilities.
</p>

<p>In QML, quantum algorithms are developed to process and analyze data
more efficiently than classical algorithms. This includes tasks like
classification, regression, clustering, and dimensionality
reduction. By exploiting quantum phenomena, QML has the potential to
accelerate machine learning processes and handle larger datasets more
effectively.
</p>

<p>Quantum computing and QML hold promise for various applications, including:</p>

<ol>
<li> Drug Discovery: Simulating molecular structures to expedite the development of new medications.</li>
<li> Financial Modeling: Optimizing portfolios and detecting fraudulent activities through complex data analysis.</li>
<li> Artificial Intelligence: Enhancing machine learning algorithms for faster and more accurate predictions.</li> 
</ol>
<p>As quantum hardware continues to advance, the integration of quantum
computing into practical applications is becoming increasingly
feasible, opening up for  a new era of computational possibilities.
</p>

<p>This thesis project deals with the study and implementation of quantum
machine learning methods applied to classical machine learning data
for supervised learning.
The methods we will focus on are
</p>
<ol>
<li> Support vector machines and quantum support vector machines</li>
<li> Neural networks and quantum neural networks and possibly (if time allows)</li>
<li> Classical and quantum Boltzmann machines</li>
</ol>
<p>The data sets will span both regression and classification problems,
with an emphasis on simulating time series of relevance for financial
problems. The thesis will be done in close collaboration with Norges
Bank Invenstment Management, Simula Research laboratory and the
University of Oslo.
</p>
<h2 id="support-vector-machines">Support vector machines </h2>

<p>A central model in classical
supervised learning is the support vector machine (SVM), which is a
max-margin classifier.  SVMs are widely used for binary classification
and have extensions to regression problems as well.
They build on statistical learning
theory and are known for finding decision boundaries with maximal
margin.  In particular, SVMs can perform non-linear classification by
employing the kernel trick, which implicitly maps data into a
high-dimensional feature space via a kernel function.
</p>

<p>A Quantum Support Vector Machine (QSVM) replaces the classical kernel
or feature map with a quantum procedure.  In QSVM, classical data
points \( \boldsymbol{x} \) are encoded into quantum states
\( |\phi(\boldsymbol{x})\rangle \) via a quantum feature map (a parameterized
quantum circuit).  The inner product (overlap) between two such states
serves as a quantum kernel, measuring data similarity in a
high-dimensional Hilbert space.
</p>
<h2 id="quantum-neural-networks-and-variational-circuits">Quantum Neural Networks and Variational Circuits </h2>

<p>The Variational Quantum Algorithm (VQA) is a 
Variational Quantum Circuit (VQC), that is  a quantum circuit with tunable
parameters and which is trained using a classical optimizer.  In practice, a
VQC (also called a Parameterized Quantum Circuit (PQC)) is used as a
Quantum Neural Network (QNN): data are encoded into quantum states, a
parameterized circuit is applied, and measurements yield outputs.
For example, it has been shown recently that certain QNNs can exhibit higher
effective dimension (and thus capacity to generalize) than comparable
classical networks , suggesting a potential quantum advantage.
</p>
<h2 id="boltzmann-machines">Boltzmann machines </h2>

<p>Boltzmann Machines (BMs) offer a powerful framework for modeling
probability distributions.  These types of neural networks use an
undirected graph-structure to encode relevant information.  More
precisely, the respective information is stored in bias coefficients
and connection weights of network nodes, which are typically related
to binary spin-systems and grouped into those that determine the
output, the visible nodes, and those that act as latent variables, the
hidden nodes.
</p>

<p>Furthermore, the network structure is linked to an energy function
which facilitates the definition of a probability distribution over
the possible node configurations by using a concept from statistical
mechanics, i.e., Gibbs states.  The aim of BM training is to learn a
set of weights such that the resulting model approximates a target
probability distribution which is implicitly given by training data.
This setting can be formulated as discriminative as well as generative
learning task.  Applications have been studied in a large variety of
domains such as the analysis of quantum many-body systems, statistics,
biochemistry, social networks, signal processing and finance
</p>

<p>Quantum Boltzmann Machines (QBMs) are a natural adaption of BMs to the
quantum computing framework. Instead of an energy function with nodes
being represented by binary spin values, QBMs define the underlying
network using a Hermitian operator, normally a parameterized Hamiltonian, see reference [1] below.
</p>
<h3 id="specific-tasks-and-milestones">Specific tasks and milestones  </h3>

<p>The aim of this thesis is to study the implementation and development of codes for
several quantum machine learning methods, including quantum support vector machines, quantum neural networks and possibly  Boltzmann machines, and possibly other classical machine learning algorithms, on a quantum computer. The thesis consists of three basic steps:
</p>

<ol>
<li> Develop a classical machine code for studies of classification and regression problems.</li>
<li> Compare the results from the classical Boltzmann machine with other deep learning methods.</li>
<li> Develop an implementation of a quantum Boltzmann machine code to be run on existing quantum computers and classical computers. Compare the performance of the quantum Boltzmann machines with exisiting classical deep learning methods.</li>
</ol>
<p>The milestones are:</p>
<ol>
<li> Spring 2025: Develop a code for classical Boltzmann machines to be applied to both classification and regression problems. In particular, the latter type of problem can be tailored to solving classical spin problems like the Ising model or quantum mechanical problems.</li> 
<li> Fall 2025: Develop a code for variational Quantum Boltzmann machines following reference [2] here.  Make comparisons with classical deep learning algorithms on selected classification and regression problems.</li>
<li> Spring 2026: The final part is to use the variational Quantum Boltzmann machines to study quantum mechanical systems. Finalize thesis.</li> 
</ol>
<p>The thesis is expected to be handed in May/June 2026.</p>
<h3 id="literature">Literature </h3>

<ol>
<li> Amin et al., <b>Quantum Boltzmann Machines</b>, Physical Review X <b>8</b>, 021050 (2018).</li>
<li> Maria Schuld and Francesco Petruccione, <b>Supervised Learning with Quantum Computers</b>, Springer, 2018.</li>
<li> Claudio Conti, Quantum Machine Learning (Springer), sections 1.5-1.12 and chapter 2, see <a href="https://link.springer.com/book/10.1007/978-3-031-44226-1" target="_blank"><tt>https://link.springer.com/book/10.1007/978-3-031-44226-1</tt></a>.</li>
</ol>
<!-- ------------------- end of main content --------------- -->
</body>
</html>

