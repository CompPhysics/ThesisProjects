%%
%% Automatically generated file from DocOnce source
%% (https://github.com/hplgit/doconce/)
%%
%%


%-------------------- begin preamble ----------------------

\documentclass[%
oneside,                 % oneside: electronic viewing, twoside: printing
final,                   % draft: marks overfull hboxes, figures with paths
10pt]{article}

\listfiles               %  print all files needed to compile this document

\usepackage{relsize,makeidx,color,setspace,amsmath,amsfonts,amssymb}
\usepackage[table]{xcolor}
\usepackage{bm,ltablex,microtype}

\usepackage[pdftex]{graphicx}

\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

\usepackage{lmodern}         % Latin Modern fonts derived from Computer Modern

% Hyperlinks in PDF:
\definecolor{linkcolor}{rgb}{0,0,0.4}
\usepackage{hyperref}
\hypersetup{
    breaklinks=true,
    colorlinks=true,
    linkcolor=linkcolor,
    urlcolor=linkcolor,
    citecolor=black,
    filecolor=black,
    %filecolor=blue,
    pdfmenubar=true,
    pdftoolbar=true,
    bookmarksdepth=3   % Uncomment (and tweak) for PDF bookmarks with more levels than the TOC
    }
%\hyperbaseurl{}   % hyperlinks are relative to this root

\setcounter{tocdepth}{2}  % levels in table of contents

% prevent orhpans and widows
\clubpenalty = 10000
\widowpenalty = 10000

% --- end of standard preamble for documents ---


% insert custom LaTeX commands...

\raggedbottom
\makeindex
\usepackage[totoc]{idxlayout}   % for index in the toc
\usepackage[nottoc]{tocbibind}  % for references/bibliography in the toc

%-------------------- end preamble ----------------------

\begin{document}

% matching end for #ifdef PREAMBLE

\newcommand{\exercisesection}[1]{\subsection*{#1}}


% ------------------- main content ----------------------



% ----------------- title -------------------------

\thispagestyle{empty}

\begin{center}
{\LARGE\bf
\begin{spacing}{1.25}
Quantum Machine Learning
\end{spacing}
}
\end{center}

% ----------------- author(s) -------------------------

\begin{center}
{\bf Master of Science thesis project${}^{}$} \\ [0mm]
\end{center}

\begin{center}
% List of all institutions:
\end{center}
    
% ----------------- end author(s) -------------------------

% --- begin date ---
\begin{center}
Dec 1, 2020
\end{center}
% --- end date ---

\vspace{1cm}


\subsection*{Quantum Computing and Machine Learning}



\textbf{Quantum Computing and Machine Learning} are two of the most promising
approaches for studying complex physical systems where several length
and energy scales are involved.  Traditional many-particle methods,
either quantum mechanical or classical ones, face huge dimensionality
problems when applied to studies of systems with many interacting
particles. To be able to define properly effective potentials for
realistic molecular dynamics simulations of billions or more
particles, requires both precise quantum mechanical studies as well as
algorithms that allow for parametrizations and simplifications of
quantum mechanical results. Quantum Computing offers now an
interesting avenue, together with traditional algorithms, for studying
complex quantum mechanical systems. Machine Learning on the other hand
allows us to parametrize these results in terms of classical
interactions. These interactions are in turn suitable for large scale
molecular dynamics simulations of complicated systems spanning from
subatomic physics to materials science and life science.

\subsection*{Thesis Project}

Boltzmann Machines (BMs) offer a powerful framework for modeling
probability distributions.  These types of neural networks use an
undirected graph-structure to encode relevant information.  More
precisely, the respective information is stored in bias coefficients
and connection weights of network nodes, which are typically related
to binary spin-systems and grouped into those that determine the
output, the visible nodes, and those that act as latent variables, the
hidden nodes.

Furthermore, the network structure is linked to an energy function
which facilitates the definition of a probability distribution over
the possible node configurations by using a concept from statistical
mechanics, i.e., Gibbs states.  The aim of BM training is to learn a
set of weights such that the resulting model approximates a target
probability distribution which is implicitly given by training data.
This setting can be formulated as discriminative as well as generative
learning task.  Applications have been studied in a large variety of
domains such as the analysis of quantum many-body systems, statistics,
biochemistry, social networks, signal processing and finance


However, BMs are complicated to train in practice because the loss
function's derivative requires the evaluation of a normalization
factor, the partition function, that is generally difficult to
compute.  Usually, it is approximated using Markov Chain Monte Carlo
methods which may require long runtimes until convergence

Quantum Boltzmann Machines (QBMs) are a natural adaption of BMs to the
quantum computing framework. Instead of an energy function with nodes
being represented by binary spin values, QBMs define the underlying
network using a Hermitian operator, normally a parameterized Hamiltonian, see reference [1] below.

\paragraph{Specific tasks and milestones.}
The aim of this thesis is to study the implementation of Boltzmann machines, and possibly other classical machine learning algorithms, on a quantum computer. The thesis consists of three basic steps:

\begin{enumerate}
\item Develop a classical Boltzmann machine code for studies of classification and regression problems.

\item Compare the results from the classical Boltzmann machine with other deep learning methods.

\item Develop an implementation of a quantum Boltzmann machine code to be run on existing quantum computers and classical computers. Compare the performance of the quantum Boltzmann machines with exisiting classical deep learning methods.
\end{enumerate}

\noindent
The milestones are:
\begin{enumerate}
\item Spring 2021: Develop a code for classical Boltzmann machines to be applied to both classification and regression problems. In particular, the latter type of problem can be tailored to solving classical spin problems like the Ising model or quantum mechanical problems. 

\item Fall 2021: Develop a code for variational Quantum Boltzmann machines following reference [2] here.  Make comparisons with classical deep learning algorithms on selected classification and regression problems.

\item Spring 2022: The final part is to use the variational Quantum Boltzmann machines to study quantum mechanical systems. Finalize thesis. 
\end{enumerate}

\noindent
The thesis is expected to be handed in May/June 2022.

\paragraph{Literature.}
\begin{enumerate}
\item Amin et al., \textbf{Quantum Boltzmann Machines}, Physical Review X \textbf{8}, 021050 (2018).

\item Zoufal et al., \textbf{Variational Quantum Boltzmann Machines}, ArXiv:2006.06004.

\item Maria Schuld and Francesco Petruccione, \textbf{Supervised Learning with Quantum Computers}, Springer, 2018.

\item Yuan et al., \textbf{Theory of Variational Quantum Simulations}, ArXiv:1812.08767. 
\end{enumerate}

\noindent

% ------------------- end of main content ---------------

\end{document}

