As discussed in the previous chapter, supervised machine learning algorithms require label training data, meaning that both x and y components exist. Therefore, a supervised machine learning algorithm learns to approximate a function, f, during its training process such that f(x) = y for every value of x in the data set.  

%% Why the traditional way of training makes a bad extrapolator
However, supervised machine learning algorithms tend to fail when asked to make predictions outside their training range (for example, see Ref. \cite{Ref6}). Put another way, supervised machine learning algorithms tend to make more extrapolators and are thus not used for extrapolation applications. However, one type of supervised machine learning performed well with extrapolations; in fact, it was designed to make extrapolations on time series data. This algorithm is the recurrent neural network discussed in the last section. RNNs perform very well on extrapolations as their design inherently gives them some "memory" of previous data that they can use to predict new data.

%% neural network drawbacks
While RNNS may be a good choice for performing time series analysis and extrapolations, they have some significant drawbacks when applying them to \textit{ab initio} data sets. First, RNNs, like all neural networks, require much training data to make accurate predictions and avoid overfitting. While this is not a problem for many applications of neural networks, this is a significant drawback when applying any neural network to \textit{ab initio} data sets. Since each new point in an \textit{ab initio} data set can represent significant computational time and resource investment, these data sets are usually relatively small, especially by neural network standards \cite{Ref6}. One way around this is to use an interpolation algorithm to increase the size of the data set artificially, but this represents an additional step in the workflow and possibly an additional source of error in the analysis \cite{Ref6}. Thus in this work, we want to avoid using interpolation as much as possible.  

Secondly, due to how their weights are initialized, neural networks are inherently random \cite{Ref6}. This means a neural network, including RNNs, will produce slightly different results each time it is trained, even when the same training data is used. The irreproducibility is a significant drawback when using neural networks to predict physical values. Additionally, the uncertainty of a neural network's prediction is an open research question. 

Finally, as mentioned in the previous chapter, neural networks generally have a large number of hyperparameters, including the number of hidden layers, the number of neurons per hidden layer, and the activation function, among many others. This leads to complicated and long tuning processes, dramatically increasing the runtime needed to perform an ML analysis.

%% time series formatting with regression algorithms: SRE
While RNNs are not a good choice of a machine learning algorithm for this application, we can take inspiration from them and create a machine learning algorithm that can. A standard machine learning algorithm is trained to take a point from the training set, $x_i$, and match it to its corresponding y values, $y_i$. Thus training a standard machine learning algorithm has the following relations to be learned:

\begin{equation}
    f_{ML}(x_1) = y_1, f_{ML}(x_2) = y_2, ... .
\end{equation}

While this training pattern makes many supervised machine learning algorithms excellent at matching new inputs to the correct output, they typically only perform well in the range of data encompassed by the training data.

However, there is a way to train RNNs around this problem. This training pattern is typically used when RNNs perform time-series analysis, which is common in the financial industry. Instead of learning the relationship between the x and y components of the data set, a time-series data formatting teaches the RNN to learn the pattern between a sequence of y values and the next y value in the training data. The training points in this form will look as follows:

\begin{equation}
    f_{RNN}(y_{k-3}, y_{k-2}, y_{k-1}) = y_k,
\end{equation}

making the RNN much better at extrapolating because it is trained to predict the next value in a sequence. Note that the sequence length in the inputs could be of any length, thus adding another hyperparameter.

The drawback of this form of training here is that the data must be sequential (have an order to it), and the data must be evenly spaced regarding the dependent variables. Additionally, since the RNN only sees the y component of the data, it may lose the information encoded in the x component. However, an RNN trained in this manner is an extremely powerful extrapolator.

Nevertheless, as explained earlier in this section, there are several drawbacks to using RNNs in this application. However, we can combine the training style of an RNN with a simpler machine-learning algorithm to make a more straightforward but still powerful extrapolator. Furthermore, since none of the data in this thesis is time-dependent, instead of calling this style of formatting the data a time series, we will call it sequential formatting, as the data must be arranged sequentially. This gives rise to the name of the machine learning algorithm we are building to extrapolate many-body data sets, sequential regression extrapolation (SRE); so named because it combines sequential data formatting with a regression algorithm to create an extrapolator that can make predictions from small data sets. Thus we will train a regression algorithm using the following format:

\begin{equation}
    f_{R}(y_{k-3}, y_{k-2}, y_{k-1}) = y_k.
\end{equation}

The length of the input, now known as the SRE sequence length, can be any length. However, the larger the sequence length is, the fewer total data points present once the data has been formatted, so there is a balancing act between choosing a sequence length long enough to encode the sequential patterns in the data but not so long that there are very few points remaining after the formatting.

Now that we have developed the algorithm generally, we can apply it to remove the basis incompleteness and finite size errors arising from many-body calculations of infinite matter systems. The remainder of this section will develop the specific SRE formulism to remove these errors.


