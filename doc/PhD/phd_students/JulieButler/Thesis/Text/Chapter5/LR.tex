Linear regression is one of the most basic machine learning algorithms. However, it is often the first algorithm studied in a machine learning class or textbook because it still contains all of the essential elements of a supervised machine learning algorithm (a loss function, parameterizes to be optimized, an optimization). One of the main reasons linear regression is still studied as a machine learning algorithm is that it has analytical expressions for its parameters. This contrasts with other supervised machine learning algorithms, such as neural networks, which use numerical algorithms to optimize the parameters. Furthermore, due to the analytical expressions for the parameters, linear regression will always return the same output if given the same input, again in contrast to other machine learning algorithms such as neural networks. This reproducibility makes linear regression and its related algorithms attractive for physical applications. 

The output of the linear regression algorithm can be written as follows: 
\begin{equation}
    \hat{y}_{Linear} = X\theta,
\end{equation}

where X is the input of the algorithm and $\theta$ is a vector of constant parameters, often called the weights. The "machine learning" part of the algorithm is tuning $\theta$ so that:

\begin{equation}
    y_{True} = \hat{y}_{Linear}.
\end{equation}

In machine learning, a loss function measures the error in the algorithm's prediction from the real data. The loss function for the linear regression algorithm is the mean-squared error function:

\begin{equation}
    J_{Linear}(\theta) = \frac{1}{n}\sum_{i=0}^{n-1} (y_i - \hat{y}_{Linear,i})^2
\end{equation}

where y = y$_{True}$.

The optimal values of $\theta$ are the ones that make the output of the linear regression algorithm the same as the actual data. Another way of phrasing this is that the optimal values of $\theta$ are the values that minimize the loss function. Thus linear regression becomes a simple minimization problem!

Minimizing the loss function with respect to the parameters $\theta$ yields the optimal parameters, the values of $\theta$ that make the output of the linear regression algorithm as close to the actual data as possible. 

\begin{equation}
    \theta_{Linear} = (X^TX)^{-1}X^Ty.
\end{equation}

These optimized values of $\theta$ can then be multiplied by new data not in the training set to make predictions for new data.

