The following two machine learning algorithms we will investigate belong to a family of algorithms called Bayesian machine learning, so called because they rely on Bayes' theorem and Bayesian statistics to both make predictions and determine the uncertainty of those predictions.  The main draw of using Bayesian machine learning algorithms over standard machine learning algorithms is that Bayesian algorithms determine their hyperparameters using Bayesian statistics to determine the most likely values.  They also produce uncertainty in their results, which is essential for machine learning applications in the physical sciences \cite{Ref11, Ref12, Ref169}.

Therefore an alternative to performing traditional ridge regression with hyperparameter tuning is to use Bayesian ridge regression, the Bayesian form of ridge regression where the training process sets the value of $\lambda$ \cite{Ref11, Ref12, Ref170, Ref171}. In Bayesian ridge regression, the output data, y, is assumed to be in a Gaussian distribution around X$\theta$:

\begin{equation}
    p(y|X,\theta,\lambda) = N(y | X\theta, \lambda).
\end{equation}

Priors for the weights, $\theta$, are give by a spherical Gaussian, where $\beta^{-1}$ is taken to be the precision:

\begin{equation}
    p(\theta | \beta) = N(\theta | 0, \beta^{-1}\textbf{I}),
\end{equation}

where \textbf{I} is the identity matrix.

The priors over $\lambda$ and $\beta$ are assumed to be gamma distributions, and $\theta$, $\lambda$, and $\beta$ are estimated jointly when the model is fit. The Bayesian ridge regression analysis will result in different weights than a regular ridge regression analysis, but a Bayesian ridge regression algorithm is more robust. Additionally, since the value of $\lambda$ is set when the algorithm is trained, using Bayesian ridge regression eliminates the need for hyperparameter tuning and a validation data set, making it a better choice than ridge regression when time savings is essential, and the size of the data sets is minimal.