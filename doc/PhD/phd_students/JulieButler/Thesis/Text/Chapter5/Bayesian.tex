Bayesian statistics is a subset of the field of statistics, often presented in contrast to what is thought of as normal statistics, also known as the frequentist approach.  The frequentist approach to statistics defense the probability of an event occuring as its relative frequency over many trials.  Therefore, probabilities are found by a repeatable process (thus leaving out the opinion of events occuring).  Bayesin statistics, on the other hand, expresses probability as a degree of belief in an event, which may be based on prior knowledge of the event or on personal beliefs about the event, thus introducing opinions into the analysis.  These initial beliefs on the event are updates with new evidence, leading the initial prior belief to be updated to a posterior belief.

Bayesian approaches to statistics provide a way in which the prior belief can be mathematically represented and updated using new data. Methods which are based on Bayesian statistics use Baye's theorom to compute and update the probabilities after obtaining new data.  Bayes theorem can be defined as:

\begin{equation}\label{bayes}
	P(A|B) = frac{|(B|A)P(A)}{P(B)},
\end{equation}

where A and B are two events, P(X) is the probability event X will happen and P(X|Y) is the probability that event X will happen given that event Y has happened.  Note that P(B) $\neq$ 0.  In Bayesian statistics, the interpretations of each quantity in Eq. \ref{bayes} are as follows.  A represents the proposition, or the event that is being studied.  B represents the evidence, or new data that is being taken into account about the trials that could cause A to occur. P(A) is the prior probability, which is based on the initial beliefs before any evidence or data is taken into account.  P(B|A) is called the liklihood function, which quantifies to what extent B supports A being true.  P(A|B) is called the posterior probability, and it is the probability of A occuring after the evidence or new data is taken into account. 

Bayesian statistics can provide a backbone on which to build statistical models which can then be turned into machine learning algorithms.  These algorithms which have been built on Bayesian statistics form a family of ML algorithms called Bayesian machine learning.  In Bayesian machine learning A is the parameters of the machine learning model and B is the training data.

